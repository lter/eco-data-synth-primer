---
title: "Tying It All Together"
---

## Learning Objectives

After completing this module, you will be able to:

- **Identify** the features that make published data easily re-useable
- **Evaluate** the metadata and fitness-for-use of published synthesis datasets
- **Understand** the three primary "products" that come out of synthesis groups
- **Understand** concepts of data provenance, reproducible analysis, and citations
- **Evaluate** the reproducibility of a recent data synthesis project
- **Understand** the different models of data accessibility, licensing, and authorship practices and **apply** them to a synthesis group’s desired outcomes
- **Understand** several funding opportunities that for synthesis research, their requirements and expectations, and their respective strengths and weaknesses for starting and sustaining synthesis research.
- **Create** a plan to maintain a synthesis project and associated data over the long-term

# Introduction

So far, we've made the point that ecological synthesis research is collaborative and inclusive, and that it integrates a wide range of data. Synthesis research is also intended to be **influential** and **useful**. There are many definitions of "influential and useful" to consider here, but successful synthesis research tends to expand the boundaries of knowledge and aims to improve human lives or the environment. The ability to accomplish this in synthesis research frequently depends on what knowledge or products are created, and how the synthesis team disseminates and communicates them to the outside world.

<img src="images/mod3_fig1_tying.png" style="float: right" width="50%"/>
There are three interconnected, publishable products for a synthesis project (or any research project, really): the data, analytical workflows (code for data cleaning or statistics, for example), and research results. Each of these elements is a valuable product of synthesis science, and each one should reference the others. In this module we'll discuss the mechanics of publishing each one, and then how they can be connected and made accessible for the long-term.


# Designing and publishing synthesis datasets

**Estimated time: 12 min**

In Module 2 we discussed some considerations for creating and formatting harmonized data files useful for synthesis research. We also introduced the importance of metadata for describing data and making it more usable. Publishing harmonized data files and descriptive metadata together as a **dataset** ensures that the data products produced by a synthesis team are findable, accessible, interoperable, and reusable (FAIR). FAIR data are an important output for almost any ecological synthesis project.

::: {.callout-tip collapse="true"}
## More about Findable, Accessible, Interoperable, Reusable (FAIR) data

The FAIR principles, standing for Findability, Accessibility, Interoperability, and Reusability,  are a community-standard set of guidelines for evaluating the quality and utility of published research data. Making an effort to meet the FAIR criteria promotes both human and machine usability of data, and is a worthy objective when preparing to publish the data products from a synthesis research project.

The FAIR principles were first defined in the paper by Wilkinson et al (2018)[^1]. Since this time, many resources have arisen to guide the implementation the FAIR principles[^2][^3], and to quantify FAIR data successes and failures in the research and publishing communities[^4][^5].

[^1]: Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18
[^2]: [GoFAIR initiative](https://www.go-fair.org/how-to-go-fair/)
[^3]: [The FAIR Cookbook](https://faircookbook.elixir-europe.org)
[^4]: Bahim, C., Casorrán-Amilburu, C., Dekkers, M., Herczog, E., Loozen, N., Repanas, K., Russell, K. and Stall, S. (2020) ‘The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments’, Data Science Journal, 19(1), p. 41. Available at: https://doi.org/10.5334/dsj-2020-041.
[^5]: Gries, Corinna, et al. "The environmental data Initiative: Connecting the past to the future through data reuse." Ecology and Evolution 13.1 (2023): e9592. https://doi.org/10.1002/ece3.9592

:::

## Metadata

Metadata are data about the data. As a general rule, metadata should describe

  * **Who** collected the data
  * **What** was observed or measured
  * **When** the data were collected
  * **Where** the data were collected
  * **How** the data were collected (methods, instruments, etc.)

Oftentimes, including information about **why** the data were collected can help future users understand the context of the data and use them. Including metadata of this nature makes data more usable, and helps prevent the deterioration of information about data over time, as illustrated in the figure below (from Michener et al. 1997[^6]).

[^6]: Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2

![Example of the normal degradation in information content associated with data and metadata over time (“information entropy”). Accidents or changes in technology (dashed line) may eliminate access to remaining raw data and metadata at any time (Michener et al 1997.](images/michener97_information_loss.png){width=60%}

### Data provenance metadata

<img style="float: right;" src="images/provenance.png" alt="What data should you publish" width="25%" padding="10px"/>
Provenance metadata deserves special attention for ecological data synthesis projects. **Data provenance** refers to information detailing the origin of the values in a dataset, which is particularly important for synthesis projects that bring together data from many different sources. Synthesis activities typically produce new data products that are **derived** from the original source data after they have been cleaned, harmonized, and analyzed. Provenance metadata should be included with the derived products to point back to the original source data, similar to the way bibliographic references point to the source material for a book or scholarly article.

A few other notes on provenance:

  * At its simplest, documenting data sources as you collect and analyze the source data is a great start on provenance metadata. 
  * Many data repositories provide guidelines, tools, and features for data provenance metadata[^7].
  * Provenance metadata can become very detailed if the software and computing environment is also taken into account. This is an active area of study [^8][^9].

[^7]: [Provenance metadata at the EDI repository](https://edirepository.org/resources/provenance-metadata)
[^8]: Lerner, et al., "Making Provenance Work for You", The R Journal, 2023. https://journal.r-project.org/articles/RJ-2023-003/
[^9]: [End-to-End Provenance](https://end-to-end-provenance.github.io/)

### Licensing

Published datasets should include a license in every copy of the metadata that defines who has what rights to use, reproduce, or distribute the data. Licensing decisions should be made in consultation with the synthesis team after considering the nature of the data (does it contain human subject data, for instance?), its origin (including restrictions on source data, if applicable), and the requirements of the funders and institutions associated with the project. For publicly-funded environmental research data, it is generally appropriate to use open licenses, and the Creative Commons CC-BY attribution, and CC0 public domain, licenses are probably a good choice for most ecological synthesis data. This is not legal advice and your mileage may vary.

### Metadata creation and management

Assembling metadata should be an integral part of the data synthesis activities discussed in Module 2, and can even be built-in to the workflow and project management practices of a project. **Make sure to plan for and start creating metadata early** in a synthesis project. Below are a few ways to do that.

1. Keep a detailed project log and populate it with metadata for the project, including information like
    a. what source data the team is using and where they came from.
    b. how data are being analyzed and methods used to create derived products.
    c. who is doing what.
2. Start creating distinct publishable datasets (data plus metadata) as data are processed and analyzed. The team can do this
    a. locally, using a labeled directory for the cleaned, harmonized, of derived data, along with related code and metadata files. Metadata files may be plain text, or use [a metadata template](https://github.com/jornada-im/documentation/raw/main/templates/Jornada_metadata_template.docx).
    b. with a repository-based metadata editor, such as [ezEML](https://ezeml.edirepository.org) from the Environmental Data Initiative (EDI) repository.
3. Get a professional data manager or data curator involved with the synthesis project. For example, the LTER Network has a community of "Information Managers" [^10] trained in data management, metadata creation, and data publishing. Research data repositories[^11] and associated data curators[^12] may also be a good resource. 

[^10]: [List of LTER Information Managers](https://lternet.edu/using-lter-data/#im)
[^11]: [The Registry of Research Data Repositories (re3data.org)](https://www.re3data.org/)
[^12]: [Data curation network](https://datacurationnetwork.org/)

## Deciding what to publish

<img style="float: right;" src="images/what_data.png" alt="What data should you publish" width="20%" padding="10px"/>
The overall design of the dataset to be published is often difficult to imagine, particularly for people new to using or creating datasets. One of the most common questions data managers hear is "*What should I publish?*" The answer usually comes down to:

- Publish any data used to generate research results.
- Publish any data that will be used by others (scientists, managers, public stakeholders), including raw data.
- If reproducibility is of interest or concern, publish the workflow (usually code) used to process or analyze the data, or to generate research results like figures.
- Always publish descriptive metadata about any of the above.

In the activity below we will browse a few published datasets to get a feel for what useful data does and doesn't look like. You can also look at advice from a repositories like [EDI](https://edirepository.org/resources/designing-a-data-package) and [BCO-DMO](https://guide.bco-dmo.org/prepare/what-is-a-dataset), or from a research network like [NEON](https://www.neonscience.org/data-samples/guidelines-policies/publishing-research-outputs).

## Choosing and publishing to a repository

There are many, many research data repositories available to researchers now[^11], making the choice of where to publish data fairly challenging. A few basic data repository features are essential when publishing a synthesis dataset. First, the repository should issue persistent, internet-resolveable, unique identifiers for every dataset published. Generally this will be a [Digital Object Identifier](https://doi.org), or DOI, that can be cited every time the dataset is used after publication. Second, repositories should require, and provide the means to create/publish, metadata describing each dataset. Without requiring at least minimal metadata, no repository can ensure that published data are FAIR. Finally, research data repositories should be stable and well supported so that data remain available and usable in perpetuity. Choosing a repository from the [CoreTrustSeal certified repository list](https://amt.coretrustseal.org/certificates) is one way to assess this. Beyond this, asking a few questions about the dataset will help with repository selection:

1. Who are the likely users for this data? Will they belong to a specific scientific discipline, research network, or community of stakeholders?
2. How specialized are your data? Do they fall into a common data type or follow a speical formatting standard?
3. Will the data be updated regularly?

![A limited slice from the broad spectrum of research data repositories available for publishing synthesis data.](images/repository_spectrum.png){width=90%}

After making a choice, the process of publishing data varies from repository to repository. More specialized repositories tend to offer enhanced documentation, custom software tools, or even data curation staff to assist users with data publication. It also helps to consult a project data manager if one is available to the synthesis team.

## Activity 1: Evaluate published datasets

**Estimated time: 10 min**

Form breakout groups and course instructors will assign each group a dataset (a DOI) for evaluation. With your group, answer these questions about the dataset:

1. Where were the data collected?
2. What variables were measured and in what units?
3. What is the origin of the data and how have they been altered since collection?
4. Were the first three questions easy to answer? Why or why not?

:::{.panel-tabset}
## Example 1

Data from the SoDaH LTER synthesis working group.

https://portal.edirepository.org/nis/mapbrowse?packageid=edi.521.1

## Example 2

A Dryad dataset from a synthesis paper about oligotrophication.

https://doi.org/10.5061/dryad.v2k2607

## Example 3

Maybe this: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.493.16 (needs editing)

Maybe this: https://doi.org/10.6084/m9.figshare.10735652.v1 (but pretty bad)

:::

## Additional data publishing resources

* [NEON's derived data publishing guide](https://www.neonscience.org/data-samples/guidelines-policies/publishing-research-outputs)
* [EDI repository data authorship guide](https://edirepository.org/resources/resources-for-data-authors)

# Sharing the synthesis workflow

One of the most valuable, shareable outputs of synthesis research is the analytical workflow used to derive datasets and produce scientific results. Most often, these workflows are written in computer code, such as R, Python, or another language. Workflows may consist of a collection of scripts, or they may be organized into stand-alone modules or libraries. The latter is easier to share and re-use, but requires more advanced knowledge of software design. Sharing workflows and code are one of the most important needs for ensuring the reproducibility of science.

Publishing the workflow also gives interested parties an understanding of

* the origin of the data
* the process, or the code itself, for data cleaning, harmonization, analysis, and presentation of results (figures), which may be useful in future work
* how the workflow was developed or changed over time
* the contributions made by the team

In other parts of the course, we have strongly recommended using version control and collaboration platforms to manage coding, writing, and other elements of the synthesis team workflow. In particular, we have focused on using GitHub as a one-stop shop for many of these tasks. In combination with other software and services, GitHub can be reliably used to publish workflows as well. By integrating with repositories that archive GitHub content and issue a DOI (commonly Zenodo)[^13], workflows can be published and cited by the research products that they were used to generate. This is commonly done in near-term ecological forecasting projects [^14].

some kinda schematic...

[^13]: [GitHub documentation for referencing and citing content](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content)
[^14]: White EP, Yenni GM, Taylor SD, et al. Developing an automated iterative near-term forecasting system for an ecological study. Methods Ecol Evol. 2019; 10: 332–344. https://doi.org/10.1111/2041-210X.13104 


# Disseminating research results

* Most often this means writing a paper
* Data papers
  - Anatomy of a data paper?
* Other products may be apps, teaching modules, etc.
  - MacrosystemsEDDIE as an example?
  - Shiny apps

# Connecting the elements of a synthesis project

**Estimated time: 25 min**

## Citing synthesis products


## Giving everyone credit


## Activity 2: Synthesis project detective

**Estimated time: 12 min**

Form breakout groups and course instructors will assign each one a link to a product from a synthesis project (the code, a paper, a dataset, etc.). Using any means necessary (metadata, web search, etc.) figure out what other products are related (other publications, source/derived data, etc.) and who is involved in the synthesis team. Answer these questions as a group:

1. If your group received a link to a paper, were you able to find datasets and an analytical workflow?
2. If your group received a link to a code repository, could you track down papers and datasets?
3. If your group received a link to a dataset, were the connected to papers and an analytical workflow?
4. Who was involved in the synthesis project?
5. Could you understand the overall scope and impact of the synthesis project? Why or why not?

:::{.panel-tabset}
## Example 1

SoDAH

## Example 2

CoRRE or metacommunities

https://corredata.weebly.com/

## Example 3

Silica exports

https://github.com/lter/lterwg-silica-data

:::



## Additional resources

* 

# Maintaining momentum

**Estimated time: 10 min?**

## Funding sources

???
