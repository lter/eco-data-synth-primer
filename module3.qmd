---
title: "Tying It All Together"
---

## Learning Objectives

After completing this module, you will be able to:

- **Identify** the features that make published data easily re-useable
- **Evaluate** the metadata and fitness-for-use of published synthesis datasets
- **Understand** the three primary "products" that come out of synthesis groups
- **Understand** concepts of data provenance, reproducible analysis, and citations
- **Evaluate** the reproducibility of a recent data synthesis project
- **Understand** the different models of data accessibility, licensing, and authorship practices and **apply** them to a synthesis group’s desired outcomes
- **Understand** several funding opportunities that for synthesis research, their requirements and expectations, and their respective strengths and weaknesses for starting and sustaining synthesis research.
- **Create** a plan to maintain a synthesis project and associated data over the long-term


## Introduction

So far, we've made the point that ecological synthesis research is collaborative and inclusive, and that it integrates a wide range of data. Synthesis research is also intended to be **influential** and **useful**. There are many definitions of "influential and useful" to consider here, but successful synthesis research tends to expand the boundaries of knowledge and aims to improve human lives or the environment. The ability to accomplish this in synthesis research frequently depends on what knowledge or products are created, and how the synthesis team disseminates and communicates them to the outside world.

<img src="images/mod3_fig1_tying.png" alt="Three circles labeled 'data', 'results' and 'analytical workflow' with arrows connecting each pair pointing in both directions" style="float: right" width="50%"/>

There are three interconnected, publishable products for a synthesis project (or any research project, really): the data, analytical workflows (code for data cleaning or statistics, for example), and research results. Each of these elements is a valuable product of synthesis science, and each one should reference the others. In this module we'll discuss the mechanics of publishing each one, and then how they can be connected and made accessible for the long-term.


## Designing and Publishing Synthesis Datasets

**Estimated time: 12 min**

In Module 2 we discussed some considerations for creating and formatting harmonized data files useful for synthesis research. We also introduced the importance of metadata for describing data and making it more usable. Publishing harmonized data files and descriptive metadata together as a **dataset** ensures that the data products produced by a synthesis team are findable, accessible, interoperable, and reusable (FAIR). FAIR data are an important output for almost any ecological synthesis project.

::: {.callout-tip collapse="true"}
### More about Findable, Accessible, Interoperable, Reusable (FAIR) data

The FAIR principles, standing for Findability, Accessibility, Interoperability, and Reusability,  are a community-standard set of guidelines for evaluating the quality and utility of published research data. Making an effort to meet the FAIR criteria promotes both human and machine usability of data, and is a worthy objective when preparing to publish the data products from a synthesis research project.

The FAIR principles were first defined in the paper by Wilkinson et al (2018)[^1]. Since this time, many resources have arisen to guide the implementation the FAIR principles[^2][^3], and to quantify FAIR data successes and failures in the research and publishing communities[^4][^5].

[^1]: Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18
[^2]: [GoFAIR initiative](https://www.go-fair.org/how-to-go-fair/)
[^3]: [The FAIR Cookbook](https://faircookbook.elixir-europe.org)
[^4]: Bahim, C., Casorrán-Amilburu, C., Dekkers, M., Herczog, E., Loozen, N., Repanas, K., Russell, K. and Stall, S. (2020) ‘The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments’, Data Science Journal, 19(1), p. 41. Available at: https://doi.org/10.5334/dsj-2020-041.
[^5]: Gries, Corinna, et al. "The environmental data Initiative: Connecting the past to the future through data reuse." Ecology and Evolution 13.1 (2023): e9592. https://doi.org/10.1002/ece3.9592

:::


## Activity 1: Evaluate published datasets

Lets start our journey to publishing datasets by looking at some that are already published. Form breakout groups and course instructors will assign each group a dataset (a DOI) for evaluation. With your group, answer these questions about the dataset:

1. Where were the data collected?
2. What variables were measured and in what units?
3. What is the origin of the data and how have they been altered since collection?
4. Were the first three questions easy to answer? Why or why not?

:::{.panel-tabset}
### Group 1

**Example dataset:** <https://doi.org/10.6073/pasta/9733f6b6d2ffd12bf126dc36a763e0b4>

This is an EDI dataset is from the SoDaH (Soil Data Harmonization) LTER working group.

::: {.callout-caution collapse="true" icon="false"}

### Observations from the instructors

This dataset provides a nice example of a harmonized data product that includes **provenance metadata**. We'll talk a little more about this later, but note that all the original data sources are linked to this dataset on the landing page.

- ...
- ...
- ...

:::

### Group 2

**Example dataset:** <https://doi.org/10.5061/dryad.v2k2607>

This is a Dryad dataset from a synthesis paper about oligotrophication.

::: {.callout-caution collapse="true" icon="false"}

### Observations from the instructors

This dataset

- ...
- ...
- ...

:::

### Group 3

**Example dataset:** 

Maybe this: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.493.16 (needs editing)

Maybe this: https://doi.org/10.6084/m9.figshare.10735652.v1 (but pretty bad)

::: {.callout-caution collapse="true" icon="false"}

### Observations from the instructors

This dataset

- ...
- ...
- ...

:::

### Group 4

**Example dataset:**

::: {.callout-caution collapse="true" icon="false"}

### Observations from the instructors

This dataset

- ...
- ...
- ...

:::

### Group 5

**Example dataset:**

::: {.callout-caution collapse="true" icon="false"}

### Observations from the instructors

This dataset

- ...
- ...
- ...

:::

:::


## Metadata

One thing that Activity 1 should be introducing is the importance of **metadata**. Metadata are data about the data. As a general rule, metadata should describe

  * **Who** collected the data
  * **What** was observed or measured
  * **When** the data were collected
  * **Where** the data were collected
  * **How** the data were collected (methods, instruments, etc.)

Oftentimes, including information about **why** the data were collected can help future users understand the context of the data and use them. Including metadata of this nature makes data more usable, and helps prevent the deterioration of information about data over time, as illustrated in the figure below (from Michener et al. 1997[^6]).

[^6]: Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2

![Example of the normal degradation in information content associated with data and metadata over time ("information entropy"). Accidents or changes in technology (dashed line) may eliminate access to remaining raw data and metadata at any time (Michener et al 1997.](images/michener97_information_loss.png){width=60%}


### Data Provenance Metadata

<img style="float: right;" src="images/provenance.png" alt="Provenance: where did your data come from?" width="25%" padding="10px"/>

Provenance metadata deserves special attention for ecological data synthesis projects. **Data provenance** refers to information detailing the origin of the values in a dataset, which is particularly important for synthesis projects that bring together data from many different sources. Synthesis activities typically produce new data products that are **derived** from the original source data after they have been cleaned, harmonized, and analyzed. Provenance metadata should be included with the derived products to point back to the original source data, similar to the way bibliographic references point to the source material for a book or scholarly article.

A few other notes on provenance:

  * At its simplest, documenting data sources as you collect and analyze the source data is a great start on provenance metadata. 
  * Many data repositories provide guidelines, tools, and features for data provenance metadata[^7].
  * Provenance metadata can become very detailed if the software and computing environment is also taken into account. This is an active area of study [^8][^9].

[^7]: [Provenance metadata at the EDI repository](https://edirepository.org/resources/provenance-metadata)
[^8]: Lerner, et al., "Making Provenance Work for You", The R Journal, 2023. https://journal.r-project.org/articles/RJ-2023-003/
[^9]: [End-to-End Provenance](https://end-to-end-provenance.github.io/)


### Licensing

Published datasets should include a license in every copy of the metadata that defines who has what rights to use, reproduce, or distribute the data. Licensing decisions should be made in consultation with the synthesis team after considering the nature of the data (does it contain human subject data, for instance?), its origin (including restrictions on source data, if applicable), and the requirements of the funders and institutions associated with the project. For publicly-funded environmental research data, it is generally appropriate to use open licenses, and the Creative Commons CC-BY attribution, and CC0 public domain, licenses are probably a good choice for most ecological synthesis data. This is not legal advice and your mileage may vary.


### Metadata Creation and Management

Assembling metadata should be an integral part of the data synthesis activities discussed in Module 2, and can even be built-in to the workflow and project management practices of a project. **Make sure to plan for and start creating metadata early** in a synthesis project. Below are a few ways to do that.

1. Keep a detailed project log and populate it with metadata for the project, including information like
    a. what source data the team is using and where they came from.
    b. how data are being analyzed and methods used to create derived products.
    c. who is doing what.
2. Start creating distinct publishable datasets (data plus metadata) as data are processed and analyzed. The team can do this
    a. locally, using a labeled directory for the cleaned, harmonized, of derived data, along with related code and metadata files. Metadata files may be plain text, or use [a metadata template](https://github.com/jornada-im/documentation/raw/main/templates/Jornada_metadata_template.docx).
    b. with a repository-based metadata editor, such as [ezEML](https://ezeml.edirepository.org) from the Environmental Data Initiative (EDI) repository.
3. Get a professional data manager or data curator involved with the synthesis project. For example, the LTER Network has a community of "Information Managers" [^10] trained in data management, metadata creation, and data publishing. Research data repositories[^11] and associated data curators[^12] may also be a good resource. 

[^10]: [List of LTER Information Managers](https://lternet.edu/using-lter-data/#im)
[^11]: [The Registry of Research Data Repositories (re3data.org)](https://www.re3data.org/)
[^12]: [Data curation network](https://datacurationnetwork.org/)


## Deciding What to Publish

<img style="float: right;" src="images/what_data.png" alt="What data should you publish" width="20%" padding="10px"/>

The overall design of the dataset to be published is often difficult to imagine, particularly for people new to using or creating datasets. One of the most common questions data managers hear is "*What should we publish?*" This is usually a question about what files to include in the published dataset, or what data will be useful as a published dataset.

:::{.panel-tabset}
### Discussion question

> **What should be included in a published dataset?**

### Some general rules

As we learned in Activity 1, every dataset is different, but the answer to "What should we publish?" usually comes down to:

- Publish any data used to generate research results.
- Publish any data that will be used by others (scientists, managers, public stakeholders), including raw data.
- If reproducibility is of interest or concern, publish the workflow.
  - Usually this means publishing code, such as scripts written in R, python, or a shell language.
  - What code? Any scripts used to process or analyze the data, or to generate research results like figures, are fair game.
  - Sometimes code, especially detailed, reusable workflows like an R package, can stand alone as an independent publication. We discuss that in a later section.
  
And of course... **always publish descriptive metadata about any of the above.**

These are general rules, but you can also look at advice from a repositories like [EDI](https://edirepository.org/resources/designing-a-data-package) and [BCO-DMO](https://guide.bco-dmo.org/prepare/what-is-a-dataset), or from a research network like [NEON](https://www.neonscience.org/data-samples/guidelines-policies/publishing-research-outputs). Asking a data manager, especially one involved with the synthesis group's work, can also be helpful, as will discussion among the full synthesis team.

:::


## Choosing and Publishing to a Repository

There are many, many research data repositories available to researchers now[^11], making the choice of where to publish data fairly challenging. A few basic data repository features are essential when publishing a synthesis dataset. First, the repository should issue persistent, internet-resolveable, unique identifiers for every dataset published. Generally this will be a [Digital Object Identifier](https://doi.org), or DOI, that can be cited every time the dataset is used after publication. Second, repositories should require, and provide the means to create/publish, metadata describing each dataset. Without requiring at least minimal metadata, no repository can ensure that published data are FAIR. Finally, research data repositories should be stable and well supported so that data remain available and usable in perpetuity. Choosing a repository from the [CoreTrustSeal certified repository list](https://amt.coretrustseal.org/certificates) is one way to assess this. Beyond this, asking a few questions about the dataset will help with repository selection:

1. Who are the likely users for this data? Will they belong to a specific scientific discipline, research network, or community of stakeholders?
2. How specialized are your data? Do they fall into a common data type or follow a speical formatting standard?
3. Will the data be updated regularly?

![A limited slice from the broad spectrum of research data repositories available for publishing synthesis data.](images/repository_spectrum.png){width=90%}

After making a choice, the process of publishing data varies from repository to repository. More specialized repositories tend to offer enhanced documentation, custom software tools, or even data curation staff to assist users with data publication. It also helps to consult a project data manager if one is available to the synthesis team.


### Additional Data Publishing Resources

- [NEON's derived data publishing guide](https://www.neonscience.org/data-samples/guidelines-policies/publishing-research-outputs)
- [EDI repository data authorship guide](https://edirepository.org/resources/resources-for-data-authors)
- [BCO-DMO repository data publishing guide](https://guide.bco-dmo.org)


## Sharing the Synthesis Workflow

One of the most valuable, shareable outputs of synthesis research is the analytical workflow used to derive datasets and produce scientific results. Most often, these workflows are written in computer code, such as R, Python, or another language. Workflows may consist of a collection of scripts, or they may be organized into stand-alone modules or libraries. The latter is easier to share and re-use, but requires more advanced knowledge of software design. Sharing workflows and code are one of the most important needs for ensuring the reproducibility of science.

Publishing the workflow also gives interested parties an understanding of

* the origin of the data
* the process used for data cleaning, harmonization, analysis, and presentation of results (figures), which may be useful in future work
* how the workflow was developed or changed over time
* the contributions made by the team

In other parts of the course, we have strongly recommended using version control and collaboration platforms to manage coding, writing, and other elements of the synthesis team workflow. In particular, we have focused on using GitHub as a one-stop shop for many of these tasks. In combination with other software and services, GitHub can be reliably used to publish workflows as well. By integrating with repositories that archive GitHub content and issue a DOI (commonly Zenodo)[^13], workflows can be published and cited by the research products that they were used to generate. This is commonly done in near-term ecological forecasting projects [^14].

[^13]: [GitHub documentation for referencing and citing content](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content)
[^14]: White EP, Yenni GM, Taylor SD, et al. Developing an automated iterative near-term forecasting system for an ecological study. Methods Ecol Evol. 2019; 10: 332–344. https://doi.org/10.1111/2041-210X.13104 


## Communicating Research Results

One of the primary goals of synthesis research is to find useful, generalizable research results about the system under study. Most often this means writing scientific journal articles. While we aren't going to go into full detail about what constitutes, or how to write, a manuscript for a journal, there are some unique features of writing articles for synthesis projects. First, **data papers** are often an important product for synthesis groups. Second, given, the large size and cooperative nature of most synthesis teams, a **collaborative writing process** is called for.

### Data papers

A data descriptor article, usually know as a data paper, is a peer-reviewed journal article written to introduce and describe a (usually) new dataset. For synthesis teams, who are often producing a harmonized dataset as their first major research product, writing a data paper to accompany the dataset makes sense as a way to introduce the data, demonstrate their utility, and get the word out about the dataset. However, there are a few gotchas and best practices to mention.

1. Though the publisher of the data paper may archive a copy of the data to accompany the article, it is common for the data paper to reference a dataset in a research data repository. We recommend using a repository dataset for the data paper for a variety of reasons (namely, ease in discovery & usability), but this may create some confusion in which DOI to cite - the paper's or the repository's).
2. Be sure to consider the future usability of the dataset the data paper is describing. Data papers don't necessarily provide the detailed metadata necessary for re-use of the dataset, so this should be provided
Though the journal publishing the Even though the published Below are some example 
  - Anatomy of a data paper?
  - These are more common for synthesis groups and are often a first output.
  - Venuess for a data paper
    - Nature Scientific Data
    - Ecology
    - Data (MDPI journal)
    
### Writing collaboratively

## Activity 2: Synthesis project detective

**Estimated time: 12 min**

Form breakout groups and course instructors will assign each one a link to a product from a synthesis project (the code, a paper, a dataset, etc.). Using any means necessary (metadata, web search, etc.) figure out what other products are related (other publications, source/derived data, etc.) and who is involved in the synthesis team. Answer these questions as a group:

1. If your group received a link to a paper, were you able to find datasets and an analytical workflow?
2. If your group received a link to a code repository, could you track down papers and datasets?
3. If your group received a link to a dataset, were the connected to papers and an analytical workflow?
4. Who was involved in the synthesis project?
5. Could you understand the overall scope and impact of the synthesis project? Why or why not?

:::{.panel-tabset}
### Group 1

**Clue:** SoDAH

::: {.callout-caution collapse="true" icon="false"}

### Cracking the case

This is the SoDAH LTER synthesis working group. The group

*Papers*

- ...
- ...
- ...

*Workflows*

- ...

*Datasets*

- ...

*Other*

- ...

:::

### Group 2

**Clue:** https://corredata.weebly.com/

::: {.callout-caution collapse="true" icon="false"}

### Cracking the case

This is the CoRRE synthesis working group. The group

*Papers*

- ...
- ...
- ...

*Workflows*

- ...

*Datasets*

- ...

*Other*

- ...

:::

### Group 3

**Clue:** https://github.com/lter/lterwg-silica-data

::: {.callout-caution collapse="true" icon="false"}

### Cracking the case

This is the silica exports working group. The group

*Papers*

- ...
- ...
- ...

*Workflows*

- ...

*Datasets*

- ...

*Other*

- ...

:::

### Group 4

**Clue:**

::: {.callout-caution collapse="true" icon="false"}

### Cracking the case

This is the...

*Papers*

- ...
- ...
- ...

*Workflows*

- ...

*Datasets*

- ...

*Other*

- ...

:::

### Group 5

**Clue:**

::: {.callout-caution collapse="true" icon="false"}

### Cracking the case

This is the...

*Papers*

- ...
- ...
- ...

*Workflows*

- ...

*Datasets*

- ...

*Other*

- ...

:::

:::

## More ways to synthesize
    
<img src="images/mod3_more_products.png" alt="Three circles labeled 'data', 'results' and 'analytical workflow', plus many more possible products" style="float: right" width="50%"/>

We've talked about the three most common products of synthesis: papers, datasets, and workflows. What are some other ways to share synthesis research? There are plenty!

:::{.panel-tabset}
### Teaching materials

Synthesis research produces new scientific knowledge that other researchers, students, or stakeholders can learn and build on. Synthesis can also generate applied-science tools and methods that others need to learn how to use for themselves. Teaching modules are an important way of sharing both of these outcomes, and of broadening the reach of a synthesis project.

Examples:

- The [EDDIE project](https://serc.carleton.edu/eddie/teaching_materials/index.html) is a clearinghouse of contributed teaching materials for the earth and environmental sciences.
- This website is an example of teaching materials produced by a synthesis team.

### Web apps

Interactive web applications can provide users with easy access to scientific datasets, especially large ones, analytical results, visualizations, interpretation, and many, many other things. Creating web apps is not necessarily an easy task, but if your synthesis team has the expertise, or access to web developers, web apps may be useful for outreach, or as tools the synthesis team itself can use. Frameworks like [Shiny](https://shiny.posit.co/) (for R), [Streamlit](https://streamlit.io), or [Flask](https://flask.palletsprojects.com) (both for python), and services like [Shinyapps.io](https://www.shinyapps.io/) and [Plotly](https://plotly.com), can make creation of apps relatively painless.

Examples

  - A [dashboard app](https://projects.ecoforecast.org/neon4cast-dashboard/phenology) for the NEON ecological forecasting challenge.
  - The [Jornada LTER interactive viewer](https://jornada-data.shinyapps.io/jrn_dataviewer/) for weather station data.
  
### Automation

Some research efforts have developed automation systems for research data processing, analytics, and forecasting. These often fall into the "continuous integration/continuous deployment" class of web-enabled software and data pipelines, in which one software processes (data processing, analytics, publication, etc.) may be automatically triggered by events that occur in another, connected software service (such as adding new data to a GitHub repository). These technologies enable researchers to build software pipelines that can be useful for quality control of new data, updating forecasts, and rapid deployment of data or analysis products. 

Examples: 

  - The Portal Project in southeast Arizona has developed a well-described [near-term ecological forecasting pipeline](https://portal.naturecast.org/).[^14]
  - [Automated quality control](https://github.com/SCBI-ForestGEO/Dendrobands) of dendrometer band data.[^15]
  - [Forecasting Lake and Reservoir Ecosystems](https://flare-forecast.org/) (FLARE) project.


[^15]:  Kim, A. Y., Herrmann, V., Barreto, R., Calkins, B., Gonzalez-Akre, E., Johnson, D. J., Jordan, J. A., Magee, L., McGregor, I. R., Montero, N., Novak, K., Rogers, T., Shue, J., & Anderson-Teixeira, K. J. (2022). Implementing GitHub Actions continuous integration to reduce error rates in ecological data collection. Methods in Ecology and Evolution, 13, 2572–2585. https://doi.org/10.1111/2041-210X.13982 
  
### Project websites

At a certain point, the outputs of a synthesis project can become numerous and challenging to present to the public in an organized way. Project websites can serve as a gateway to an entire synthesis project by providing comprehensive listings of project outputs (papers, datasets, GitHub repositories, etc), a narrative for the research, appealing images or graphics for outreach, and links to related projects, funders, or institutions. [GitHub Pages](https://pages.github.com/) sites are a common solution for creating simple, cost-effective (free, usually) project websites nowadays, but there are other options. A good project website can become a cohesive, engaging clearinghouse for information about a synthesis project, but they can become laborious to create and keep up-to-date.

Examples:

  - [The Portal Project](https://portal.weecology.org/)
  - [The CoRRE project](https://corredata.weebly.com/)

:::


## Connecting the Elements of a Synthesis Project

...

### Persistent unique identifiers

- DOIs
- ORCIDs & RORs
- Other identifiers

### Citing synthesis products

- Journal requirements
- Proper data availability statements
- Citing datasets and code


## Maintaining momentum

As we discussed in Module 1, starting a synthesis project benefits from motivating scientific questions, a well-planned foundation for team science, and significant activation energy from the team. When successful, synthesis projects gather enough momentum to be productive for many years. Below are a few ideas on how to maintain this momentum.

### Give everyone credit

- Associate ORCID's with published products
- Defining the type of contribution
    - Projects should define in advance
    - Use frameworks like CReDIT... etc.
- Make contributions visible whenever possible.

### Encourage new contributions

- Provide a path for new data contributions
- Have ways for people to get involved
- Give team members the freedom and support to lead analyses and papers

### Find monetary support

Maintaining momentum for a synthesis project over the long term is highly dependent on the ability to support dedicated personnel time.

- Refer to funding sources in Module 1
- Personnel support may need to come from larger grants


