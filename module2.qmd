---
title: "Module 2: Operational synthesis"
---

# Learning Objectives

After completing this module, you will be able to:

- **Identify** characteristics of reproducible coding / project organization
- **Explain** benefits of reproducibility (to your team and beyond)
- **Summarize** the advantages of creating a defined contribution workflow
- **Define** fundamental vocabulary of version control systems
- **Create** a repository on GitHub
- **Explain** how synthesis teams can use GitHub to collaborate more efficiently and reproducibly

# What happens during synthesis research? (Greg added - please review)

Here are a couple of serviceable definitions of what research is:

> "creative and systematic work undertaken in order to increase the stock of knowledge"
>
> *from the 2015 Frascati Manual* (https://doi.org/10.1787/9789264239012-en)

or 

> "studious inquiry or examination," and especially "investigation or experimentation aimed at the discovery and interpretation of facts"
>
> *from the Merriam-Webster dictionary* (https://www.merriam-webster.com)
    
To these basic definitions of research, our definition of synthesis research adds **collaborative work**, and the **compilation and analysis of a wide range of data sources**, to achieve a more complete or generalizable research result. Some of the key activities that must happen during synthesis research are:

- **Asking scientific questions** (often at large spatial or temporal scales)
- **Finding information** (or data) from a wide variety of sources to answer those questions
- **Harmonizing diverse data** to make it analyzable
- **Analyzing data** to answer questions
- **Interpreting** the results of your analysis
- **Writing** the papers or **creating** other research products 

As we discussed in Module 1, a collaborative, inclusive team can set the stage for successful synthesis research. Each of the specific research activities above will also benefit from this mindset, and in this module we lay out some of the most important considerations and practices for a *team science* approach to the nuts-and-bolts of synthesis research.

# Reproducibility Practices

<p align="center">
<img src="images/mod2_repro.png" width="90%"/>
</p>

Making one's work "reproducible"--particularly in code contexts--has become increasingly popular but is not always clearly defined. For the purposes of this short course, we believe that reproducible work:

- Uses scripted workflows for all interactions with data
- Contains sufficient documentation for those outside of the project team to navigate the project's contents
- Contains detailed metadata for all data products
- Allows anyone to recreate the entire workflow from start to finish
- Leads to modular, extensible research projects. Adding data from a new site, or a new analysis, should be relatively easy in a reproducible workflow.
- 



## General Considerations

**Estimated time: 10 min**

### {{< fa file-lines >}} Documentation

- One folder per project
- Further organize content via sub-folders
- Make file names informative and intuitive
   - Avoid spaces and special characters in file names
   - Follow a consistent naming convention throughout
- Use READMEs to record organization rules / explanation
- Keep a log of where data came from. 
   - Where did you search?
   - What search terms did you use?
   - List the dataset identifiers you downloaded/used.
- 

### {{< fa laptop-code >}} Code

- Use a version control system
- Load libraries/packages explicitly
- Track (and document) software versions
- Namespace functions (if not already required by your coding language)
- Use relative file paths that are operating system-agnostic
- Balance how descriptive object names are with striving for concise names
- _Use comments_ in the code!
- Consider custom functions
- For scripts that need to be run in order, consider adding step numbers to the file name
- 

### {{< fa people-group >}} Contributing

- Create a formal plan for collaborating _with which your whole team agrees_
- Quarantine external inputs
- Plan for "future you"
- Communicate to your collaborators whenever you're working on a specific script to avoid conflicting edits
- 

## Synthesis Considerations

**Estimated time: 10 min**

How does reproducibility in synthesis considerations differ from individual / non-synthesis applications?

- Judgement calls need to be made / agreed to as a group
    - But "defer to the doers"
- Increased emphasis on contribution guidelines / planning being formalized
- More communication needs
- Must ensure that every team member has sufficient access to the project files
- 

# Cleaning and harmonizing data (Greg added - please review)

The scientific questions being asked in synthesis projects are usually broad in scope, and it is therefore common to bring together datasets collected from different sources for analysis. The datasets selected for analysis (source data) may have been collected by different people, in different places, using different methods, as part of different projects... or all of the above. Typically, some amount of data **cleaning** - filtering or removing unwanted observations - and data **harmonization** - putting data together in common structures, file formats, and units of measurement - is necessary before analysis can begin. This process can be easy or difficult depending on the quality of the source data, how different they are, and on how much **metadata** (see callout below) is available to understand them. 

::: {.callout-tip collapse="true"}
## What is metadata?

Metadata is "data about the data," or information that describes **who** collected the data, **what** was observed or measured, **when** the data were collected, **where** the data were collected, **how** the observations or measurements were made, and **why** they were collected. Metadata provide important contextual information about the origin of the data and how they can be analyzed or used. They are most useful when attached or linked to the data being described, and data and related metadata together are commonly referred to as a *dataset*.

Metadata for ecological research data are well described in Michener et al (1997)^[Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2], but there are many other kinds of metadata with different purposes.^[Mayernik, M.S. and Acker, A. (2018), Tracing the traces: The critical role of metadata within networked communications. Journal of the Association for Information Science and Technology, 69: 177-180. https://doi.org/10.1002/asi.23927] When publishing datasets, research data repositories typically provide guidance on metadata needs, as seen [here](https://edirepository.org/resources/creating-metadata-for-publication), for example. 
:::

## Data cleaning and filtering

Usually, not all data in a given dataset are useful. This may be because there are some real or suspected errors in the data, or because they are simply not needed to answer the scientific question being asked (wrong variable, site not of interest, etc.). Data that are not useful are usually excluded from analysis or removed altogether.

There are a variety of ways to approach cleaning and filtering data, and these are beyond the scope of this course. Below are a few guidelines to remember when cleaning and filtering data, followed by some more in-depth resources on the subject

1. Always preserve the raw data
2. Use a scripted workflow to clean and filter the raw data, and follow the usual rules about reproducibility (comments, version control, functionalization)

## Data harmonization 

Data harmonization is the process of bringing different datasets into a common structure or format for analysis.

### A harmonization example

In the figure below, two datasets from different LTER sites have been harmonized into one file for analysis. We don't have all the metadata here, but based on the column naming we can assume that the file on the left (`Konza_harvestplots.txt`, yellow) contains columns for the sampling date, a plot identifier, a treatment categorical value, and measured biomass values. The filename suggests that the data come from the Konza Prarie LTER site. The file on the right (`2022_clips.csv`, blue) has a column denoting the LTER site, in this case Sevilleta LTER (abbreviated as SEV), as well as similar date, plot identifier, treatment, and measured biomass columns. 

::: {.callout-note appearance="simple" icon="false"}
There are some similarities and some differences in these two source files. A harmonized file (`lter_grass_biomass_harmonized.txt`) appears below.

**Can you identify some changes made to data structure, format, or units to harmonize these data?**
:::

<p align="center">
<img src="images/data_harmonization_example.png" width="90%"/>
</p>

Even though the data files were similar, several important changes were made to create the harmonized file. Among them:

- The **site** column was preserved and contains the "SEV" and "KNZ" categorical values denoting which LTER site is observed in each row.
- The dates in the **date** column were converted to a standard format (*YYYY-MM-DD*). Incidentally, this matches the International Organization for Standardization (ISO) recommendation for date/time data, making it generally a good choice for dates.
- A new **rep** column was created to identify the replicate measurements at each site. This is essentially a standard version the plot identifier columns (**PlotID** and **plot**) in the original data file. Also note that the original values are preserved in the new **plot_orig** column.
- The treatment columns from the original data files (**Treatment** and **trt**) were standardized to one column with "C" and "F" categorical values, for control and fertilized treatments, respectively.
- The biomass values from Konza were converted to units grams per meter squared (g/m^2^) because the original Konza measurements were for total biomass in 2x2 meter plots. *Note that this conversion is for illustration purposes only* - we couldn't be sure if this conversion was correct without it being spelled out in the metadata, or asking the data provider directly.

## A word on output formats

The harmonized data format chosen for a synthesis project depends on the source data, analysis plans, and overall project goals. **It makes the best sense to decide on a harmonized data format as a team BEFORE analysis begins, if at all possible.** As a general rule, it is also wise to use a scripted workflow that is as reproducible as possible to accomplish the harmonization you need for your project. Following this guidance lets others understand, check, and adapt your work, and will also make it much, much easier to bring new data into the analysis.

# Version Control

In all scientific research, the data work (cleaning, harmonizing, analyzing) and the writing are iterative processes. They change over time and the resulting dataset, paper, or other output is usually the product of a long series of revisions. In synthesis research, the process can become even more complex because the team is usually large, and multiple people are contributing data, analysis, writing revisions, etc. Using some kind of **version control** helps manage this complexity by recording the changes, tracking individual contributions, and ensuring that something can be rolled-back to an earlier state if needed.

<p align="center">
<img src="images/mod2_final.png" width="80%"/>
</p>

## Vocabulary

**Estimated time: 5 min**

Brief definitions for a selection of fundamental version control vocabulary terms

- Version control system: software that tracks iterative changes to your code and other files
- Repository: the specific folder/directory that is being tracked by a version control system
- Git: a popular open-source distributed version control system
- GitHub: a website that allows users to store their Git repositories online and share them with others 
- 

## GitHub

**Estimated time: 10 min**

While this section of the module focuses on [GitHub](https://github.com/), there are several other viable alternatives for working with Git individually or as part of a larger team (e.g., [GitLab](https://about.gitlab.com/), [GitKraken](https://www.gitkraken.com/), etc.). Any of these may be viable option for your team and we focus on GitHub here only to ensure a standard backdrop for the case studies we'll discuss shortly.

There are a _lot_ of GitHub tutorials that exist already so, rather than add our own variant to the list, we'll work through part of one created by the Scientific Computing team of the [National Center for Ecological Analysis and Synthesis](https://www.nceas.ucsb.edu/) (NCEAS). 

**See the workshop materials [here](https://nceas.github.io/scicomp-workshop-collaborative-coding/github.html).**

Given the time restrictions for this short course, we'll only cover how you engage with GitHub directly through the GitHub website. However, your chosen software for writing code will _certainly_ have a method of connecting to GitHub/etc., so if this topic is of interest it will be beneficial for you to search out the relevant tutorial.

# Synthesis Group Case Studies

**Estimated time: 10 min**

To make some of these concepts more tangible, let's consider some case studies. The following tabs contain GitHub repositories for real teams that have engaged in synthesis research and chosen to preserve and maintain their scripts in GitHub. Each has different strengths and you may find that facets of each feel most appropriate for your group to adopt. There is no single "right" way of tackling this but hopefully parts of these exemplars inspire you.

:::{.panel-tabset}
## Example 1

Highlights:

- Straightforward & transparent numbering of workflow scripts
- Custom `.gitignore` safety net

[https://github.com/lter/lter-sparc-soil-p](https://github.com/lter/lter-sparc-soil-p)

## Example 2

Highlights:

- Really consistent file naming conventions & folder organization
- Active contribution to code base by all group members

[https://github.com/lter/lterwg-flux-gradient](https://github.com/lter/lterwg-flux-gradient)

## Example 3

Highlights:

- Separate repositories for each manuscript
- Nice use of README as pseudo-bookmarks for later reference to other repositories


[https://github.com/lter/lterwg-silica-data](https://github.com/lter/lterwg-silica-data)

:::

For more information about LTER synthesis working groups and how you can get involved in one, click [here](https://lternet.edu/synthesis/). 

# Additional Resources

## Courses, Workshops, and Tutorials

- [Synthesis Skills for Early Career Researchers](https://lter.github.io/ssecr/) (SSECR) course. **2024**. LTER Network Office
- [Reproducible Approaches to Arctic Research Using R](https://learning.nceas.ucsb.edu/2024-02-arctic/) workshop. **2024**. Arctic Data Center & NCEAS Learning Hub
- [Collaborative Coding with GitHub](https://nceas.github.io/scicomp-workshop-collaborative-coding/) workshop. **2024**. NCEAS Scientific Computing team
- [Coding in the Tidyverse](https://nceas.github.io/scicomp-workshop-tidyverse/) workshop. **2023**. NCEAS Scientific Computing team
- [Shiny Apps for Sharing Science](https://njlyon0.github.io/asm-2022_shiny-workshop/) workshop. **2022**. Lyon, N.J. _et al._
- [Ten Commandments for Good Data Management](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/). **2016**. McGill, B.

## Literature

- Harrer, M. _et al._ [Doing Meta-Analysis with R: A Hands-On Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/). **2023**. _GitHub_
- Todd-Brown, K.E.O., _et al._ [Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential](https://bg.copernicus.org/articles/19/3505/2022/). **2022**. _Biogeosciences_
- Borer, E.T. _et al._ [Some Simple Guidelines for Effective Data Management](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/0012-9623-90.2.205). **2009**. _Ecological Society of America Bulletin_


