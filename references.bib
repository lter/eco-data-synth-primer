@article{gries_environmental_2023,
	title = {The {Environmental} {Data} {Initiative}: {Connecting} the past to the future through data reuse},
	volume = {13},
	issn = {2045-7758},
	shorttitle = {The {Environmental} {Data} {Initiative}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.9592},
	doi = {10.1002/ece3.9592},
	abstract = {The Environmental Data Initiative (EDI) is a trustworthy, stable data repository, and data management support organization for the environmental scientist. In a bottom-up community process, EDI was built with the premise that freely and easily available data are necessary to advance the understanding of complex environmental processes and change, to improve transparency of research results, and to democratize ecological research. EDI provides tools and support that allow the environmental researcher to easily integrate data publishing into the research workflow. Almost ten years since going into production, we analyze metadata to provide a general description of EDI's collection of data and its data management philosophy and placement in the repository landscape. We discuss how comprehensive metadata and the repository infrastructure lead to highly findable, accessible, interoperable, and reusable (FAIR) data by evaluating compliance with specific community proposed FAIR criteria. Finally, we review measures and patterns of data (re)use, assuring that EDI is fulfilling its stated premise.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {Ecology and Evolution},
	author = {Gries, Corinna and Hanson, Paul C. and O'Brien, Margaret and Servilla, Mark and Vanderbilt, Kristin and Waide, Robert},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.9592},
	keywords = {data reuse, environmental data repository, FAIR data, metadata, open science},
	pages = {e9592},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2024-11-05},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Publication characteristics, Research data},
	pages = {160018},
	file = {Full Text PDF:/Users/gmaurer/Zotero/storage/KJFG7SVW/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:application/pdf},
}

@misc{noauthor_how_nodate,
	title = {How to {GO} {FAIR}},
	url = {https://www.go-fair.org/how-to-go-fair/},
	abstract = {How to GO FAIR Since its beginning in early 2018, the GO FAIR community has been working towards implementations of the FAIR Guiding Principles. This collective effort has resulted in a three-point framework that formulates the essential steps towards the… Continue reading →},
	language = {en-US},
	urldate = {2025-07-01},
	journal = {GO FAIR},
	file = {Snapshot:/Users/gmaurer/Zotero/storage/NH3C9UZJ/how-to-go-fair.html:text/html},
}

@misc{noauthor_fair_nodate,
	title = {{FAIR} {Cookbook}},
	url = {https://fairplus.github.io/the-fair-cookbook/content/home.html},
	abstract = {An online, open and live resource for the Life Sciences with recipes that help you to make and keep data Findable, Accessible, Interoperable and Reusable; in one word FAIR.},
	urldate = {2025-07-01},
	file = {Snapshot:/Users/gmaurer/Zotero/storage/ZAW8BXGN/home.html:text/html},
}

@article{bahim_fair_2020,
	title = {The {FAIR} {Data} {Maturity} {Model}: {An} {Approach} to {Harmonise} {FAIR} {Assessments}},
	volume = {19},
	issn = {1683-1470},
	shorttitle = {The {FAIR} {Data} {Maturity} {Model}},
	url = {https://datascience.codata.org/articles/10.5334/dsj-2020-041},
	doi = {10.5334/dsj-2020-041},
	abstract = {In the past years, many methodologies and tools have been developed to assess the FAIRness of research data. These different methodologies and tools have been based on various interpretations of the FAIR principles, which makes comparison of the results of the assessments difficult. The work in the RDA FAIR Data Maturity Model Working Group reported here has delivered a set of indicators with priorities and guidelines that provide a ‘lingua franca’ that can be used to make the results of the assessment using those methodologies and tools comparable. The model can act as a tool that can be used by various stakeholders, including researchers, data stewards, policy makers and funding agencies, to gain insight into the current FAIRness of data as well as into the aspects that can be improved to increase the potential for reuse of research data. Through increased efficiency and effectiveness, it helps research activities to solve societal challenges and to support evidence-based decisions. The Maturity Model is publicly available and the Working Group is encouraging application of the model in practice. Experience with the model will be taken into account in the further development of the model.},
	language = {en-US},
	number = {1},
	urldate = {2025-07-01},
	journal = {Data Science Journal},
	author = {Bahim, Christophe and Casorrán-Amilburu, Carlos and Dekkers, Makx and Herczog, Edit and Loozen, Nicolas and Repanas, Konstantinos and Russell, Keith and Stall, Shelley},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/gmaurer/Zotero/storage/W6ZCK8AM/Bahim et al. - 2020 - The FAIR Data Maturity Model An Approach to Harmo.pdf:application/pdf},
}

@article{michener_nongeospatial_1997,
	title = {Nongeospatial {Metadata} for the {Ecological} {Sciences}},
	volume = {7},
	copyright = {© 1997 by the Ecological Society of America},
	issn = {1939-5582},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1890/1051-0761%281997%29007%5B0330%3ANMFTES%5D2.0.CO%3B2},
	doi = {10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2},
	abstract = {Issues related to data preservation and sharing are receiving increased attention from scientific societies, funding agencies, and the broad scientific community. Ecologists, for example, are increasingly using data collected by other scientists to address questions at broader spatial, temporal, and thematic scales (e.g., global change, biodiversity, sustainability). No data set is perfect and self-explanatory. Ecologists must, therefore, rely upon a set of instructions or documentation to acquire a specific data set, determine its suitability for meeting specific research objectives, and accurately interpret results from subsequent processing, analysis, and modeling. “Metadata” represent the set of instructions or documentation that describe the content, context, quality, structure, and accessibility of a data set. Although geospatial metadata standards have been developed and widely endorsed by the geographical science community, such standards do not yet exist for the ecological sciences. In this paper, we examine potential benefits and costs associated with developing and implementing metadata for nongeospatial ecological data. We present a set of generic metadata descriptors that could serve as the basis for a “metadata standard” for nongeospatial ecological data. Alternative strategies for metadata implementation that meet differing organizational or investigator-specific objectives are presented. Finally, we conclude with several recommendations related to future development and implementation of ecological metadata.},
	language = {en},
	number = {1},
	urldate = {2025-07-01},
	journal = {Ecological Applications},
	author = {Michener, William K. and Brunt, James W. and Helly, John J. and Kirchner, Thomas B. and Stafford, Susan G.},
	year = {1997},
	note = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1890/1051-0761\%281997\%29007\%5B0330\%3ANMFTES\%5D2.0.CO\%3B2},
	keywords = {data archive, data lineage, data management, information science, metadata, quality assurance},
	pages = {330--342},
	file = {Snapshot:/Users/gmaurer/Zotero/storage/BPSWYEU2/1051-0761(1997)007[0330NMFTES]2.0.html:text/html},
}

@misc{noauthor_provenance_nodate,
	title = {Provenance {Metadata}},
	url = {https://edirepository.org/resources/provenance-metadata},
	urldate = {2025-07-01},
	file = {Provenance Metadata:/Users/gmaurer/Zotero/storage/UXTQEM9B/provenance-metadata.html:text/html},
}

@misc{noauthor_end--end-provenancegithubio_nodate,
	title = {End-to-end-provenance.github.io},
	url = {http://end-to-end-provenance.github.io/},
	language = {en-US},
	urldate = {2025-07-01},
	journal = {End-to-end-provenance.github.io},
	file = {Snapshot:/Users/gmaurer/Zotero/storage/KAGEZDJM/end-to-end-provenance.github.io.html:text/html},
}

@article{lerner_making_2023,
	title = {Making {Provenance} {Work} for {You}},
	volume = {14},
	issn = {2073-4859},
	url = {https://rjournal.github.io/},
	abstract = {To be useful, scientific results must be reproducible and trustworthy. Data provenance---the history of data and how it was computed---underlies reproducibility of, and trust in, data analyses. Our work focuses on collecting data provenance from R scripts and providing tools that use the provenance to increase the reproducibility of and trust in analyses done in R. Specifically, our "End-to-end provenance tools" ("E2ETools") use data provenance to: document the computing environment and inputs and outputs of a script's execution; support script debugging and exploration; and explain differences in behavior across repeated executions of the same script. Use of these tools can help both the original author and later users of a script reproduce and trust its results.},
	number = {4},
	urldate = {2025-07-01},
	journal = {The R Journal},
	author = {Lerner, Barbara and Boose, Emery and Brand, Orenna and Ellison, Aaron M. and Fong, Elizabeth and Lau, Matthew and Ngo, Khanh and Pasquier, Thomas and Perez, Luis A. and Seltzer, Margo and Sheehan, Rose and Wonsil, Joseph},
	month = feb,
	year = {2023},
	pages = {141--159},
	file = {Full Text PDF:/Users/gmaurer/Zotero/storage/TKZNX8XQ/Lerner et al. - 2023 - Making Provenance Work for You.pdf:application/pdf},
}

@article{white_developing_2019,
	title = {Developing an automated iterative near-term forecasting system for an ecological study},
	volume = {10},
	copyright = {© 2018 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13104},
	doi = {10.1111/2041-210X.13104},
	abstract = {Most forecasts for the future state of ecological systems are conducted once and never updated or assessed. As a result, many available ecological forecasts are not based on the most up-to-date data, and the scientific progress of ecological forecasting models is slowed by a lack of feedback on how well the forecasts perform. Iterative near-term ecological forecasting involves repeated daily to annual scale forecasts of an ecological system as new data becomes available and regular assessment of the resulting forecasts. We demonstrate how automated iterative near-term forecasting systems for ecology can be constructed by building one to conduct monthly forecasts of rodent abundances at the Portal Project, a long-term study with over 40 years of monthly data. This system automates most aspects of the six stages of converting raw data into new forecasts: data collection, data sharing, data manipulation, modelling and forecasting, archiving, and presentation of the forecasts. The forecasting system uses R code for working with data, fitting models, making forecasts, and archiving and presenting these forecasts. The resulting pipeline is automated using continuous integration (a software development tool) to run the entire pipeline once a week. The cyberinfrastructure is designed for long-term maintainability and to allow the easy addition of new models. Constructing this forecasting system required a team with expertise ranging from field site experience to software development. Automated near-term iterative forecasting systems will allow the science of ecological forecasting to advance more rapidly and provide the most up-to-date forecasts possible for conservation and management. These forecasting systems will also accelerate basic science by allowing new models of natural systems to be quickly implemented and compared to existing models. Using existing technology, and teams with diverse skill sets, it is possible for ecologists to build automated forecasting systems and use them to advance our understanding of natural systems.},
	language = {en},
	number = {3},
	urldate = {2025-07-01},
	journal = {Methods in Ecology and Evolution},
	author = {White, Ethan P. and Yenni, Glenda M. and Taylor, Shawn D. and Christensen, Erica M. and Bledsoe, Ellen K. and Simonis, Juniper L. and Ernest, S. K. Morgan},
	year = {2019},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13104},
	keywords = {forecasting, iterative forecasting, mammals, Portal Project, prediction},
	pages = {332--344},
	file = {Full Text PDF:/Users/gmaurer/Zotero/storage/W47IBUL3/White et al. - 2019 - Developing an automated iterative near-term foreca.pdf:application/pdf;Snapshot:/Users/gmaurer/Zotero/storage/A5MFCGKW/2041-210X.html:text/html},
}

@article{kim_implementing_2022,
	title = {Implementing {GitHub} {Actions} continuous integration to reduce error rates in ecological data collection},
	volume = {13},
	copyright = {© 2022 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13982},
	doi = {10.1111/2041-210X.13982},
	abstract = {Accurate field data are essential to understanding ecological systems and forecasting their responses to global change. Yet, data collection errors are common, and data analysis often lags far enough behind its collection that many errors can no longer be corrected, nor can anomalous observations be revisited. Needed is a system in which data quality assurance and control (QA/QC), along with the production of basic data summaries, can be automated immediately following data collection. Here, we implement and test a system to satisfy these needs. For two annual tree mortality censuses and a dendrometer band survey at two forest research sites, we used GitHub Actions continuous integration (CI) to automate data QA/QC and run routine data wrangling scripts to produce cleaned datasets ready for analysis. This system automation had numerous benefits, including (1) the production of near real-time information on data collection status and errors requiring correction, resulting in final datasets free of detectable errors, (2) an apparent learning effect among field technicians, wherein original error rates in field data collection declined significantly following implementation of the system, and (3) an assurance of computational reproducibility—that is, robustness of the system to changes in code, data and software. By implementing CI, researchers can ensure that datasets are free of any errors for which a test can be coded. The result is dramatically improved data quality, increased skill among field technicians, and reduced need for expert oversight. Furthermore, we view CI implementation as a first step towards a data collection and analysis pipeline that is also more responsive to rapidly changing ecological dynamics, making it better suited to study ecological systems in the current era of rapid environmental change.},
	language = {en},
	number = {11},
	urldate = {2025-07-01},
	journal = {Methods in Ecology and Evolution},
	author = {Kim, Albert Y. and Herrmann, Valentine and Barreto, Ross and Calkins, Brianna and Gonzalez-Akre, Erika and Johnson, Daniel J. and Jordan, Jennifer A. and Magee, Lukas and McGregor, Ian R. and Montero, Nicolle and Novak, Karl and Rogers, Teagan and Shue, Jessica and Anderson-Teixeira, Kristina J.},
	year = {2022},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13982},
	keywords = {continuous integration, ecological forecasting, field research, forest ecology, Forest Global Earth Observatory, GitHub Actions, quality assurance and quality control, reproducibility},
	pages = {2572--2585},
	file = {Full Text PDF:/Users/gmaurer/Zotero/storage/MGGWHLTH/Kim et al. - 2022 - Implementing GitHub Actions continuous integration.pdf:application/pdf;Snapshot:/Users/gmaurer/Zotero/storage/JZF33PXK/2041-210X.html:text/html},
}

@article{currier_precipitation_2022,
	title = {Precipitation versus temperature as phenology controls in drylands},
	volume = {103},
	copyright = {© 2022 The Ecological Society of America.},
	issn = {1939-9170},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3793},
	doi = {10.1002/ecy.3793},
	abstract = {Cycles of plant growth, termed phenology, are tightly linked to environmental controls. The length of time spent growing, bounded by the start and end of season, is an important determinant of the global carbon, water, and energy balance. Much focus has been given to global warming and consequences for shifts in growing-season length in temperate regions. In conjunction with warming temperatures, altered precipitation regimes are another facet of climate change that have potentially larger consequences than temperature in dryland phenology globally. We experimentally manipulated incoming precipitation in a semiarid grassland for over a decade and recorded plant phenology at the daily scale for 7 years. We found precipitation to have a strong relationship with the timing of grass greenup and senescence but temperature had only a modest effect size on grass greenup. Pre-season drought strongly resulted in delayed grass greenup dates and shorter growing-season lengths. Spring and summer drought corresponded with earlier grass senescence, whereas higher precipitation accumulation over these seasons corresponded with delayed grass senescence. However, extremely wet conditions diluted this effect and caused a plateaued response. Deep-rooted woody shrubs showed few effects of variable precipitation or temperature on phenology and displayed consistent annual phenological timing compared with grasses. Whereas rising temperatures have already elicited phenological consequences and extended growing-season length for mid and high-latitude ecosystems, precipitation change will be the major driver of phenological change in drylands that cover 40\% of the land surface with consequences for the global carbon, water, and energy balance.},
	language = {en},
	number = {11},
	urldate = {2025-07-01},
	journal = {Ecology},
	author = {Currier, Courtney M. and Sala, Osvaldo E.},
	year = {2022},
	note = {\_eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecy.3793},
	keywords = {climate change, drylands, phenology, precipitation, temperature},
	pages = {e3793},
	file = {Snapshot:/Users/gmaurer/Zotero/storage/8EMCNFQG/ecy.html:text/html},
}

@article{obrien_ecocomdp_2021,
	title = {{ecocomDP}: {A} flexible data design pattern for ecological community survey data},
	volume = {64},
	issn = {1574-9541},
	shorttitle = {{ecocomDP}},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954121001655},
	doi = {10.1016/j.ecoinf.2021.101374},
	abstract = {The idea of harmonizing data is not new. Decades of amassing data in databases according to community standards - both locally and globally - have been more successful for some research domains than others. It is particularly difficult to harmonize data across studies where sampling protocols vary greatly and complex environmental conditions need to be understood to apply analytical methods correctly. However, a body of long-term ecological community observations is increasingly becoming publicly available and has been used in important studies. Here, we discuss an approach to preparing harmonized community survey data by an environmental data repository, in collaboration with a national observatory. The workflow framework and repository infrastructure are used to create a decentralized, asynchronous model to reformat data without altering original data through cleaning or aggregation, while retaining metadata about sampling methods and provenance, and enabling programmatic data access. This approach does not create another data ‘silo’ but will allow the repository to contribute subsets of available data to a variety of different analysis-ready data preparation efforts. With certain limitations (e.g., changes to the sampling protocol over time), data updates and downstream processing may be completely automated. In addition to supporting reuse of community observation data by synthesis science, a goal for this harmonization and workflow effort is to contribute these datasets to the Global Biodiversity Information Facility (GBIF) to increase the data's discovery and use.},
	urldate = {2025-07-01},
	journal = {Ecological Informatics},
	author = {O'Brien, Margaret and Smith, Colin A. and Sokol, Eric R. and Gries, Corinna and Lany, Nina and Record, Sydne and Castorani, Max C. N.},
	month = sep,
	year = {2021},
	keywords = {Data harmonization, ecocomDP, Ecological community survey, LTER, LTREB, NEON, Workflow},
	pages = {101374},
	file = {ScienceDirect Snapshot:/Users/gmaurer/Zotero/storage/N9RQ6TNY/S1574954121001655.html:text/html},
}