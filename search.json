[
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nRecognize the importance of establishing group norms and expectations\nEvaluate strategies for group organization and project management\n\nWorkshop Slides - Under Construction\nGoogle Slides link\nDefine Synthesis\nFor the purposes of today’s discussion, we define synthesis as:\n\nBringing together the results of multiple studies to test novel hypotheses–usually with a team of people."
  },
  {
    "objectID": "module1.html#learning-objectives",
    "href": "module1.html#learning-objectives",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nRecognize the importance of establishing group norms and expectations\nEvaluate strategies for group organization and project management\n\nWorkshop Slides - Under Construction\nGoogle Slides link\nDefine Synthesis\nFor the purposes of today’s discussion, we define synthesis as:\n\nBringing together the results of multiple studies to test novel hypotheses–usually with a team of people."
  },
  {
    "objectID": "module1.html#why-synthesis",
    "href": "module1.html#why-synthesis",
    "title": "Starting with Team Science",
    "section": "Why Synthesis?",
    "text": "Why Synthesis?\n\nThis process–of bringing diverse expertise together to combine existing primary data–definitely has its challenges. Ideniftying and engaging the necessary combination of skills and experience is challenging. Scheduling meeting times can be difficult. Commitment to the process can be uneven. Nonetheless, virtually all who have experienced it find it to be a deeply rewarding experience. Why?\n\nThe products emerging from synthesis typically have higher impact (as defined by both academic and applied measures)\nThe process allows researchers to access and incorporate skills that they don’t (yet) have themselves.\nWorking groups help early career researchers build their science networks.\nKeep experienced researchers fresh and engaging with new ideas\nBuilds on existing investments of the science community by re-using data\nOffers a way to involve individuals who can’t or don’t want to do fieldwork in original research and expands opportunity to less research-intensive institutions.\n\n\nAdditional Resources\n\nAdam, 2023. ‘Disruptive’ science: in-person teams make more breakthroughs than remote groups\nHackett, 2020. Collaboration and Sustainability: Making Science Useful, Making Useful Science\nHampton and Parker, 2011. Collaboration and Productivity in Scientific Synthesis\nHackett et al., 2021. Do synthesis centers synthesize? A semantic analysis of topical diversity in research\nWyborn et al., 2018. Understanding the Impacts of Research Synthesis"
  },
  {
    "objectID": "module1.html#process-overview",
    "href": "module1.html#process-overview",
    "title": "Starting with Team Science",
    "section": "Process Overview",
    "text": "Process Overview\nTypically, a group of researchers–or researchers and managers or community members–will plan a series of meetings over 2-3 years. The mix of in-person v. virtual meetings and work will vary across different groups and different funders, but the general pattern is similar.\n\n\n\nEarly meetings focus on narrowing the questions and deciding what data is needed and what analyses will be most useful. A period of data gathering and assembly comes next. The assembly of data almost always prompts a revision of the initial questions, as data rarely comes in exactly the form that researchers expected. This can be both the most frustrating and the most interesting part of the process as new hypotheses and models are floated and discussed. It is especially important to have the full participation of researchers familiar with different fields and ecosystems in this process.\nWith tractable questions refined, the group will move into analysis mode. Often, a few individuals will do most of the data wrangling and coding, but will need continuous input on analytical decisions. In our experience, GitHub issues is one very good tool for facilitating and recording these decisions. But the “best” tool will be the one that most members of the team are most comfortable with.\nLater meetings will focus on developing manuscripts and/or application-related products such as white papers and decision support tools.\nHow to get Involved\nOften, early career researchers will be excited about the idea of synthesis but be unsure how to connect with existing or nascent synthesis efforts. Here are a few ideas for how to make yourself available and valuable to synthesis groups.:\n\nMake it known you want to be involved in synthesis\n\nLet your advisor know\nShare your enthusiasm\n\nSkill building:\n\nSynthesis Skills for Early Career Researchers: SSECR\nData Carpentries\nESIIL: innovation summit, hackathons\nEnvironmental Data Science Summit\n\nBuild your community\n\nAsk questions at meetings\nInitiate conversations\n\nStart your own!\n\nIdentifying a Synthesis-Ready Question\nLots of questions are interesting, but not terribly well-suited for a synthesis approach. We’ve learned through experience that there are a few qualities that make some questions a better fit for a) combining data; and b) work by a group. The main qualities that we seek in synthesis projects include:\n\nNovel and interesting enough to keep you engaged for 2-3 years.\nData already exist and you know (at least generally) where to find it\nQuestions cover a large geographic area or data that aren’t normally collected or analyzed together\nClearly framed, but flexible enough to allow adaptation through the process\nResponsive to the funding call\nOutputs could include several kinds of products. As the project progresses, gard students will become postdocs, postdocs will get faculty positions. For the project to remain satisfying to all participants, people will need to be able to take a leadership role on different kinds of products.\n\nPapers (including data papers, perspectives, gap analyses, as well as primary analyses)\nSymposia\nDatasets\nAnalytical Packages"
  },
  {
    "objectID": "module1.html#sources-of-support-for-synthesis",
    "href": "module1.html#sources-of-support-for-synthesis",
    "title": "Starting with Team Science",
    "section": "Sources of Support for Synthesis",
    "text": "Sources of Support for Synthesis\nWhile it is certainly possible to conduct synthesis with no external support, a bit of funding will allow your group to travel to meet up in person and can, in some cases, provide salary support for postdocs, grad students or or assistance with analysis.\n\n\n\n\n\n\nNote\n\n\n\n\n\nDiscuss: What funding sources support synthesis work?\nSources of Funding and Support\nSynthesis Centers\n\nEnvironmental Science Innovation and Inclusion Laboratory (ESIIL)\nNational Center for Ecological Analysis and Synthesis (NCEAS)\n\nMorpho Program\n\nUSGS Powell Center\nS-div\nCanadian Institute of Ecology and Evolution (CIEE)\n\nSocieties\n\nNew Phytologist Workshops\nGordon Research Conferences\nChapman Conferences\nBritish Ecological Society\n\none-third of participants from developing world\n\n\nNSF Programs\n\nULTRA-data Dear Colleague letter\nNSF Core Programs, e.g. Division of Environmental Biology: “synthesis activities”, “synthesis projects”\nNSF workshops"
  },
  {
    "objectID": "module1.html#building-a-team",
    "href": "module1.html#building-a-team",
    "title": "Starting with Team Science",
    "section": "Building a Team",
    "text": "Building a Team\n\nThe Leadership Team\nThe composition of the leadership team will affect the success of the project and who you will be able to recruit to the larger group. Look for:\n\nDifferent (and complementary) areas of expertise\nComplementary professional networks\nFacilitation skills\nEmotional Intelligence\n\n\n\nThe Broader Team\nIn our experience at NCEAS and the LTER Network Office, we’ve found teams of up to 10 to 15 people to be optimal for synthesis work. As individuals, we all have strengths and weaknesses. The beauty of working in teams is that you can invite people who offset your own weaknesses and who bring strengths you don’t have. Often, you’ll have a few core team members who have generated a synthesis idea, but then you’ll want to take a clear-eyed look at what additional skills and qualities to invite. When you do so, be sure to consider:\n\nSkills, Aptitudes, and Communication Styles\n\nLook for a mix of empiricists, theorists, and modellers\nBig-picture thinkers, organizers, task-oriented do-ers\nDeep thinkers and risk-takers\nAt least some skilled coders\n\nCareer stage\n\nSenior investigators connect the team to existing literature and fields of study, connect to a broad network of experienced researchers, and have good knowledge of resources, but are often have a very limited amount of time to devorte to discussion and analyses\nJunior team members often bring a fresh perspective, familiarity with newer literature, strong coding skills, and time to devote to the project\n\nEmotional intelligence\n\nResearch shows (add ref) that the bump in creativity seen in mixed-gender teams is typically due to an increase in emotional intelligence and attention to team dynamics. Include at least a few people with a process orientation and strong people skills.\n\nPower dynamics\n\nYou won’t be able to anticipate all of the issues related to power dynamics that can arise, but keep them front of mind as you assemble a team.\n\nRemember that participation in synthesis represents a significant career opportunity\n\nBe mindful that such career-building opportunities have not been fairly distributed\nBe intentional seeking out people who may not be part of your typical circles (including gender, ethnicity, career stage, family status, (dis)abilities, etc.)\n\n\nAdditional Resources\nHorowitz, S.K. and Horwitz, I.B. (2007) The effects of team diversity on team outcomes: A meta-analytic review of team demography. Journal of Management 33: 987-1015. DOI: 10.1177/0149206307308587 Aggarwal, I. & Woolley, A.W. (2018) Team Creativity, Cognition, and Cognitive Style Diversity. Management Science 65(4):1586-1599. DOI: 10.1287/mnsc.2017.3001"
  },
  {
    "objectID": "module1.html#setting-expectations",
    "href": "module1.html#setting-expectations",
    "title": "Starting with Team Science",
    "section": "Setting Expectations",
    "text": "Setting Expectations\nIn any team project, people have different reasons for wanting to participate - and different anxieties about what “participation” will mean. For you, getting a high profile paper may be the most important thing. For others, it may be expanding their network or a chance to practice new skills. Being transparent about those goals, and the behaviors that support them, makes it easier to resolve tensions when they arise.\n\n\n\n\nImportance of various benefits to working group participants\n\n\n\n\n\n\n\nTime Commitments\n\n\n\nWe all misjudge our availability once in a while, but consistently failing to deliver on commitments disrupts others’ work plans and is a major source of group dissatisfaction. Get buy-in for commitments and plan for both reminders and accountability.\n\n\nThere are many approaches to establishing group norms, but a shared process that helps create ownership and buy-in is one key to a smoothly-functioning working group.\nThe process can be as simple as taking 15 minutes to ask the group about their shared (and diverging) values and what those imply about how the group should function. The best choice for your group will depend on the mix of participants, the nature of the content, and the duration of your collaboration.\n\n\n\n\n\n\nFrom Biodiversity on a Changing Planet working group (PI: Peter Adler)\n\n\n\n\n\nThis version is simplest and most appropiate when your group has some shared history and mainly needs a reminder to attend to their share values. Even so, be sure to leave enough time and space for participants to add new ideas or concerns. Offer some basic starting point values and norms, then ask the group to add any that haven’t been raised yet. Record the results and return to them at the start of meetings.\n\nGroup Values\n\nInclusion\nCreative Thinking\nTeamwork\nAccountability\nFun\n\nWhat else?\n\n\n\n\n\n\n\n\n\n\nFrom Response Diversity Network Workshop\n\n\n\n\n\nThis version is a little more involved and directive with resepct to behaviors as well as values. Edit, add, or delete suggestions depending on any concerns in your group.\n\nCommunity Rules for Inclusive and Productive Discussions\n\nWe are all responsible for cultivating a respectful and inclusive atmosphere to benefit from our diverse community of participants.\nListen with curiosity and resilience, not judgement.\nAssume the best of intentions.\nAllow others to participate and avoid dominating the conversation.\nAccept/meet people where they are.\nBring a spirit of generosity (for yourself and others).\nBe kind to yourself.\nSupport learning.\nBe attentive to power and privilege.\nEnjoy yourself!\n\n\n\n\n\n\n\n\n\n\n\nFrom Entering Mentoring (CIMER)\n\n\n\n\n\nFor longer collaborations with more challenging power dynamics, it may be worth engaging in a slightly more involved process. In workshops run by the Center for Improved Mentoring of Experiences in Research (CIMER), a facilitator presents a set of group behaviors categorized as group-oriented or self-oriented.\n\n\n\nBreak onto small groups and discuss the following questions:\n\nWhat are some strategies to maximize group-oriented & minimize self-oriented behaviors?\nAre there special considerations for an online space? How can group dynamics be different in an online vs. face-to-face environment?\nHow can we leverage our knowledge of group behaviors as we work together during this workshop?\n\nThe exercise presents an opportunity for self-reflection and for participants to commit to (and ask for help in) curbing their unhelpful behaviors. After all, we all have them!"
  },
  {
    "objectID": "module1.html#making-a-communication-and-work-plan",
    "href": "module1.html#making-a-communication-and-work-plan",
    "title": "Starting with Team Science",
    "section": "Making a Communication and Work Plan",
    "text": "Making a Communication and Work Plan\nGood teams are both chosen and made. Diversity on teams uncovers novel approaches, perspectives, and insights and it can slow the pace and cause misunderstandings that highlight unexamined assumptions. As a synthesis team leader (and even as a participant), there are many things you can do to create opportunities for everyone to contribute their best thinking, learn from one another, and feel heard and respected.\nAs the group gets started\n\nCreate a shared vision for your group\n\nMake sure everyone starts on the same footing with a brief overview of the context for the group and the questions you’re starting with\n\nCo-develop group norms. It builds ownership of shared norms.\nConduct a “pre-mortem” to talk about worries and visions for a healthy group dynamic\nRecognize who you are as a group, culturally, and any power dynamics that might entail\n\nSometimes, simply articulating the potential for oppressive power dynamics can give group members the confidence to assert themselves\n\n\nThroughout the process\n\nGet to know each other\n\nUse creative icebreakers to break a pattern of silence\nInvest in “social time”\n\nValue and accommodate various styles of contribution\n\nFast and slow thinkers,\nVisual, auditory and kinesthetic learners,\nSynchronous and asynchronous contributions\n\nPractice “cultural norming” by educating participants on systemic/structural oppression and racism and ways to work against our own implicit biases\n\nAt intervals\n\nWhen you find yourself questioning whether a practice or activity is still valuable, ask each member for a quick read on whether the group should “Start, Stop, or Continue” the activity"
  },
  {
    "objectID": "module1.html#intro-to-establishing-a-project-management-plan",
    "href": "module1.html#intro-to-establishing-a-project-management-plan",
    "title": "Starting with Team Science",
    "section": "Intro to Establishing a Project Management Plan",
    "text": "Intro to Establishing a Project Management Plan\nAs your group gets started working together, it it easy to assume that you will use the tools and planning strategies that the PI or project organizer is used to using. That information should carry some weight. They will devote a lot of time to the project. But also try to survey the group at an early meeting so that you know which platforms other group members use. Make decisions based on balancing platforms that will allow maximum group participation with those that will make the work easiest for those likely to be doing the work.\nConsider:\n\nHow will you communicate? (Email, Slack, Google Group, Discourse, Discord, Zulip…) Who maintains the list, and how?\n\nA regular “update” email (every 2 weeks or once a month) is a great practice\nYou will likely generate lots of ideas for papers and other products. How do group members who didn’t happen to be in that conversation find out about them?\n\nWhere will you share common documents and how will you organize them? (Box, Dropbox, Google Drive, Other…email them around :eye_roll:)\nHow will you keep track of relevant references (a file folder of pdfs?, Zotero, EndNote, Mendeley…)? Each has strengths and weaknesses–consider how participants are most likely to contribute and how easy they will be to fnd later.\n\nDo you need a shared calendar or virtual bulletin board? Consider Google Calendar, GitHub Pages, Quarto, or a simple WordPress or Weebly site, but beware of the time it can consume.\nWho is doing most of the coding?\n\nWhat platfoms do they use?\nDo others need to see/weigh in on the code? Can they access those platforms? How will they get notice that their input is needed?\n\nHow do you write together? The platform matters, but so does the process.\n\nSome groups outline as a team and then assign sections to different writers.\nIn others, everyone contributes figures, and concepts as bullet points, but a single author crafts the actual prose.\nSuggesting and track changes features are great for modest edits on a nearly completed document, but can be overwhelming when a paper is still in development.\n\nAn alternative is to create a read-only version of a document with line numbers and ask for comments by line number.\nIt is also helpful to be clear about what kind of input you are after at each stage of writing. Are you just trying to get the analysis clear or do you want wordcraft?"
  },
  {
    "objectID": "module1.html#data-sources",
    "href": "module1.html#data-sources",
    "title": "Starting with Team Science",
    "section": "Data Sources",
    "text": "Data Sources\nSome sources of data–such as modern remote sensing products, NEON data, and census data–have very clear, explicit ways to access and download them or work with them in the cloud. But the most interesting synthesis questions often involve combining such “big” data with other data sources that may have been collected manually, by a variety of methods and different technicians, over decades.\n\nDiscussion QuestionA Few Ideas\n\n\n\nWhat kinds of data sources might you consider including in a synthesis project, in addition to your own or others’ field data?\n\n\n\n\nDataONE\nEnvironmental Data Initiative (EDI)\nGenBank (NCBI)\nNational Ecological Observatory Network (NEON)\nUS Geological Survey Data\nGlobal Biodiversity Information Facility (GBIF)\nNASA Remote Sensing Data\nUS Park Service\nFluxNet\nPhenocam network\niNaturalist\neBird\nCensus data\nData extracted from papers\nScraping social media\nText analysis\n…."
  },
  {
    "objectID": "module1.html#data-use-principles",
    "href": "module1.html#data-use-principles",
    "title": "Starting with Team Science",
    "section": "Data Use Principles",
    "text": "Data Use Principles\nThere are a few ethical and practical guidelines that will save you a lot of trouble if you can adhere to them from the start of a project.\n\nKeep track of your data sources (sources, permissions, notes, related metadata, what’s included)\nData sources should always be cited\nCommunicate with data creators whenever possible\n\nThis doesn’t need to be onerous and it can uncover issues and opportunities associated with data sources.\n\n\n\n\n\n\n\n\nSample data author outreach email for public dataset:\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include your dataset titled “xxx” (doi: xxx). We have downloaded the data from yyy repository, but wanted to let you know we are using it and to inquire whether there is any additional context we should be aware of or related datasets we should be sure to include. A short description of the synthesis project follows. Please let me know by xxx date if you have any questions or concerns with our use of this data.\nThank you,\n\n\n\n\n\n\n\n\n\n\nSample data author outreach email for unpublished dataset:\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include the dataset behind your paper titled {xxx} (doi:{yyy}), which seems highly relevant. Would it be possible to obtain the data? Ideally, we would access it through through a public repository, such as the Environmental Data Initiative, which offers assistance in curation and submission of datasets. Either way, we would credit you as the data originator and want you know we are using it. If there is any additional context we should be aware of or related datasets we should be sure to include, please let us know. A short description of the synthesis project follows.\nThank you,\n\n\n\n\n\n\nAuthorship Discussion\n\nShould all data contributors be offered authorship? How would you handle a data creator who demanded authorship in order to use their data?\n\n\n\nA Few Considerations\nThere are no pat answers for this situation, but having agreed-on authorship guidelines is really valuable when it comes up. We’ll cover that in more detail soon, but for now, there are a few questions to ask yourself.\n\nAre they really committed enough to join the working group and contribute to the papers? If so, it may be a good investment.\nHow much work will they need to put in to make the data ready?\nHow critical is this particular data source for your analysis?\nYou will need to make your derived dataset public. Are they placing conditions on the use of their data that make that impossible?"
  },
  {
    "objectID": "module1.html#keeping-track-of-data",
    "href": "module1.html#keeping-track-of-data",
    "title": "Starting with Team Science",
    "section": "Keeping Track of Data",
    "text": "Keeping Track of Data\nWe’ve pulled together a few of the forms that we have use to keep track of the data that synthesis groups plan to use…\nSample spreadsheet for initial data surveying\n\nOnce you get into the details of the process, you’ll want to track some more specific information, but you’ll hear more about that in Module 2.\n\nURL to the data (and metadata) source\nSampling location and site (including both coordinates and associated organizations)\nShort Data Description\nCoverage Dates/Frequency\nFilename (as stored on Google Drive or shared file repository)\nURL to Files (cloud drive, website, server; e.g. google drive link)\nDate the data was last accessed / downloaded\nData Creator/Owner’s Name\nData Creator/Owner’s Email/contact\nWorking group participant who got the data (Name)\nUsed in your analysis? (Y/N)\nAny additional notes or decisions about how the data is or will be harmonized and analyzed"
  },
  {
    "objectID": "module1.html#proceed-to-module-2",
    "href": "module1.html#proceed-to-module-2",
    "title": "Starting with Team Science",
    "section": "Proceed to Module 2",
    "text": "Proceed to Module 2"
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "A Course for Collaborative Ecologists",
    "section": "Abstract",
    "text": "Abstract\nIn recent decades, ecology has become a more collaborative discipline motivated by the search for generality across ecosystems. At the same time, the availability, quantity, and quality of environmental data have grown rapidly, creating opportunities for re-use of these data in ecological synthesis research. Though synthesis research is complex and demanding, taking an inclusive and collaborative approach to both the scientific process and the data pays dividends throughout the lifetime of a project. This short course is a survey of methods for making ecological synthesis research a “team sport”.\nObjectives for learners are twofold:\n\nDevelop an end-to-end (conception to publication) plan for collaborative synthesis research\nGain data synthesis skills that are immediately useful in a research team setting.\n\nInstructors will cover assembling the team, study design, communication, collecting primary data sources, assembly/harmonization of data, analytical workflows, and publication of derived datasets. The course uses real-world examples, demonstrations, and interactive lessons. Ecologists with synthesis experience will be on hand with seasoned research advice and data tips. Many workshop activities will be oriented toward helping learners develop their own ideas and plans for ecological data synthesis."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "A Course for Collaborative Ecologists",
    "section": "Agenda",
    "text": "Agenda\nThis agenda is subject to change!\n\n\n\nTiming (PT)\nContent\n\n\n\n\n1:00 - 1:15p\nWelcome & Introductions\n\n\n1:15 - 2:15p\nModule 1: Starting with Team Science\n\n\n2:15 - 3:00p\nModule 2: Operational Synthesis\n\n\n3:00 - 3:15p\nBreak\n\n\n3:15 - 4:00p\nModule 3: Tying It All Together"
  },
  {
    "objectID": "index.html#introductions-icebreaker",
    "href": "index.html#introductions-icebreaker",
    "title": "A Course for Collaborative Ecologists",
    "section": "Introductions & Icebreaker",
    "text": "Introductions & Icebreaker\nBefore we begin, we’d love to get a sense for who you all are and why you’re interested in synthesis work! To that end, we’ll take a few minutes and go around the room for introductions. Please include:\n\nYour name and pronouns\nA 1-sentence summary of your work\nBriefly, why are you interested in synthesis?"
  },
  {
    "objectID": "index.html#note-on-course-materials",
    "href": "index.html#note-on-course-materials",
    "title": "A Course for Collaborative Ecologists",
    "section": "Note on Course Materials",
    "text": "Note on Course Materials\nWhile we are excited to offer this short course for the first time at ESA 2024, we’ve chosen to assemble these materials as a living website so that we can revisit and improve the materials over time. So, we recommend that you save the link to this site so that you can have easy access to these materials now and as they are refined going forward.\nIf you are a  GitHub aficionado, we have deployed this website via GitHub Pages so you could also “star” the website’s repository. Simply click the  GitHub octocat logo on the right side of the navbar (at the top of the screen) to be redirected to the GitHub repository underpinning this website.\nFinally, we have developed all of this website using Quarto."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "A Course for Collaborative Ecologists",
    "section": "Credits",
    "text": "Credits\nThe course and its content were developed by a large team. See the People page to learn more.\nSupported by:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollage photo credits: Jacob Bøtter via Flickr, CC BY-SA 2.0 | Jeremy Yoder via Flickr, CC BY-SA 2.0 | Marco Pfeiffer, CC BY-SA 4.0 | Gabriel De La Rosa, CC BY-SA 4.0 | Weecology lab CC BY 4.0 | NEON (National Ecological Observatory Network)"
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nSummarize the advantages of creating a defined contribution workflow\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly\nUnderstand best practices for preparing and analyzing data to be used in synthesis projects"
  },
  {
    "objectID": "module2.html#learning-objectives",
    "href": "module2.html#learning-objectives",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nSummarize the advantages of creating a defined contribution workflow\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly\nUnderstand best practices for preparing and analyzing data to be used in synthesis projects"
  },
  {
    "objectID": "module2.html#introduction",
    "href": "module2.html#introduction",
    "title": "Operational Synthesis",
    "section": "Introduction",
    "text": "Introduction\nHere are a few serviceable definitions of what research is:\n\n“Creative and systematic work undertaken in order to increase the stock of knowledge” from the 2015 Frascati Manual\n“Studious inquiry or examination,” and especially “investigation or experimentation aimed at the discovery and interpretation of facts” from the Merriam-Webster dictionary\n\nTo these basic definitions of research, our definition of synthesis research adds collaborative work, and the integration and analysis of a wide range of data sources, to achieve a more complete, generalizable, or useful research result. In Module 1 we discussed many of the collaborative considerations for synthesis research, including creating a diverse and inclusive team, asking synthesis-ready scientific questions (often broad in scope or spatial scale), and finding suitable information (or data) from a wide variety of sources to answer those questions. Once the synthesis team moves into the operational phase of research, which includes the integration and analysis of data, there are some key activities that must happen:\n\nCleaning and harmonizing data to make it usable\nAnalyzing data to answer questions\nInterpreting the results of your analysis\nWriting the papers or creating other research products\n\nWe’ve already seen that creating a collaborative, inclusive team can set the stage for successful synthesis research. Each of the operational activities above will also benefit from this mindset, and in this module we highlight some of the most important considerations and practices for a team science approach to the nuts-and-bolts of synthesis research."
  },
  {
    "objectID": "module2.html#reproducibility-practices",
    "href": "module2.html#reproducibility-practices",
    "title": "Operational Synthesis",
    "section": "Reproducibility Practices",
    "text": "Reproducibility Practices\n\n\n\nMaking one’s work “reproducible”–particularly in code contexts–has become increasingly popular but is not always clearly defined. For the purposes of this short course, we believe that reproducible work:\n\nUses scripted workflows for all interactions with data\nContains sufficient documentation for those outside of the project team to navigate the project’s contents\nContains detailed metadata for all data products\nAllows anyone to recreate the entire workflow from start to finish\nLeads to modular, extensible research projects. Adding data from a new site, or a new analysis, should be relatively easy in a reproducible workflow.\n\n\n Contributing\n\nCreate a formal plan for collaborating with which your whole team agrees\nQuarantine external inputs\nPlan for “future you”\nCommunicate to your collaborators whenever you’re working on a specific script to avoid conflicting edits\n\n\n\n Documentation\n\n\n\nOne folder per project\nFurther organize content via sub-folders\nMake file names informative and intuitive\n\nAvoid spaces and special characters in file names\nFollow a consistent naming convention throughout\nGood names should be machine readable, human readable, and sorted in a useful way\n\nUse READMEs to record organization rules / explanation\nKeep a log of where source data came from.\n\nWhere did you search?\nWhat search terms did you use?\nList the dataset identifiers you downloaded/used\nIdeally, include downloading data as part of your scripted workflow\n\n\n\n\n\n\nExample project structure:\n project_new\n |–   README.txt\n |–   01_grant_management\n |–   02_project_coordination\n |–   03_documentation\n |–   04_participant_tracking\n |–   05_data\n |       |–   README.txt\n |       |–   hydrology\n |       L   water_chemistry\n |–   06_src\n |       |–   README.txt\n |       |–   data_aggregation\n |       |–   data_harmonization\n |       L   modeling\n L   06_publications\n        L   biogeochemistry\n\n\n\n\n\n Code\n\nUse a version control system\nLoad libraries/packages explicitly\nTrack (and document) software versions\nNamespace functions (if not already required by your coding language)\n\nE.g., dplyr::mutate(mtcars, hp_disp = hp / disp)\n\nUse relative file paths that are operating system-agnostic\nBalance how descriptive object names are with striving for concise names\nUse comments in the code!\nConsider custom functions\nFor scripts that need to be run in order, consider adding step numbers to the file name\n\n\n\nSynthesis Considerations\nHow does reproducibility in synthesis considerations differ from individual / non-synthesis applications?\n\nJudgement calls need to be made / agreed to as a group\n\nBut “defer to the doers”\n\nIncreased emphasis on contribution guidelines / planning being formalized\nMore communication needs\nMust ensure that every team member has sufficient access to the project files\nIts best to keep track of who contributed what, so that everyone gets credit. This can be challenging in practice.\n\n\nReproducibility Questions\n\n\nIn groups of 3-5, discuss the following questions:\n\nWhat elements of reproducibility from our list have you used/are interested in using?\nWhich feel unreasonable or confusing?\nWhat activities do you do in your own work to ensure reproducibility that our list is missing?"
  },
  {
    "objectID": "module2.html#version-control",
    "href": "module2.html#version-control",
    "title": "Operational Synthesis",
    "section": "Version Control",
    "text": "Version Control\nIn all scientific research, the data work (cleaning, harmonizing, analyzing) and the writing are iterative processes. The process and products change over time and usually require a series of revisions. In synthesis research, the process can become even more complex because the team is usually large and multiple people are contributing data, analysis, writing, revisions, and more. Using version control helps manage this complexity by recording changes, tracking individual contributions, and ensuring that things can be rolled-back to an earlier state if needed.\n\n\n\nLike this comic shows here, you might have several drafts of your paper before the finalized version. With a version control system, all the revisions in each draft are saved. Version control systems provide a framework for preserving these changes without cluttering your computer with all of the files that precede the final version.1\nUsing version control enhances your workflow by allowing you to:\n\nmaintain a descriptive history of your research project’s development while keeping a clean workspace\n\nno more cryptic file names or commented-out lines of code to track your progress\n\ncollaborate with team members and merge everyone’s edits together\nexplore bugs or new features without disrupting your team members’ work.2\n\n\nVocabulary\n\n \n\nHere are some brief definitions for a selection of fundamental version control vocabulary terms.\n\nVersion control system: software that tracks iterative changes to your code and other files\nRepository: the specific folder/directory that is being tracked by a version control system\nGit: a popular open-source distributed version control system\nGitHub: a website that allows users to store their Git repositories online and share them with others\n\n\n\nGitHub\nWhile this section of the module focuses on GitHub, there are several other viable alternatives for working with Git individually or as part of a larger team (e.g., GitLab, GitKraken, etc.). Any of these may be viable option for your team and we focus on GitHub here only to ensure a standard backdrop for the case studies we’ll discuss shortly.\nThere are a lot of GitHub tutorials that exist already so, rather than add our own variant to the list, we’ll work through part of one created by the Scientific Computing team of the National Center for Ecological Analysis and Synthesis (NCEAS).\nSee the workshop materials here.\nGiven the time restrictions for this short course, we’ll only cover how you engage with GitHub directly through the GitHub website. However, your chosen software for writing code will certainly have a method of connecting to GitHub/etc., so if this topic is of interest it will be beneficial for you to search out the relevant tutorial."
  },
  {
    "objectID": "module2.html#data-preparation",
    "href": "module2.html#data-preparation",
    "title": "Operational Synthesis",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe scientific questions being asked in synthesis projects are usually broad in scope, and it is therefore common to bring together many datasets from different sources for analysis. The datasets selected for analysis (source data) may have been collected by different people, in different places, using different methods, as part of different projects… or all of the above. Typically, some amount of data cleaning - filtering or removing unwanted observations - and data harmonization - putting data together in common structures, file formats, and units of measurement - is necessary before analysis can begin. This process can be easy or difficult depending on the quality of the source data, the differences between source data, and how much metadata (see callout below) is available to understand them.\n\nData Preparation Group Questions\n\n\n\nHow many of you work directly with data in your day-to-day?\nWhat percentage of the time that you spend working on data is spent on data cleaning?\nHow much on metadata creation?\nHow much on data preparation?\n\n\n\n\n\n\n\n\n\n\nMore about Metadata\n\n\n\n\n\nMetadata is “data about the data,” or information that describes who collected the data, what was observed or measured, when the data were collected, where the data were collected, how the observations or measurements were made, and why they were collected. Metadata provide important contextual information about the origin of the data and how they can be analyzed or used. They are most useful when attached or linked to the data being described, and data and related metadata together are commonly referred to as a dataset.\nMetadata for ecological research data are well described in Michener et al (1997),3 but there are many other kinds of metadata with different purposes.4 If you are publishing a research dataset and have questions about metadata, ask a data manager for your project, or staff at the repository you are working with, for help. Either can typically provide guidance on creating metadata that will describe your data and be useful to the community (here is one example). We’ll return to the subject of metadata in Module 3.\n\n\n\n\nCleaning Data\nWhen assembling large datasets from diverse sources, as in synthesis research, not all the source data will be useful. This may be because there are real or suspected errors, missing values, or simply because they are not needed to answer the scientific question being asked (wrong variable, different ecosystem, etc.). Data that are not useful are usually excluded from analysis or removed altogether. Data cleaning tends to be a stepwise, iterative process that follows a different path for every dataset and research project. There are some standard techniques and algorithms for cleaning and filtering data, but they are beyond the scope of this course. Below are a few guidelines to remember, and more in-depth resources for data cleaning are found at the end of this section.\n\nAlways preserve the raw data. Chances are you’ll want to go back and check the original source data at least once.\nUse a scripted workflow to clean and filter the raw data, and follow the usual rules about reproducibility (comments, version control, functionalization).\nConsider using the concept of data processing “levels,” meaning that defined sets of data flagging, removal, or transformation operations are applied consistently to the data in stepwise fashion. For example, incoming raw data would be labeled “level 0” data, and “level 1” data is reached after the first set of processing steps is applied.\nSpread the data cleaning workload around! Data cleaning typically demands a HUGE fraction of the total time devoted to working with data,567 and it can be tedious work. Make sure the team shares this workload equitably.\n\n\n\nData Harmonization\nData harmonization is the process of bringing different datasets into a common format for analysis. The harmonized data format chosen for a synthesis project depends on the source data, analysis plans, and overall project goals. It is best to make a plan for harmonizing data BEFORE analysis begins, which means discussing this with the team in the early stages of a synthesis project. As a general rule, it is also wise to use a scripted workflow that is as reproducible as possible to accomplish the harmonization you need for your project. Following this guidance lets others understand, check, and adapt your work, and will also make it much, much easier to bring new data and analysis methods into the project.\nData harmonization is hard work that sometimes requires trial and error to arrive at a useful end product. At the end of this section are some additional data harmonization resources to help you get started. Looking at a simple example might also help.\n\n\n\n\n\n\nExample: Harmonizing grassland biomass data\n\n\n\nIn the figure below, two datasets from different LTER sites have been harmonized into one file for analysis. We don’t have all the metadata here, but based on the column naming we can assume that the file on the left (Konza_harvestplots.txt, yellow) contains columns for the sampling date, a plot identifier, a treatment categorical value, and measured biomass values. The filename suggests that the data come from the Konza Prarie LTER site. The file on the right (2022_clips.csv, blue) has a column denoting the LTER site, in this case Sevilleta LTER (abbreviated as SEV), as well as similar date, plot identifier, treatment, and measured biomass columns.\nThere are some similarities and some differences in these two source files. A harmonized file (lter_grass_biomass_harmonized.txt) appears below.\n\n\n\nTake a minute to look at the harmonized file and consider how these data were harmonized. Then, answer the question below.\n\n\n\n\n\n\n\n\nWhat changes were made to data structure, variable formatting, or units in these data?\n\n\n\n\n\nEven though the source data files were similar, several important changes were made to create the harmonized file. Among them:\n\nThe site column was preserved and contains the “SEV” and “KNZ” categorical values denoting which LTER site is observed in each row.\nThe dates in the date column were converted to a standard format (YYYY-MM-DD). Incidentally, this matches the International Organization for Standardization (ISO) recommendation for date/time data, making it generally a good choice for dates.\nA new rep column was created to identify the replicate measurements at each site. This is essentially a standard version the plot identifier columns (PlotID and plot) in the original data file. Also note that the original values are preserved in the new plot_orig column.\nThe treatment columns from the original data files (Treatment and trt) were standardized to one column with “C” and “F” categorical values, for control and fertilized treatments, respectively.\nThe biomass values from Konza were converted to units grams per meter squared (g/m2) because the original Konza measurements were for total biomass in 2x2 meter plots. Note that this conversion is for illustration purposes only - we can’t be sure if this conversion is correct without it being spelled out in the metadata, or asking the data provider directly.\n\n\n\n\n\n\nA Word about Harmonized Data Formats\nAbove, we have discussed several aspects of selecting a data format. There are at least three related, but not exactly equivalent, concepts to consider when formatting data. First, formats describe the way data are structured, organized, and related within a data file. For example, in a tabular data file about biomass, the measured biomass values might appear in one column, or in muiltiple columns. Second, the values of any variable can be represented in more than one format. The same date, for example, could be formatted using text as “July 2, 1974” or “1974-07-02.” Third, format may refer to the file format used to hold data on a disk or other storage medium. File formats like comma separated value text files (CSV), Excel files (.xlsx), JPEG images, are commonly used for research data, and each has particular strengths for certain kinds of data.\nA few guidelines apply:\n\nFor formatting a tabular dataset, err towards simpler data structures, which are usually easier to clean, filter, and analyze. Long-format tables, or tidy data 8, is one common recommendation for this.\nWhen choosing a file format, err towards open, non-proprietary file formats that more people know and have access to. Delimited text files, such as CSV files, are a good choice for tabular data.\nUse existing community standards for formatting variables and files as long they suit your project methods and scientific goals. Using ISO standards for date-time variables, or species identifiers from a taxonomic authority, are good examples of this practice.\nThere is no perfect data format! Harmonizing data always involves some judgement calls and tradeoffs.\n\nWhen choosing a destination format for the harmonized data for a synthesis project, the audience and future uses of the data are also an important consideration. Consider how your synthesis team will analyze the data, as well as how the world outside that team will use and interact with the data once it is published. Again, there is no one answer, but below are a few examples of harmonized destination formats to consider.\n\nLong (Tidy)Wide (Untidy)Relational DatabaseCloud-nativeOther…\n\n\nHere our grassland biomass data is in long format, often referred to as “tidy” data. Data in this format is generally easy to understand and use. There are three rules for tidy data:\n\nEach column is one variable.\nEach row is one observation.\nEach cell contains a single value.\n\n\n\n\nvisual representation of the tidy data structure\n\n\nAdvantages: clear meaning of rows and columns; ease in filtering/cleaning/appending\nDisadvantages: not as human-friendly so it can be difficult to assess the data visually\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, in long format.\n\n\n\n\nIn this dataset, our grassland data has been restructured into wide format, often referred to (sometimes unfairly) as “messy” or “untidy” data. Note that the biomass variable has been split into two columns, one for control plots and one for fertilized plots.\nAdvantages: easier for some statistical analyses (ANOVA, for example); easier to assess the data visually\nDisadvantages: may be more difficult to clean/filter/append, multiple observations per row; more likely to contain empty (NULL) cells\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, restructured into wide format with biomass values in control and fertilized columns.\n\n\n\n\nBelow is an example of how we might structure our grassland data in a relational database. The schema consists of three tables that house information about sampling events (when, where data were collected), the plots from which the samples are collected, and the biomass values for each collection. The schema allows us to define the data types (e.g., text, integer), add constraints (e.g., values cannot be missing), and to describe relationships between tables (keys). Relational formats are normalized to reduce data redundancy and increase data integrity, which can help us to manage complex data9.\n\n\n\nexample grassland database schema\n\n\nAdvantages: reduced redundancy, greater integrity; community standard; powerful extensions (e.g., store and process spatial data); many different database flavors to meet specific needs\nDisadvantages: significant metadata needed to describe and use; more complex to publish; learning curve\nPossible file formats: Database stores, can be represented in delimited text (CSV)\nA richer example is a schematic of the related tables that comprise the ecocomDP10 harmonized data format for biodiversity data. Eight tables are defined, along with a set of relationships between tables (keys), and constraints on the allowable values in each table.\n\n\n\nThe ecocomDP schema. Each table has a name (top cell) and a list of columns. Shaded column names are primary keys, hashed columns have constraints, and arrows represent relations between keys/constraints in different tables.\n\n\n\n\nThere are many possibilities to make large synthesis datasets available and useful in the cloud. These require specialized knowledge and tooling, and reliable access to cloud platforms.\nAdvantages: easier access to big (high volume) data, can integrate with web apps\nDisadvantages: less familiar/accessible to many scientists, few best practices to follow, costs can be higher\nPossible file formats: Parquet files, object storage, distributed/cloud databases\n\n\n\nA few of the cloud-native technologies that might be useful for synthesis research products.\n\n\n\n\nThere are many, many other possible harmonized data formats. Here are a few possible examples:\n\nDarwinCore archives for biodiversity data\nOrganismal trait databases\nArchives of cropped, labeled images for training machine or deep learning models\nLibraries of standardized raster imagery in Google Earth Engine"
  },
  {
    "objectID": "module2.html#data-analysis",
    "href": "module2.html#data-analysis",
    "title": "Operational Synthesis",
    "section": "Data Analysis",
    "text": "Data Analysis\nOnce the team has found sufficient source data, then cleaned, filtered, and harmonized countless datasets, and documented and described everything with quality metadata, it is finally time to analyze the data! Great! Load up R or Python and get started, and then tell us how it goes. We simply don’t have enough time to cover all the ins and outs of data analysis in a three-hour course. However, we have put a few helpful resources below to get you started, and many of the best practices we have talked about, or will talk about, apply:\n\nDocument your analysis steps and comment your code, and generally try to make everything reproducible.\nUse version control as you analyze data.\nGive everyone a chance! Analyzing data is challenging, exciting, and a great learning opportunity. Having more eyes on the analysis process also helps catch interesting results or subtle errors."
  },
  {
    "objectID": "module2.html#synthesis-group-case-studies",
    "href": "module2.html#synthesis-group-case-studies",
    "title": "Operational Synthesis",
    "section": "Synthesis Group Case Studies",
    "text": "Synthesis Group Case Studies\nEstimated time: 10 min\nTo make some of these concepts more tangible, let’s consider some case studies. The following tabs contain GitHub repositories for real teams that have engaged in synthesis research and chosen to preserve and maintain their scripts in GitHub. Each has different strengths and you may find that facets of each feel most appropriate for your group to adopt. There is no single “right” way of tackling this but hopefully parts of these exemplars inspire you.\n\nExample 1Example 2Example 3\n\n\nLTER SPARC Group: Soil Phosphorus Control of Carbon and Nitrogen\nStored their code here:  lter / lter-sparc-soil-p\nHighlights\n\nStraightforward & transparent numbering of workflow scripts\n\nFile names also reasonably informative even without numbering\n\nSimple README in each folder written in human-readable language\nCustom .gitignore safety net\n\nControls which files are “ignored” by Git (prevents accidentally sharing data/private information)\n\n\n\n\nLTER Full Synthesis Working Group: The Flux Gradient Project\nStored their code here:  lter / lterwg-flux-gradient\nHighlights\n\nExtremely consistent file naming conventions\nStrong use of sub-folders for within-project organization\nTop-level README includes robust description of naming convention, folder structure, and order of scripts in workflow\nActive contribution to code base by nearly all group members\n\nFacilitated by strong internal documentation and consenus-building prior to choosing this structure\n\n\n\n\nLTER Full Synthesis Working Group: From Poles to Tropics: A Multi-Biome Synthesis Investigating the Controls on River Si Exports\nStored their code here:  lter / lterwg-silica-spatial\nHighlights\n\nFiles performing similar functions share a prefix in their file name\nUse of GitHub “Release” feature to get a persistent DOI for their codebase\nSeparate repositories for each manuscript\nNice use of README as pseudo-bookmarks for later reference to other repositories\n\n\n\n\nFor more information about LTER synthesis working groups and how you can get involved in one, click here."
  },
  {
    "objectID": "module2.html#additional-resources",
    "href": "module2.html#additional-resources",
    "title": "Operational Synthesis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nData Preparation\nData cleaning and filtering resources\n\nData cleaning is complicated and varied, and entire books have been written on the subject.1112 For some general considerations on cleaning data, see EDI’s “Cleaning Data and Quality Control” resource\nOpenRefine is an open-source, cross-platform tool for iterative, scripted data cleaning.\nIn the R language, the tidyverse libraries (particularly tidyr and dplyr) are often used for data cleaning, as are additional libraries like janitor.\nIn Python, pandas and numpy libraries provide useful data cleaning features. There are also some stand-alone cleaning tools like pyjanitor (started as a re-implementation of the R version) and cleanlab (geared towards machine learning applications).\nBoth the R and Python data science ecosystems have excellent documentation resources that thoroughly cover data cleaning. For R, consider starting with Hadley Wickham’s R for Data Science book chapter on data tidying,13 and for python check Wes McKinney’s Python for Data Analysis book chapter on data cleaning and preparation.14\n\nData harmonization resources\n\nFor R and Python users, there are, again, excellent documentation resources that thoroughly cover data harmonization techniques like data filtering, reformatting, joins, and standardization. In Hadley Wickham’s R for Data Science book, the chapters on data transforms and data tidying are a good place to start. In Wes McKinney’s Python for Data Analysis book, the chapter on data wrangling is helpful.\n\n\n\nData Analysis\n\nHarrer, M. et al. Doing Meta-Analysis with R: A Hands-On Guide. 2023. GitHub\nOnce again, for R and Python users, the same two books mentioned above provide excellent beginning guidance on data analysis techniques (exploratory analysis, summary stats, visualization, model fitting, etc). In Wickham’s R for Data Science book, the chapter on exploratory data analysis will help. In McKinney’s Python for Data Analysis book, try the chapters on plotting and visualization and the introduction to modeling.\n\n\n\nCourses, Workshops, and Tutorials\n\nSynthesis Skills for Early Career Researchers (SSECR) course. 2024. LTER Network Office\nReproducible Approaches to Arctic Research Using R workshop. 2024. Arctic Data Center & NCEAS Learning Hub\nCollaborative Coding with GitHub workshop. 2024. NCEAS Scientific Computing team\nCoding in the Tidyverse workshop. 2023. NCEAS Scientific Computing team\nShiny Apps for Sharing Science workshop. 2022. Lyon, N.J. et al.\nTen Commandments for Good Data Management. 2016. McGill, B.\n\n\n\nLiterature\n\nTodd-Brown, K.E.O., et al. Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. 2022. Biogeosciences\nBorer, E.T. et al. Some Simple Guidelines for Effective Data Management. 2009. Ecological Society of America Bulletin\n\n\n\nOther\n\nBetter commit messages with Conventional Commits"
  },
  {
    "objectID": "module2.html#footnotes",
    "href": "module2.html#footnotes",
    "title": "Operational Synthesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLyon, N. J., Chen, A., Brun, J. (2023). Collaborative Coding with GitHub. LNO Scientific Computing Team. https://nceas.github.io/scicomp-workshop-collaborative-coding/.↩︎\nPoulsen, C. V. & Chen, A. (2024). NCEAS coreR for Delta Science Program. NCEAS Learning Hub. https://learning.nceas.ucsb.edu/2024-06-delta.↩︎\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nMayernik, M.S. and Acker, A. (2018), Tracing the traces: The critical role of metadata within networked communications. Journal of the Association for Information Science and Technology, 69: 177-180. https://doi.org/10.1002/asi.23927↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nNew York Times, 2014↩︎\nAnaconda State of Data Science Report, 2022↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nZimmerman, N. 2016. Hand-crafted relational databases for fun and science↩︎\nO’Brien, Margaret, et al. “ecocomDP: a flexible data design pattern for ecological community survey data.” Ecological Informatics 64 (2021): 101374. https://doi.org/10.1016/j.ecoinf.2021.101374↩︎\nOsborne, Jason W. Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data. Sage publications, 2012.↩︎\nVan der Loo, Mark, and Edwin De Jonge. Statistical data cleaning with applications in R. John Wiley & Sons, 2018. https://doi.org/10.1002/9781118897126↩︎\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for data science. ” O’Reilly Media, Inc.”, 2023. https://r4ds.hadley.nz/↩︎\nMcKinney, Wes. Python for data analysis. ” O’Reilly Media, Inc.”, 2022. https://wesmckinney.com/book↩︎"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "The course is currently being organized by Greg Maurer (gmaurer@nmsu.edu), with generous assistance from Marty Downs and Nick Lyon. See the Contributors section for more information."
  },
  {
    "objectID": "people.html#contributors",
    "href": "people.html#contributors",
    "title": "People",
    "section": "Contributors",
    "text": "Contributors\nWe have a long list of synthesis scientists actively involved in the course working group who are contributing expertise and instructional content, and many will be present as instructors at the 2024 ESA Meeting. Names and web profiles are listed below in alphabetical order by last name.\nIcon legend:  = website |  = publications |  = GitHub |  = ORCID\n\nKathryn Barry\nJoanna Carey – |  \nAngel Chen (she/her) – LTER Network Office |   \nLaura Dee – |   \nMarty Downs – LTER Network Office |  \nStevan Earl – |   \nSarah Elmendorf – |   \nJalene LaMontagne – |   \nNick J Lyon (they/them) – LTER Network Office |   \nGregory Maurer (he/him) – |   \nColin Smith\nEric Sokol (he/him) – NEON |   \nAlexandra (Sasha) Wright"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "People",
    "section": "Other Contributors",
    "text": "Other Contributors\nIn addition to those above, several people contributed to early iterations on the concept and content of the course.\n\nForest Isbell\nKim Komatsu"
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the three primary “products” that come out of synthesis groups.\nUnderstand the metadata and other features that make published datasets useful.\nEvaluate the reach and reproducibility of an ecological synthesis project’s outputs.\nCreate a plan for your synthesis team’s research products, and apply the contribution, publishing, and citation practices that will benefit the team."
  },
  {
    "objectID": "module3.html#learning-objectives",
    "href": "module3.html#learning-objectives",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the three primary “products” that come out of synthesis groups.\nUnderstand the metadata and other features that make published datasets useful.\nEvaluate the reach and reproducibility of an ecological synthesis project’s outputs.\nCreate a plan for your synthesis team’s research products, and apply the contribution, publishing, and citation practices that will benefit the team."
  },
  {
    "objectID": "module3.html#introduction",
    "href": "module3.html#introduction",
    "title": "Tying It All Together",
    "section": "Introduction",
    "text": "Introduction\nSo far, we’ve made the point that ecological synthesis research is collaborative and inclusive, and that it integrates a wide range of data. Synthesis research is also intended to be influential and useful. There are many definitions of “influential and useful” to consider here, but successful synthesis research tends to expand the boundaries of knowledge and aims to improve human lives or the environment.  The ability to accomplish this in synthesis research frequently depends on what knowledge or products are created, and how the synthesis team disseminates and communicates them to the outside world.\nThere are three interconnected, publishable products that are the most common outputs from a synthesis project (or potentially any research project, really): the data, analytical workflows (code for data cleaning or statistics, for example), and research results. Each of these elements is a valuable product of synthesis science, and each one should reference the others. In this module we’ll discuss the mechanics of publishing each one, and then how they can be connected and made accessible for the long-term."
  },
  {
    "objectID": "module3.html#designing-and-publishing-datasets",
    "href": "module3.html#designing-and-publishing-datasets",
    "title": "Tying It All Together",
    "section": "Designing and Publishing Datasets",
    "text": "Designing and Publishing Datasets\nEstimated time: 12 min\nIn Module 2 we discussed some considerations for creating and formatting harmonized data files useful for synthesis research. We also introduced the importance of metadata for describing data and making it more usable. Publishing harmonized data files and descriptive metadata together as a dataset ensures that the data products produced by a synthesis team are findable, accessible, interoperable, and reusable (FAIR). FAIR data are an important output for almost any ecological synthesis project.\n\n\n\n\n\n\nMore about Findable, Accessible, Interoperable, Reusable (FAIR) data\n\n\n\n\n\nThe FAIR principles, standing for Findability, Accessibility, Interoperability, and Reusability, are a community-standard set of guidelines for evaluating the quality and utility of published research data. Making an effort to meet the FAIR criteria promotes both human and machine usability of data, and is a worthy objective when preparing to publish data from a synthesis research project.\nThe FAIR principles were first defined in the paper by Wilkinson et al (2018)1. Since this time, many resources have arisen to guide the implementation the FAIR principles23, and to quantify FAIR data successes and failures in the research and publishing communities45.\n\n\n\n\nActivity 1: Evaluate published datasets\nLets start our journey to publishing datasets by looking at some that are already published. Form breakout groups and course instructors will assign each group a dataset (a DOI) for evaluation. With your group, answer these questions about the dataset:\n\nWhere were the data collected?\nWhat variables were measured and in what units?\nWhat is the origin of the data and how have they been altered since collection?\nWere the first three questions easy to answer? Why or why not?\n\n\nGroup 1Group 2Group 3Group 4Group 5\n\n\nExample dataset: https://doi.org/10.6073/pasta/9733f6b6d2ffd12bf126dc36a763e0b4\nThis is an EDI dataset is from the SoDaH (Soil Data Harmonization) LTER working group.\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset provides a nice example of a harmonized data product that includes provenance metadata. We’ll talk a little more about this later, but note that all the original data sources are linked to this dataset on the landing page.\n\n…\n…\n…\n\n\n\n\n\n\nExample dataset: https://doi.org/10.5061/dryad.v2k2607\nThis is a Dryad dataset from a synthesis paper about oligotrophication.\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset\n\n…\n…\n…\n\n\n\n\n\n\nExample dataset:\nMaybe this: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.493.16 (needs editing)\nMaybe this: https://doi.org/10.6084/m9.figshare.10735652.v1 (but pretty bad)\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset\n\n…\n…\n…\n\n\n\n\n\n\nExample dataset:\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset\n\n…\n…\n…\n\n\n\n\n\n\nExample dataset:\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset\n\n…\n…\n…\n\n\n\n\n\n\n\n\n\nMetadata\nOne thing that Activity 1 introduces is the importance of metadata. Metadata are data about the data. As a general rule, metadata should describe\n\nWho collected the data\nWhat was observed or measured\nWhen the data were collected\nWhere the data were collected\nHow the data were collected (methods, instruments, etc.)\nSometimes, stating why the data were collected can help future users understand data context evaluate fitness for use.\n\nIncluding metadata of this nature makes data more usable, and helps prevent the deterioration of information about data over time, as illustrated in the figure below (from Michener et al. 19976).\n\n\n\nExample of the normal degradation in information content associated with data and metadata over time (“information entropy”). Accidents or changes in technology (dashed line) may eliminate access to remaining raw data and metadata at any time (Michener et al 1997.\n\n\n\nData Provenance Metadata\n\nProvenance metadata deserves special attention for ecological data synthesis projects. Data provenance refers to information detailing the origin of the values in a dataset, which is particularly important for synthesis projects that bring together data from many different sources. Synthesis activities typically produce new data products that are derived from the original source data after they have been cleaned, harmonized, and analyzed. Provenance metadata should be included with the derived products to point back to the original source data, similar to the way bibliographic references point to the source material for a book or scholarly article.\nA few other notes on provenance:\n\nAt its simplest, documenting data sources as you collect and analyze the source data is a great start on provenance metadata.\nMany data repositories provide guidelines, tools, and features for data provenance metadata7.\nProvenance metadata can become very detailed if the software and computing environment is also taken into account. This is an active area of study 89.\n\n\n\nLicensing\nPublished datasets should include a license in every copy of the metadata that defines who has what rights to use, reproduce, or distribute the data. Licensing decisions should be made in consultation with the synthesis team after considering the nature of the data (does it contain human subject data, for instance?), its origin (including restrictions on source data, if applicable), and the requirements of the funders and institutions associated with the project. For publicly-funded environmental research data, it is generally appropriate to use open licenses, and the Creative Commons CC-BY attribution, and CC0 public domain, licenses are probably a good choice for most ecological synthesis data. This is not legal advice and your mileage may vary.\n\n\nMetadata Creation and Management\nAssembling metadata should be an integral part of the data synthesis activities discussed in Module 2, and can even be built-in to the workflow and project management practices of a project. Make sure to plan for and start creating metadata early in a synthesis project. Below are a few ways to do that.\n\nKeep a detailed project log and populate it with metadata for the project, including information like\n\nwhat source data the team is using and where they came from.\nhow data are being analyzed and methods used to create derived products.\nwho is doing what.\n\nStart creating distinct publishable datasets (data plus metadata) as data are processed and analyzed. The team can do this\n\nlocally, using a labeled directory for the cleaned, harmonized, of derived data, along with related code and metadata files. Metadata files may be plain text, or use a metadata template.\nwith a repository-based metadata editor, such as ezEML from the Environmental Data Initiative (EDI) repository.\n\nGet a professional data manager or data curator involved with the synthesis project. For example, the LTER Network has a community of “Information Managers” 10 trained in data management, metadata creation, and data publishing. Research data repositories11 and associated data curators12 may also be a good resource.\n\n\n\n\n\n\n\nKey insight\n\n\n\nReproducibility and the collection of metadata are closely related. Your team’s detailed documentation of the research process allows for reproducible science, and can be mined as a source of metadata during data publication.\n\n\n\n\n\nDeciding What to Publish\n\nThe overall design of the dataset to be published is often difficult to imagine, particularly for people new to using or creating datasets. One of the most common questions data managers hear is “What should we publish?” This is usually a question about what files to include in the published dataset, or what data will be useful as a published dataset.\n\nDiscussion questionSome general rules\n\n\n\nWhat should be included in a published dataset?\n\n\n\nAs we learned in Activity 1, every dataset is different, but the answer to “What should we publish?” usually comes down to:\n\nPublish any data used to generate research results.\nPublish any data that will be used by others (scientists, managers, public stakeholders), including raw data.\nIf reproducibility is of interest or concern, publish the workflow.\n\nUsually this means publishing code, such as scripts written in R, python, or a shell language.\nWhat code? Any scripts used to process or analyze the data, or to generate research results like figures, are fair game.\nSometimes code, especially detailed, reusable workflows like an R package, can stand alone as an independent publication. We discuss that in a later section.\n\n\nAnd of course… always publish descriptive metadata about any of the above.\nThese are general rules, but you can also look at advice from a repositories like EDI and BCO-DMO, or from a research network like NEON. Asking a data manager, especially one involved with the synthesis group’s work, can also be helpful, as will discussion among the full synthesis team.\n\n\n\n\n\nChoosing and Publishing to a Repository\nThere are many, many research data repositories available to researchers now13, making the choice of where to publish data fairly challenging. A few basic data repository features are essential when publishing a synthesis dataset. First, the repository should issue persistent, internet-resolveable, unique identifiers for every dataset published. Generally this will be a Digital Object Identifier, or DOI, that can be cited every time the dataset is used after publication. Second, repositories should require, and provide the means to create/publish, metadata describing each dataset. Without requiring at least minimal metadata, no repository can ensure that published data are FAIR. Finally, research data repositories should be stable and well supported so that data remain available and usable in perpetuity. Choosing a repository from the CoreTrustSeal certified repository list is one way to assess this. Beyond this, asking a few questions about the dataset will help with repository selection:\n\nWho are the likely users for this data? Will they belong to a specific scientific discipline, research network, or community of stakeholders?\nHow specialized are your data? Do they fall into a common data type or follow a special formatting standard?\nWill the data be updated regularly?\nDoes the repository charge for publication?\n\n\n\n\nA limited slice from the broad spectrum of research data repositories available for publishing synthesis data.\n\n\nAfter making a choice, the process of publishing data varies from repository to repository. More specialized repositories tend to offer enhanced documentation, custom software tools, or even data curation staff to assist users with data publication. It also helps to consult a project data manager if one is available to the synthesis team.\n\n\nAdditional Data Publishing Resources\n\nNEON’s derived data publishing guide\nEDI repository data authorship guide\nBCO-DMO repository data publishing guide"
  },
  {
    "objectID": "module3.html#sharing-the-teams-workflow",
    "href": "module3.html#sharing-the-teams-workflow",
    "title": "Tying It All Together",
    "section": "Sharing the Team’s Workflow",
    "text": "Sharing the Team’s Workflow\nOne of the most valuable, shareable outputs of synthesis research is the analytical workflow used to derive datasets and produce scientific results. Most often, these workflows are written in computer code, such as R, Python, or another language. Workflows may consist of a collection of scripts, or they may be organized into stand-alone modules or libraries. The latter is easier to share and re-use, but requires more advanced knowledge of software design. Sharing workflows and code are one of the most important needs for ensuring the reproducibility of science.\nPublishing the workflow also gives interested parties an understanding of…\n\nthe origin of the data.\nthe process used for data cleaning, harmonization, analysis, and presentation of results (figures), which may be useful in future work.\nhow the workflow was developed or changed over time.\nthe contributions made by the team.\n\nIn other parts of the course, we have strongly recommended using version control and collaboration platforms to manage coding, writing, and other elements of the synthesis team workflow. In particular, we have focused on using GitHub as a one-stop shop for many of these tasks. In combination with other software and services, GitHub can be reliably used to publish workflows as well. By using repositories to archive GitHub content and issue a DOI (Zenodo integration is already included in GitHub)14 linked to a particular version of the code, workflows can be published and cited by the research products that they were used to generate. This is commonly done in near-term ecological forecasting projects 15."
  },
  {
    "objectID": "module3.html#communicating-research-results",
    "href": "module3.html#communicating-research-results",
    "title": "Tying It All Together",
    "section": "Communicating Research Results",
    "text": "Communicating Research Results\nOne of the primary goals of synthesis research is to find useful, generalizable research results about the system under study. Most often this means writing scientific journal articles. While we aren’t going to go into full detail about what constitutes, or how to write, a manuscript for a journal, there are some unique features of writing articles for synthesis projects. First, data papers are often an important product for synthesis groups, and these are somewhat different than standard research journal articles. Second, given, the large size and cooperative nature of most synthesis teams, a collaborative writing process is called for. An appropriate collaborative writing method, and some team norms and contribution guidelines, should be in place to reduce the potential for conflict or mistakes.\n\nData papers\nA data descriptor article, usually known as a data paper, is a peer-reviewed journal article written to introduce and describe a (usually) new dataset. For synthesis teams, who are often producing a harmonized dataset as their first major research product, writing a data paper to accompany the dataset makes sense as a way to introduce the data, demonstrate their utility, and get the word out about the dataset. Data papers also lay the groundwork for any future papers that will answer the science questions of interest to the synthesis team.\nData papers may be simpler and shorter than research articles (not always though), but there are still a few gotchas that can arise. Below are some recommendations, and the rationale behind them.\n\nPublish the dataset described by the data paper in a reputable data repository.\n\nAlthough some data paper publishers host data themselves, they are usually published only as supplementary material for article, or are only held for review. Most data-focused journals require that accepted data papers should describe and reference a dataset published in a research data repository. Follow the guidance above to select a repository and prepare the dataset for publication.\n\nBe sure to cite the data paper and the dataset properly.\n\nThe existence of a data paper and a dataset, each describing the same data and each with its own DOI, can create confusion about what to cite in related works. If the novelty and utility of the dataset, or the methods used to assemble it, are being referenced by a related work, then it may be most appropriate to cite the data paper. If the actual data are being used (analyzed, interpreted, etc.) in a related work, then cite the published dataset. In many cases it is expected to cite both.\n\nDon’t shortchange the metadata in the published dataset just because there is also a data paper.\n\nConsider the future usability of the data the data paper describes, and ensure that the associated published dataset contains detailed, community-standard metadata. Not all users will see the data paper, and data paper publishers may have incomplete or quirky requirements for metadata.\n\n\n\n\n\n\n\n\nData paper examples and publication venues\n\n\n\n\n\nSome examples of data papers related to synthesis projects:\n\nKomatsu, Kimberly J., et al. “CoRRE Trait Data: A dataset of 17 categorical and continuous traits for 4079 grassland species worldwide.” Scientific Data 11.1 (2024): 795. https://doi.org/10.1038/s41597-024-03637-x\n…\n\nA few suggested venues for publishing data papers:\n\nScientific Data (Nature Publishing Group)\nData (MDPI)\nPLOS ONE (usually termed “database papers”)\nThe ESA journal Ecology, and quite a few other disciplinary journals, now publish data papers.\n\nGBIF also maintains a helpful list of data paper journals.\n\n\n\n\n\nWriting collaboratively\nWriting a paper with a large team can be a challenge. It is important to encourage team members to contribute in a way they are comfortable with, but there is the potential for technical, editorial, and personal conflict without some prior planning. Practically, there are two models for writing a manuscript with a bunch of contributors.\n\nCloud-based collaborative writing“Pass the manuscript”\n\n\nIn this model manuscripts live mainly in web-based writing platforms managed by a cloud service provider (e.g. Google Docs) and all contributors write and edit the document within that platform. Contributions may be asynchronous or synchronous since version control and conflict resolution is generally built into the platform. Most platforms have additional collaboration features, such as user account management, suggested edits, and commenting systems.\nSoftware platform: Google Docs, Microsoft 365 Online, Overleaf (LaTeX)\nPros: Strong collaboration features (user/permission management, contribution tracking, comments and suggestions). No need to distribute copies and then merge contributions.\nCons: Can be unfamiliar to senior contributors. Easy to lose track of links. Limited formatting features compared to local word processors. Privacy/tracking concerns.\n\n\nThis model relies on word processing software installed on contributors’ local machines. Copies of the manuscript are distributed to contributors for asynchronous writing and editing assignments, and contributions are then merged together into a synchronized version of the manuscript. In large teams, it may be best to have one person managing the copy/merge process.\nSoftware platform: Microsoft Word (usually), email\nPros: Familiar to most. Integrates with local data management practices. Most word processors have powerful collaboration and versioning features now. Advanced formatting and editing. Less reliance on cloud providers.\nCons: License pricing and institutional availability may be limited. Multiple versions in use, and the copy/merge workflow can easily generate conflicts or become unmanageable in large groups.\n\n\n\nIn addition to these practical considerations, there are some team considerations as well\n\nMake the expectations for contributing to a manuscript clear.\n\nHow, when, and where should contributions be made\nAuthorship expectations discussed in advance\n\nMake space for new, or early-career team members to contribute.\n\nEfficiency and experience level aren’t good reasons to exclude contributors\nSynthesis papers are a great learning experience and career opportunity\n\nTeam discussions are preferable to unilateral editorial decisions.\n\nThis can help avoid hurt feelings during the editing process.\n\nIt can be beneficial to have a manuscript coordinator.\n\nThe coordinator can help split up writing and editing tasks equitably\nSomeone needs to manage conflicts, check for consistency, etc.\nOften this is the lead author"
  },
  {
    "objectID": "module3.html#connecting-the-pieces",
    "href": "module3.html#connecting-the-pieces",
    "title": "Tying It All Together",
    "section": "Connecting the Pieces",
    "text": "Connecting the Pieces\nWe’ve now covered how a synthesis team should approach creating and publishing its main research outputs (data, code, results). Now we’ll discuss how to begin making these useful to the world, which starts with making sure the products of synthesis research point to each other. Lets begin with an activity.\n\nActivity 2: Synthesis project detective\nEstimated time: 12 min\nForm breakout groups and course instructors will assign each one a link to a product from a synthesis project (the code, a paper, a dataset, etc.). Using any means necessary (metadata, web search, etc.) figure out what other products are related (other publications, source/derived data, etc.) and who is involved in the synthesis team. Answer these questions as a group:\n\nIf your group received a link to a paper, were you able to find datasets and a code repository (for an analytical workflow)?\nIf your group received a link to a code repository, were you able to find papers and datasets?\nIf your group received a link to a dataset, were you able to find papers and a code repository?\nWho was involved in the synthesis project?\nCould you understand the overall scope and impact of the synthesis project? Why or why not?\n\n\nGroup 1Group 2Group 3Group 4Group 5\n\n\nClue: SoDAH\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the SoDAH LTER synthesis working group. The group\nPapers\n\n…\n…\n…\n\nWorkflows\n\n…\n\nDatasets\n\n…\n\nOther\n\n…\n\n\n\n\n\n\nClue: https://corredata.weebly.com/\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the CoRRE synthesis working group. The group\nPapers\n\n…\n…\n…\n\nWorkflows\n\n…\n\nDatasets\n\n…\n\nOther\n\n…\n\n\n\n\n\n\nClue: https://github.com/lter/lterwg-silica-data\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the silica exports working group. The group\nPapers\n\n…\n…\n…\n\nWorkflows\n\n…\n\nDatasets\n\n…\n\nOther\n\n…\n\n\n\n\n\n\nClue:\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the…\nPapers\n\n…\n…\n…\n\nWorkflows\n\n…\n\nDatasets\n\n…\n\nOther\n\n…\n\n\n\n\n\n\nClue:\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the…\nPapers\n\n…\n…\n…\n\nWorkflows\n\n…\n\nDatasets\n\n…\n\nOther\n\n…\n\n\n\n\n\n\n\n\n\nMore ways to synthesize\n\nWe’ve talked about the three most common products of synthesis: papers, datasets, and workflows. But, we’ve also seen that there are plenty of other ways to share synthesis research! Education and outreach can become an important goal in for some synthesis teams, and providing access to data and actionable research results, such as forecasts, can be very useful to stakeholders. As time goes, on synthesis teams may produce many things that meet these goals and needs, moving well beyond the three kinds of products we’ve already talked about. See below for a few ideas and examples.\n\nTeaching materialsWeb appsAutomationProject websites\n\n\nSynthesis research produces new scientific knowledge that other researchers, students, or stakeholders can learn and build on. Synthesis can also generate applied-science tools and methods that others need to learn how to use for themselves. Teaching modules are an important way of sharing both of these outcomes, and of broadening the reach of a synthesis project.\nExamples:\n\nThe EDDIE project is a clearinghouse of contributed teaching materials for the earth and environmental sciences.\nThis website is an example of teaching materials produced by a synthesis team.\n\n\n\nInteractive web applications can provide users with easy access to scientific datasets, especially large ones, analytical results, visualizations, interpretation, and many, many other things. Creating web apps is not necessarily an easy task, but if your synthesis team has the expertise, or access to web developers, web apps may be useful for outreach, or as tools the synthesis team itself can use. Frameworks like Shiny (for R), Streamlit, or Flask (both for python), and services like Shinyapps.io and Plotly, can make creation of apps relatively painless.\nExamples\n\nA dashboard app for the NEON ecological forecasting challenge.\nThe Jornada LTER interactive viewer for weather station data.\n\n\n\nSome research efforts have developed automation systems for research data processing, analytics, and publishing. These often fall into the “continuous integration/continuous deployment” class of web-enabled software and data pipelines, in which one software processes (data processing, analytics, publication, etc.) may be automatically triggered by events that occur in another, connected software service (such as adding new data to a GitHub repository). These technologies enable researchers to build software pipelines that can be useful for quality control of new data, updating forecasts, and rapid deployment of data or analysis products.\nExamples:\n\nThe Portal Project in southeast Arizona has developed a well-described near-term ecological forecasting pipeline.16\nAutomated quality control of dendrometer band data.17\nForecasting Lake and Reservoir Ecosystems (FLARE) project.\n\n\n\nAt a certain point, the outputs of a synthesis project can become numerous and challenging to present to the public in an organized way. Project websites can serve as a gateway to an entire synthesis project by providing comprehensive listings of project outputs (papers, datasets, GitHub repositories, etc), a narrative for the research, appealing images or graphics for outreach, and links to related projects, funders, or institutions. GitHub Pages sites are a common solution for creating simple, cost-effective (free, usually) project websites nowadays, but there are other options. A good project website can become a cohesive, engaging clearinghouse for information about a synthesis project, but they can become laborious to create and keep up-to-date.\nExamples:\n\nThe Portal Project\nThe CoRRE project\n\n\n\n\n\n\nLinking synthesis products together\nReflecting on all the information above, we can see one common feature of the many different products of a synthesis team: they exist primarily as digital objects on the internet. The internet may seem fluid, but fortunately there are ways to identify and connect these digital objects in a stable way.\n\nPersistent identifiers\nPersistent identifiers, or PIDs, are references to digital objects that are intended to last a long time. For objects on the internet, they are intended to be unique, i.e. having a 1:1 relationship between the PID and the digital object, and machine actionable, meaning they can be understood by software like web browsers. There are many different types of PIDs, but the most useful ones in the context of publishing research products are:\n\nDigital Object Identifiers (DOI), used to identify digital publications like journal articles, datasets, or governement reports.\nOpen Researcher and Contributor ID (ORCID), used to identify individuals, usually in the context of research or publishing activities.\nResearch Organization Registry (ROR), used to identify organizations, also in the context of research and publishing, primarily.\n\nThese identifiers can and should be associated with all journal articles and published datasets resulting from synthesis projects. DOIs and ORCIDs can easily used code products or associated with GitHub repositories as well.\n\n\nCiting synthesis products\nThe best way to ensure that use of a research product is recognized is through proper citation. This is already common practice for journal articles, but is only recently being adopted for published datasets. The most logical place in an article to cite a published dataset is in the Methods section and in the Data Availability Statement, which most reputable journals now require. Be sure to check journal data sharing requirements well in advance so that data publication preparation can begin early enough. When citing datasets, be sure that the full bibliographic entry is correctly included in the article’s References list. Citation of code is not as widely practiced, but some journals require it and it is a best practice.\n\nA useful data availability statementNot as helpful\n\n\nFrom Currier and Sala 202218. Note that source datasets are properly cited in the Data Availability Statement, meaning an in-text citation is given and the full bibliographic entry is provided in the article reference list. The DOIs included here are helpful for quickly finding the data.\n\nAll original and derived phenology data produced by the authors, and R scripts for data processing, statistical analyses, and figure production are publicly available in the Environmental Data Initiative (EDI) repository. EDI package knb-lter-jrn.210574001.2 (Currier & Sala, 2022a) contains daily phenocam image data and derived timeseries and associated scripts for processing and is available at https://doi.org/10.6073/pasta/836360dce9311130383c9672e836d640. EDI package knb-lter-jrn.210574002.2 (Currier & Sala, 2022b) contains observed phenological indicators and environmental drivers as well as associated scripts for final analyses and figure construction presented in this manuscript and these data are available at https://doi.org/10.6073/pasta/d327a77f6474131db8aa589011e29c29. No novel code was generated by the authors of this manuscript. The precipitation data used in all analyses are derived from G-BASN data in EDI package knb-lter-jrn.210520001 (Yao et al., 2020) available at https://doi.org/10.6073/pasta/cf3c45e5480551453f1f9041d664a28f. Daily air temperature summaries from 4 June 1914 to the present for the Jornada Experimental Range Headquarters (NOAA station GHCND:USC00294426) are freely available upon request via the National Ocean and Atmospheric Administration (https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USC00294426/detail).\n\n\n\n\nData used in the figures are included in the supplementary material. The full dataset will be provided upon reasonable request to the corresponding author."
  },
  {
    "objectID": "module3.html#maintaining-momentum",
    "href": "module3.html#maintaining-momentum",
    "title": "Tying It All Together",
    "section": "Maintaining Momentum",
    "text": "Maintaining Momentum\nAs we discussed in Module 1, starting a synthesis project benefits from motivating scientific questions, a well-planned foundation for team science, and significant activation energy from the team. When successful, synthesis projects gather enough momentum to be productive for many years. Below are a few ideas on how to maintain this momentum.\n\nGive everyone credit\nEveryone deserves credit for the work they do, and in academic environments, this is too often overlooked. Synthesis working groups commonly begin without any dedicated personnel support, which means that some participants, usually early-career scientists, will be contributing unpaid time to the project. In the absence of pay, leaders of a synthesis team should take the initiative to make sure everyone receives appropriate credit and opportunities for career advancement when they contribute to the project. Here are a few ways to do that\n\nMake sure all contributors have an ORCID. They are easy to obtain.\nUse ORCIDs whenever contributors are associated with a research product (if possible).\nDefine the type of contributions team members have made\n\nDecide this in advance.\nThe CRediT framework is a good starting point.\n\n\nIf there is no formal credit mechanism available (such as for a website), list each contributor by name, along with affiliations, bios, links to other profiles, and other information as desired.\n\n\nEncourage new contributions\nInterests and commitment to synthesis projects change over time. To sustain active research in the team, and with the data, make sure new people can find a way to contribute.\n\nProvide a path for new data contributions. This follows from making the data preparation/harmonization workflow reproducible.\nHave open meetings when possible.\nGive new team members the freedom and support to lead analyses and papers.\n\n\n\nFind monetary support\nMaintaining momentum for a synthesis project over the long term is highly dependent on the ability to support dedicated personnel time.\n\nRefer to funding sources in Module 1\nPersonnel support may need to come from larger grants."
  },
  {
    "objectID": "module3.html#footnotes",
    "href": "module3.html#footnotes",
    "title": "Tying It All Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18↩︎\nGoFAIR initiative↩︎\nThe FAIR Cookbook↩︎\nBahim, C., Casorrán-Amilburu, C., Dekkers, M., Herczog, E., Loozen, N., Repanas, K., Russell, K. and Stall, S. (2020) ‘The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments’, Data Science Journal, 19(1), p. 41. Available at: https://doi.org/10.5334/dsj-2020-041.↩︎\nGries, Corinna, et al. “The environmental data Initiative: Connecting the past to the future through data reuse.” Ecology and Evolution 13.1 (2023): e9592. https://doi.org/10.1002/ece3.9592↩︎\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nProvenance metadata at the EDI repository↩︎\nLerner, et al., “Making Provenance Work for You”, The R Journal, 2023. https://journal.r-project.org/articles/RJ-2023-003/↩︎\nEnd-to-End Provenance↩︎\nList of LTER Information Managers↩︎\nThe Registry of Research Data Repositories (re3data.org)↩︎\nData curation network↩︎\nThe Registry of Research Data Repositories (re3data.org)↩︎\nGitHub documentation for referencing and citing content↩︎\nWhite EP, Yenni GM, Taylor SD, et al. Developing an automated iterative near-term forecasting system for an ecological study. Methods Ecol Evol. 2019; 10: 332–344. https://doi.org/10.1111/2041-210X.13104↩︎\nWhite EP, Yenni GM, Taylor SD, et al. Developing an automated iterative near-term forecasting system for an ecological study. Methods Ecol Evol. 2019; 10: 332–344. https://doi.org/10.1111/2041-210X.13104↩︎\nKim, A. Y., Herrmann, V., Barreto, R., Calkins, B., Gonzalez-Akre, E., Johnson, D. J., Jordan, J. A., Magee, L., McGregor, I. R., Montero, N., Novak, K., Rogers, T., Shue, J., & Anderson-Teixeira, K. J. (2022). Implementing GitHub Actions continuous integration to reduce error rates in ecological data collection. Methods in Ecology and Evolution, 13, 2572–2585. https://doi.org/10.1111/2041-210X.13982↩︎\nCurrier, Courtney M., and Osvaldo E. Sala. 2022. “ Precipitation versus Temperature as Phenology Controls in Drylands.” Ecology 103(11): e3793. https://doi.org/10.1002/ecy.3793↩︎"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "contributing.html#overview",
    "href": "contributing.html#overview",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  }
]