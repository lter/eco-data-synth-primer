[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "The course is currently being organized by Greg Maurer (gmaurer@nmsu.edu), with generous assistance from Marty Downs and Nick Lyon. See the Contributors section for more information."
  },
  {
    "objectID": "people.html#contributors",
    "href": "people.html#contributors",
    "title": "People",
    "section": "Contributors",
    "text": "Contributors\nWe have a long list of synthesis scientists actively involved in the course working group who are contributing expertise and instructional content, and many will be present as instructors at the 2024 ESA Meeting. Names and web profiles are listed below in alphabetical order by last name.\nIcon legend:  = website |  = publications |  = GitHub |  = ORCID\n\nKathryn Barry\nJoanna Carey – |  \nAngel Chen (she/her) – LTER Network Office |   \nLaura Dee – |   \nMarty Downs – LTER Network Office |  \nStevan Earl – |   \nSarah Elmendorf – |   \nJalene LaMontagne – |   \nNick J Lyon (they/them) – LTER Network Office |   \nGregory Maurer (he/him) – |   \nColin Smith\nEric Sokol\nAlexandra (Sasha) Wright"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "People",
    "section": "Other Contributors",
    "text": "Other Contributors\nIn addition to those above, several people contributed to early iterations on the concept and content of the course.\n\nForest Isbell\nKim Komatsu"
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "A Course for Collaborative Ecologists",
    "section": "Abstract",
    "text": "Abstract\nIn recent decades, ecology has become a more collaborative discipline motivated by the search for generality across ecosystems. At the same time, the availability, quantity, and quality of environmental data have grown rapidly, creating opportunities for re-use of these data in ecological synthesis research. Though synthesis research is complex and demanding, taking an inclusive and collaborative approach to both the scientific process and the data pays dividends throughout the lifetime of a project. This short course is a survey of methods for making ecological synthesis research a “team sport”.\nObjectives for learners are twofold:\n\nDevelop an end-to-end (conception to publication) plan for collaborative synthesis research\nGain data synthesis skills that are immediately useful in a research team setting.\n\nInstructors will cover assembling the team, study design, communication, collecting primary data sources, assembly/harmonization of data, analytical workflows, and publication of derived datasets. The course uses real-world examples, demonstrations, and interactive lessons. Ecologists with synthesis experience will be on hand with seasoned research advice and data tips. Many workshop activities will be oriented toward helping learners develop their own ideas and plans for ecological data synthesis."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "A Course for Collaborative Ecologists",
    "section": "Agenda",
    "text": "Agenda\nThis agenda is subject to change!\n\n\n\nTiming (PT)\nContent\n\n\n\n\n1-1:15p\nWelcome & Introductions\n\n\n1:15-2:15p\nModule 1: Starting with Team Science\n\n\n2:15-3p\nModule 2: Operational Synthesis\n\n\n3-3:15p\nBreak\n\n\n3:15-4p\nModule 3: Tying It All Together"
  },
  {
    "objectID": "index.html#introductions-icebreaker",
    "href": "index.html#introductions-icebreaker",
    "title": "A Course for Collaborative Ecologists",
    "section": "Introductions & Icebreaker",
    "text": "Introductions & Icebreaker\nBefore we begin, we’d love to get a sense for who you all are and why you’re interested in synthesis work! To that end, we’ll take a few minutes and go around the room for introductions. Please include:\n\nYour name and pronouns\nA 1-sentence summary of your work\nBriefly, why are you interested in synthesis?"
  },
  {
    "objectID": "index.html#note-on-course-materials",
    "href": "index.html#note-on-course-materials",
    "title": "A Course for Collaborative Ecologists",
    "section": "Note on Course Materials",
    "text": "Note on Course Materials\nWhile we are excited to offer this short course for the first time at ESA 2024, we’ve chosen to assemble these materials as a living website so that we can revisit and improve the materials over time. So, we recommend that you save the link to this site so that you can have easy access to these materials now and as they are refined going forward.\nIf you are a  GitHub aficionado, we have deployed this website via GitHub Pages so you could also “star” the website’s repository. Simply click the  GitHub octocat logo on the right side of the navbar (at the top of teh screen) to be redirected to the GitHub repository underpinning this website.\nFinally, we have developed all of this website using Quarto."
  },
  {
    "objectID": "index.html#photo-credit",
    "href": "index.html#photo-credit",
    "title": "A Course for Collaborative Ecologists",
    "section": "Photo Credit",
    "text": "Photo Credit\nIn the collage above: Jacob Bøtter via Flickr, CC BY-SA 2.0 | Jeremy Yoder via Flickr, CC BY-SA 2.0 | Marco Pfeiffer, CC BY-SA 4.0 | Gabriel De La Rosa, CC BY-SA 4.0 | Weecology lab CC BY 4.0 | NEON (National Ecological Observatory Network)"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "contributing.html#overview",
    "href": "contributing.html#overview",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the features that make published data easily re-useable\nEvaluate the metadata and fitness-for-use of published synthesis datasets\nUnderstand the three primary “products” that come out of synthesis groups\nUnderstand concepts of data provenance, reproducible analysis, and citations\nEvaluate the reproducibility of a recent data synthesis project\nUnderstand the different models of data accessibility, licensing, and authorship practices and apply them to a synthesis group’s desired outcomes\nUnderstand several funding opportunities that for synthesis research, their requirements and expectations, and their respective strengths and weaknesses for starting and sustaining synthesis research.\nCreate a plan to maintain a synthesis project and associated data over the long-term"
  },
  {
    "objectID": "module3.html#learning-objectives",
    "href": "module3.html#learning-objectives",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the features that make published data easily re-useable\nEvaluate the metadata and fitness-for-use of published synthesis datasets\nUnderstand the three primary “products” that come out of synthesis groups\nUnderstand concepts of data provenance, reproducible analysis, and citations\nEvaluate the reproducibility of a recent data synthesis project\nUnderstand the different models of data accessibility, licensing, and authorship practices and apply them to a synthesis group’s desired outcomes\nUnderstand several funding opportunities that for synthesis research, their requirements and expectations, and their respective strengths and weaknesses for starting and sustaining synthesis research.\nCreate a plan to maintain a synthesis project and associated data over the long-term"
  },
  {
    "objectID": "module3.html#introduction",
    "href": "module3.html#introduction",
    "title": "Tying It All Together",
    "section": "Introduction",
    "text": "Introduction\nSo far, we’ve made the point that ecological synthesis research is collaborative and inclusive, and that it integrates a wide range of data. Synthesis research is also intended to be influential and useful. There are many definitions of “influential and useful” to consider here, but successful synthesis research tends to expand the boundaries of knowledge and aims to improve human lives or the environment. The ability to accomplish this in synthesis research frequently depends on what knowledge or products are created, and how the synthesis team disseminates and communicates them to the outside world.\n\nThere are three interconnected, publishable products for a synthesis project (or any research project, really): the data, analytical workflows (code for data cleaning or statistics, for example), and research results. Each of these elements is a valuable product of synthesis science, and each one should reference the others. In this module we’ll discuss the mechanics of publishing each one, and then how they can be connected and made accessible for the long-term."
  },
  {
    "objectID": "module3.html#designing-and-publishing-synthesis-datasets",
    "href": "module3.html#designing-and-publishing-synthesis-datasets",
    "title": "Tying It All Together",
    "section": "Designing and Publishing Synthesis Datasets",
    "text": "Designing and Publishing Synthesis Datasets\nEstimated time: 12 min\nIn Module 2 we discussed some considerations for creating and formatting harmonized data files useful for synthesis research. We also introduced the importance of metadata for describing data and making it more usable. Publishing harmonized data files and descriptive metadata together as a dataset ensures that the data products produced by a synthesis team are findable, accessible, interoperable, and reusable (FAIR). FAIR data are an important output for almost any ecological synthesis project.\n\n\n\n\n\n\nMore about Findable, Accessible, Interoperable, Reusable (FAIR) data\n\n\n\n\n\nThe FAIR principles, standing for Findability, Accessibility, Interoperability, and Reusability, are a community-standard set of guidelines for evaluating the quality and utility of published research data. Making an effort to meet the FAIR criteria promotes both human and machine usability of data, and is a worthy objective when preparing to publish the data products from a synthesis research project.\nThe FAIR principles were first defined in the paper by Wilkinson et al (2018)1. Since this time, many resources have arisen to guide the implementation the FAIR principles23, and to quantify FAIR data successes and failures in the research and publishing communities45."
  },
  {
    "objectID": "module3.html#metadata",
    "href": "module3.html#metadata",
    "title": "Tying It All Together",
    "section": "Metadata",
    "text": "Metadata\nMetadata are data about the data. As a general rule, metadata should describe\n\nWho collected the data\nWhat was observed or measured\nWhen the data were collected\nWhere the data were collected\nHow the data were collected (methods, instruments, etc.)\n\nOftentimes, including information about why the data were collected can help future users understand the context of the data and use them. Including metadata of this nature makes data more usable, and helps prevent the deterioration of information about data over time, as illustrated in the figure below (from Michener et al. 19976).\n\n\n\nExample of the normal degradation in information content associated with data and metadata over time (“information entropy”). Accidents or changes in technology (dashed line) may eliminate access to remaining raw data and metadata at any time (Michener et al 1997.\n\n\n\nData Provenance Metadata\n\nProvenance metadata deserves special attention for ecological data synthesis projects. Data provenance refers to information detailing the origin of the values in a dataset, which is particularly important for synthesis projects that bring together data from many different sources. Synthesis activities typically produce new data products that are derived from the original source data after they have been cleaned, harmonized, and analyzed. Provenance metadata should be included with the derived products to point back to the original source data, similar to the way bibliographic references point to the source material for a book or scholarly article.\nA few other notes on provenance:\n\nAt its simplest, documenting data sources as you collect and analyze the source data is a great start on provenance metadata.\nMany data repositories provide guidelines, tools, and features for data provenance metadata7.\nProvenance metadata can become very detailed if the software and computing environment is also taken into account. This is an active area of study 89.\n\n\n\nLicensing\nPublished datasets should include a license in every copy of the metadata that defines who has what rights to use, reproduce, or distribute the data. Licensing decisions should be made in consultation with the synthesis team after considering the nature of the data (does it contain human subject data, for instance?), its origin (including restrictions on source data, if applicable), and the requirements of the funders and institutions associated with the project. For publicly-funded environmental research data, it is generally appropriate to use open licenses, and the Creative Commons CC-BY attribution, and CC0 public domain, licenses are probably a good choice for most ecological synthesis data. This is not legal advice and your mileage may vary.\n\n\nMetadata Creation and Management\nAssembling metadata should be an integral part of the data synthesis activities discussed in Module 2, and can even be built-in to the workflow and project management practices of a project. Make sure to plan for and start creating metadata early in a synthesis project. Below are a few ways to do that.\n\nKeep a detailed project log and populate it with metadata for the project, including information like\n\nwhat source data the team is using and where they came from.\nhow data are being analyzed and methods used to create derived products.\nwho is doing what.\n\nStart creating distinct publishable datasets (data plus metadata) as data are processed and analyzed. The team can do this\n\nlocally, using a labeled directory for the cleaned, harmonized, of derived data, along with related code and metadata files. Metadata files may be plain text, or use a metadata template.\nwith a repository-based metadata editor, such as ezEML from the Environmental Data Initiative (EDI) repository.\n\nGet a professional data manager or data curator involved with the synthesis project. For example, the LTER Network has a community of “Information Managers” 10 trained in data management, metadata creation, and data publishing. Research data repositories11 and associated data curators12 may also be a good resource."
  },
  {
    "objectID": "module3.html#deciding-what-to-publish",
    "href": "module3.html#deciding-what-to-publish",
    "title": "Tying It All Together",
    "section": "Deciding What to Publish",
    "text": "Deciding What to Publish\n\nThe overall design of the dataset to be published is often difficult to imagine, particularly for people new to using or creating datasets. One of the most common questions data managers hear is “What should I publish?” The answer usually comes down to:\n\nPublish any data used to generate research results.\nPublish any data that will be used by others (scientists, managers, public stakeholders), including raw data.\nIf reproducibility is of interest or concern, publish the workflow (usually code) used to process or analyze the data, or to generate research results like figures.\nAlways publish descriptive metadata about any of the above.\n\nIn the activity below we will browse a few published datasets to get a feel for what useful data does and doesn’t look like. You can also look at advice from a repositories like EDI and BCO-DMO, or from a research network like NEON."
  },
  {
    "objectID": "module3.html#choosing-and-publishing-to-a-repository",
    "href": "module3.html#choosing-and-publishing-to-a-repository",
    "title": "Tying It All Together",
    "section": "Choosing and Publishing to a Repository",
    "text": "Choosing and Publishing to a Repository\nThere are many, many research data repositories available to researchers now13, making the choice of where to publish data fairly challenging. A few basic data repository features are essential when publishing a synthesis dataset. First, the repository should issue persistent, internet-resolveable, unique identifiers for every dataset published. Generally this will be a Digital Object Identifier, or DOI, that can be cited every time the dataset is used after publication. Second, repositories should require, and provide the means to create/publish, metadata describing each dataset. Without requiring at least minimal metadata, no repository can ensure that published data are FAIR. Finally, research data repositories should be stable and well supported so that data remain available and usable in perpetuity. Choosing a repository from the CoreTrustSeal certified repository list is one way to assess this. Beyond this, asking a few questions about the dataset will help with repository selection:\n\nWho are the likely users for this data? Will they belong to a specific scientific discipline, research network, or community of stakeholders?\nHow specialized are your data? Do they fall into a common data type or follow a speical formatting standard?\nWill the data be updated regularly?\n\n\n\n\nA limited slice from the broad spectrum of research data repositories available for publishing synthesis data.\n\n\nAfter making a choice, the process of publishing data varies from repository to repository. More specialized repositories tend to offer enhanced documentation, custom software tools, or even data curation staff to assist users with data publication. It also helps to consult a project data manager if one is available to the synthesis team."
  },
  {
    "objectID": "module3.html#activity-1-evaluate-published-datasets",
    "href": "module3.html#activity-1-evaluate-published-datasets",
    "title": "Tying It All Together",
    "section": "Activity 1: Evaluate published datasets",
    "text": "Activity 1: Evaluate published datasets\nEstimated time: 10 min\nForm breakout groups and course instructors will assign each group a dataset (a DOI) for evaluation. With your group, answer these questions about the dataset:\n\nWhere were the data collected?\nWhat variables were measured and in what units?\nWhat is the origin of the data and how have they been altered since collection?\nWere the first three questions easy to answer? Why or why not?\n\n\nExample 1Example 2Example 3\n\n\nData from the SoDaH LTER synthesis working group.\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=edi.521.1\n\n\nA Dryad dataset from a synthesis paper about oligotrophication.\nhttps://doi.org/10.5061/dryad.v2k2607\n\n\nMaybe this: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.493.16 (needs editing)\nMaybe this: https://doi.org/10.6084/m9.figshare.10735652.v1 (but pretty bad)\n\n\n\n\nAdditional Data Publishing Resources\n\nNEON’s derived data publishing guide\nEDI repository data authorship guide"
  },
  {
    "objectID": "module3.html#sharing-the-synthesis-workflow",
    "href": "module3.html#sharing-the-synthesis-workflow",
    "title": "Tying It All Together",
    "section": "Sharing the Synthesis Workflow",
    "text": "Sharing the Synthesis Workflow\nOne of the most valuable, shareable outputs of synthesis research is the analytical workflow used to derive datasets and produce scientific results. Most often, these workflows are written in computer code, such as R, Python, or another language. Workflows may consist of a collection of scripts, or they may be organized into stand-alone modules or libraries. The latter is easier to share and re-use, but requires more advanced knowledge of software design. Sharing workflows and code are one of the most important needs for ensuring the reproducibility of science.\nPublishing the workflow also gives interested parties an understanding of\n\nthe origin of the data\nthe process, or the code itself, for data cleaning, harmonization, analysis, and presentation of results (figures), which may be useful in future work\nhow the workflow was developed or changed over time\nthe contributions made by the team\n\nIn other parts of the course, we have strongly recommended using version control and collaboration platforms to manage coding, writing, and other elements of the synthesis team workflow. In particular, we have focused on using GitHub as a one-stop shop for many of these tasks. In combination with other software and services, GitHub can be reliably used to publish workflows as well. By integrating with repositories that archive GitHub content and issue a DOI (commonly Zenodo)14, workflows can be published and cited by the research products that they were used to generate. This is commonly done in near-term ecological forecasting projects 15."
  },
  {
    "objectID": "module3.html#disseminating-research-results",
    "href": "module3.html#disseminating-research-results",
    "title": "Tying It All Together",
    "section": "Disseminating Research Results",
    "text": "Disseminating Research Results\n\nMost often this means writing a paper\nData papers\n\nAnatomy of a data paper?\n\nOther products may be apps, teaching modules, etc.\n\nMacrosystemsEDDIE as an example?\nShiny apps"
  },
  {
    "objectID": "module3.html#connecting-the-elements-of-a-synthesis-project",
    "href": "module3.html#connecting-the-elements-of-a-synthesis-project",
    "title": "Tying It All Together",
    "section": "Connecting the Elements of a Synthesis Project",
    "text": "Connecting the Elements of a Synthesis Project\nEstimated time: 25 min"
  },
  {
    "objectID": "module3.html#citing-synthesis-products",
    "href": "module3.html#citing-synthesis-products",
    "title": "Tying It All Together",
    "section": "Citing Synthesis Products",
    "text": "Citing Synthesis Products"
  },
  {
    "objectID": "module3.html#giving-everyone-credit",
    "href": "module3.html#giving-everyone-credit",
    "title": "Tying It All Together",
    "section": "Giving Everyone Credit",
    "text": "Giving Everyone Credit"
  },
  {
    "objectID": "module3.html#activity-2-synthesis-project-detective",
    "href": "module3.html#activity-2-synthesis-project-detective",
    "title": "Tying It All Together",
    "section": "Activity 2: Synthesis project detective",
    "text": "Activity 2: Synthesis project detective\nEstimated time: 12 min\nForm breakout groups and course instructors will assign each one a link to a product from a synthesis project (the code, a paper, a dataset, etc.). Using any means necessary (metadata, web search, etc.) figure out what other products are related (other publications, source/derived data, etc.) and who is involved in the synthesis team. Answer these questions as a group:\n\nIf your group received a link to a paper, were you able to find datasets and an analytical workflow?\nIf your group received a link to a code repository, could you track down papers and datasets?\nIf your group received a link to a dataset, were the connected to papers and an analytical workflow?\nWho was involved in the synthesis project?\nCould you understand the overall scope and impact of the synthesis project? Why or why not?\n\n\nExample 1Example 2Example 3\n\n\nSoDAH\n\n\nCoRRE or metacommunities\nhttps://corredata.weebly.com/\n\n\nSilica Exports\nhttps://github.com/lter/lterwg-silica-data"
  },
  {
    "objectID": "module3.html#maintaining-momentum",
    "href": "module3.html#maintaining-momentum",
    "title": "Tying It All Together",
    "section": "Maintaining Momentum",
    "text": "Maintaining Momentum\nEstimated time: 10 min?"
  },
  {
    "objectID": "module3.html#funding-sources",
    "href": "module3.html#funding-sources",
    "title": "Tying It All Together",
    "section": "Funding Sources",
    "text": "Funding Sources\n???"
  },
  {
    "objectID": "module3.html#footnotes",
    "href": "module3.html#footnotes",
    "title": "Tying It All Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18↩︎\nGoFAIR initiative↩︎\nThe FAIR Cookbook↩︎\nBahim, C., Casorrán-Amilburu, C., Dekkers, M., Herczog, E., Loozen, N., Repanas, K., Russell, K. and Stall, S. (2020) ‘The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments’, Data Science Journal, 19(1), p. 41. Available at: https://doi.org/10.5334/dsj-2020-041.↩︎\nGries, Corinna, et al. “The environmental data Initiative: Connecting the past to the future through data reuse.” Ecology and Evolution 13.1 (2023): e9592. https://doi.org/10.1002/ece3.9592↩︎\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nProvenance metadata at the EDI repository↩︎\nLerner, et al., “Making Provenance Work for You”, The R Journal, 2023. https://journal.r-project.org/articles/RJ-2023-003/↩︎\nEnd-to-End Provenance↩︎\nList of LTER Information Managers↩︎\nThe Registry of Research Data Repositories (re3data.org)↩︎\nData curation network↩︎\nThe Registry of Research Data Repositories (re3data.org)↩︎\nGitHub documentation for referencing and citing content↩︎\nWhite EP, Yenni GM, Taylor SD, et al. Developing an automated iterative near-term forecasting system for an ecological study. Methods Ecol Evol. 2019; 10: 332–344. https://doi.org/10.1111/2041-210X.13104↩︎"
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nSummarize the advantages of creating a defined contribution workflow\nDefine fundamental vocabulary of version control systems\nCreate a repository on GitHub\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly"
  },
  {
    "objectID": "module2.html#learning-objectives",
    "href": "module2.html#learning-objectives",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nSummarize the advantages of creating a defined contribution workflow\nDefine fundamental vocabulary of version control systems\nCreate a repository on GitHub\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly"
  },
  {
    "objectID": "module2.html#introduction",
    "href": "module2.html#introduction",
    "title": "Operational Synthesis",
    "section": "Introduction",
    "text": "Introduction\nHere are a few serviceable definitions of what research is:\n\n“Creative and systematic work undertaken in order to increase the stock of knowledge” from the 2015 Frascati Manual\n“Studious inquiry or examination,” and especially “investigation or experimentation aimed at the discovery and interpretation of facts” from the Merriam-Webster dictionary\n\nTo these basic definitions of research, our definition of synthesis research adds collaborative work, and the integration and analysis of a wide range of data sources, to achieve a more complete, generalizable, or useful research result. In Module 1 we discussed many of the collaborative considerations for synthesis research, including creating a diverse and inclusive team, asking synthesis-ready scientific questions (often broad in scope or spatial scale), and finding suitable information (or data) from a wide variety of sources to answer those questions. Once the synthesis team moves into the operational phase of research, which includes the integration and analysis of data, there are some key activities that must happen:\n\nCleaning and harmonizing data to make it usable\nAnalyzing data to answer questions\nInterpreting the results of your analysis\nWriting the papers or creating other research products\n\nWe’ve already seen that creating a collaborative, inclusive team can set the stage for successful synthesis research. Each of the operational activities above will also benefit from this mindset, and in this module we highlight some of the most important considerations and practices for a team science approach to the nuts-and-bolts of synthesis research."
  },
  {
    "objectID": "module2.html#reproducibility-practices",
    "href": "module2.html#reproducibility-practices",
    "title": "Operational Synthesis",
    "section": "Reproducibility Practices",
    "text": "Reproducibility Practices\n\n\n\nMaking one’s work “reproducible”–particularly in code contexts–has become increasingly popular but is not always clearly defined. For the purposes of this short course, we believe that reproducible work:\n\nUses scripted workflows for all interactions with data\nContains sufficient documentation for those outside of the project team to navigate the project’s contents\nContains detailed metadata for all data products\nAllows anyone to recreate the entire workflow from start to finish\nLeads to modular, extensible research projects. Adding data from a new site, or a new analysis, should be relatively easy in a reproducible workflow.\n\n\n Contributing\n\nCreate a formal plan for collaborating with which your whole team agrees\nQuarantine external inputs\nPlan for “future you”\nCommunicate to your collaborators whenever you’re working on a specific script to avoid conflicting edits\n\n\n\n Documentation\n\n\n\nOne folder per project\nFurther organize content via sub-folders\nMake file names informative and intuitive\n\nAvoid spaces and special characters in file names\nFollow a consistent naming convention throughout\nGood names should be machine readable, human readable, and sorted in a useful way\n\nUse READMEs to record organization rules / explanation\nKeep a log of where source data came from.\n\nWhere did you search?\nWhat search terms did you use?\nList the dataset identifiers you downloaded/used\nIdeally, include downloading data as part of your scripted workflow\n\n\n\n\n\n\nExample project structure:\n project_new\n |–   README.txt\n |–   01_grant_management\n |–   02_project_coordination\n |–   03_documentation\n |–   04_participant_tracking\n |–   05_data\n |       |–   README.txt\n |       |–   hydrology\n |       L   water_chemistry\n |–   06_src\n |       |–   README.txt\n |       |–   data_aggregation\n |       |–   data_harmonization\n |       L   modeling\n L   06_publications\n        L   biogeochemistry\n\n\n\n\n\n Code\n\nUse a version control system\nLoad libraries/packages explicitly\nTrack (and document) software versions\nNamespace functions (if not already required by your coding language)\n\nE.g., dplyr::mutate(mtcars, hp_disp = hp / disp)\n\nUse relative file paths that are operating system-agnostic\nBalance how descriptive object names are with striving for concise names\nUse comments in the code!\nConsider custom functions\nFor scripts that need to be run in order, consider adding step numbers to the file name\n\n\n\nSynthesis Considerations\nHow does reproducibility in synthesis considerations differ from individual / non-synthesis applications?\n\nJudgement calls need to be made / agreed to as a group\n\nBut “defer to the doers”\n\nIncreased emphasis on contribution guidelines / planning being formalized\nMore communication needs\nMust ensure that every team member has sufficient access to the project files\nIts best to keep track of who contributed what, so that everyone gets credit. This can be challenging in practice."
  },
  {
    "objectID": "module2.html#version-control",
    "href": "module2.html#version-control",
    "title": "Operational Synthesis",
    "section": "Version Control",
    "text": "Version Control\nIn all scientific research, the data work (cleaning, harmonizing, analyzing) and the writing are iterative processes. The process and products change over time and usually require a series of revisions. In synthesis research, the process can become even more complex because the team is usually large and multiple people are contributing data, analysis, writing, revisions, and more. Using version control helps manage this complexity by recording changes, tracking individual contributions, and ensuring that things can be rolled-back to an earlier state if needed.\n\n\n\n\nVocabulary\nEstimated time: 5 min\nBrief definitions for a selection of fundamental version control vocabulary terms\n\nVersion control system: software that tracks iterative changes to your code and other files\nRepository: the specific folder/directory that is being tracked by a version control system\nGit: a popular open-source distributed version control system\nGitHub: a website that allows users to store their Git repositories online and share them with others\n\n\n\nGitHub\nEstimated time: 10 min\nWhile this section of the module focuses on GitHub, there are several other viable alternatives for working with Git individually or as part of a larger team (e.g., GitLab, GitKraken, etc.). Any of these may be viable option for your team and we focus on GitHub here only to ensure a standard backdrop for the case studies we’ll discuss shortly.\nThere are a lot of GitHub tutorials that exist already so, rather than add our own variant to the list, we’ll work through part of one created by the Scientific Computing team of the National Center for Ecological Analysis and Synthesis (NCEAS).\nSee the workshop materials here.\nGiven the time restrictions for this short course, we’ll only cover how you engage with GitHub directly through the GitHub website. However, your chosen software for writing code will certainly have a method of connecting to GitHub/etc., so if this topic is of interest it will be beneficial for you to search out the relevant tutorial."
  },
  {
    "objectID": "module2.html#data-preparation",
    "href": "module2.html#data-preparation",
    "title": "Operational Synthesis",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe scientific questions being asked in synthesis projects are usually broad in scope, and it is therefore common to bring together many datasets from different sources for analysis. The datasets selected for analysis (source data) may have been collected by different people, in different places, using different methods, as part of different projects… or all of the above. Typically, some amount of data cleaning - filtering or removing unwanted observations - and data harmonization - putting data together in common structures, file formats, and units of measurement - is necessary before analysis can begin. This process can be easy or difficult depending on the quality of the source data, the differences between source data, and how much metadata (see callout below) is available to understand them.\n\n\n\n\n\n\nMore about Metadata\n\n\n\n\n\nMetadata is “data about the data,” or information that describes who collected the data, what was observed or measured, when the data were collected, where the data were collected, how the observations or measurements were made, and why they were collected. Metadata provide important contextual information about the origin of the data and how they can be analyzed or used. They are most useful when attached or linked to the data being described, and data and related metadata together are commonly referred to as a dataset.\nMetadata for ecological research data are well described in Michener et al (1997),1 but there are many other kinds of metadata with different purposes.2 If you are publishing a research dataset and have questions about metadata, ask a data manager for your project, or staff at the repository you are working with, for help. Either can typically provide guidance on creating metadata that will describe your data and be useful to the community (here is one example). We’ll return to the subject of metadata in Module 3.\n\n\n\n\nCleaning Data\nWhen assembling large datasets from diverse sources, as in synthesis research, not all the source data will be useful. This may be because there are real or suspected errors, missing values, or simply because they are not needed to answer the scientific question being asked (wrong variable, different ecosystem, etc.). Data that are not useful are usually excluded from analysis or removed altogether. Data cleaning tends to be a stepwise, iterative process that follows a different path for every dataset and research project. There are some standard techniques and algorithms for cleaning and filtering data, but they are beyond the scope of this course. Below are a few guidelines to remember, and more in-depth resources for data cleaning are found at the end of this section.\n\nAlways preserve the raw data. Chances are you’ll want to go back and check the original source data at least once.\nUse a scripted workflow to clean and filter the raw data, and follow the usual rules about reproducibility (comments, version control, functionalization).\nConsider using the concept of data processing “levels,” meaning that defined sets of data flagging, removal, or transformation operations are applied consistently to the data in stepwise fashion. For example, incoming raw data would be labeled “level 0” data, and “level 1” data is reached after the first set of processing steps is applied.\nSpread the data cleaning workload around! Data cleaning typically demands a HUGE fraction of the total time devoted to working with data,345 and it can be tedious work. Make sure the team shares this workload equitably.\n\n\n\nData Harmonization\nData harmonization is the process of bringing different datasets into a common format for analysis. The harmonized data format chosen for a synthesis project depends on the source data, analysis plans, and overall project goals. It is best to make a plan for harmonizing data BEFORE analysis begins, which means discussing this with the team in the early stages of a synthesis project. As a general rule, it is also wise to use a scripted workflow that is as reproducible as possible to accomplish the harmonization you need for your project. Following this guidance lets others understand, check, and adapt your work, and will also make it much, much easier to bring new data and analysis methods into the project.\nData harmonization is hard work that sometimes requires trial and error to arrive at a useful end product. At the end of this section are some additional data harmonization resources to help you get started. Looking at a simple example might also help.\n\n\n\n\n\n\nExample: Harmonizing grassland biomass data\n\n\n\nIn the figure below, two datasets from different LTER sites have been harmonized into one file for analysis. We don’t have all the metadata here, but based on the column naming we can assume that the file on the left (Konza_harvestplots.txt, yellow) contains columns for the sampling date, a plot identifier, a treatment categorical value, and measured biomass values. The filename suggests that the data come from the Konza Prarie LTER site. The file on the right (2022_clips.csv, blue) has a column denoting the LTER site, in this case Sevilleta LTER (abbreviated as SEV), as well as similar date, plot identifier, treatment, and measured biomass columns.\nThere are some similarities and some differences in these two source files. A harmonized file (lter_grass_biomass_harmonized.txt) appears below.\n\n\n\n\n\n\n\n\n\n\n\nCan you identify some changes made to data structure, variable formatting, or units to harmonize these data?\n\n\n\n\n\nEven though the data files were similar, several important changes were made to create the harmonized file. Among them:\n\nThe site column was preserved and contains the “SEV” and “KNZ” categorical values denoting which LTER site is observed in each row.\nThe dates in the date column were converted to a standard format (YYYY-MM-DD). Incidentally, this matches the International Organization for Standardization (ISO) recommendation for date/time data, making it generally a good choice for dates.\nA new rep column was created to identify the replicate measurements at each site. This is essentially a standard version the plot identifier columns (PlotID and plot) in the original data file. Also note that the original values are preserved in the new plot_orig column.\nThe treatment columns from the original data files (Treatment and trt) were standardized to one column with “C” and “F” categorical values, for control and fertilized treatments, respectively.\nThe biomass values from Konza were converted to units grams per meter squared (g/m2) because the original Konza measurements were for total biomass in 2x2 meter plots. Note that this conversion is for illustration purposes only - we can’t be sure if this conversion is correct without it being spelled out in the metadata, or asking the data provider directly.\n\n\n\n\n\n\nA Word about Harmonized Data Formats\nAbove, we’ve discussed several aspects of selecting a data format. There are at least three related, but not exactly equivalent, concepts to consider when formatting data. First, formats describe the way data are structured, organized, and related within a data file. For example, in a tabular data file about biomass, the measured biomass values might appear in one column, or in muiltiple columns. Second, the values of any variable can be represented in more than one format. The same date, for example, could be formatted using text as “June 1, 1977” or “1977-06-01.” Third, format may refer to the file format used to hold data on a disk or other storage medium. File formats like comma separated value text files (CSV), Excel files (.xlsx), JPEG images, are commonly used for research data, and each has particular strengths for certain kinds of data.\nA few guidelines apply:\n\nFor formatting a tabular dataset, err towards simpler data structures, which are usually easier to clean, filter, and analyze. Long-format tables, or tidy data 6, is one common recommendation for this.\nWhen choosing a file format, err towards open, non-proprietary file formats that more people know and have access to. Delimited text files, such as CSV files, are a good choice for tabular data.\nUse existing community standards for formatting variables and files as long they suit your project methods and scientific goals. Using ISO standards for date-time variables, or species identifiers from a taxonomic authority, are good examples of this practice.\nThere is no perfect data format! Harmonizing data always involves some judgement calls and tradeoffs.\n\nWhen choosing a destination format for the harmonized data for a synthesis project, the audience and future uses of the data are also an important consideration. Consider how your synthesis team will analyze the data, as well as how the world outside that team will use and interact with the data once it is published. Again, there is no one answer, but below are a few examples of harmonized destination formats to consider.\n\nLong (Tidy)Wide (Untidy)Relational (Database-Style)Cloud-nativeOther…\n\n\nHere our grassland biomass data is in long format, often referred to as “tidy” data. Data in this format is generally easy to understand and use. There are three rules for tidy data:\n\nEach column is one variable.\nEach row is one observation.\nEach cell contains a single value.\n\nAdvantages: clear meaning of rows and columns, ease in filtering/cleaning/appending\nDisadvantages: observation information is repeated, file size larger\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, in long format.\n\n\n\n\nIn this dataset, our grassland data has been restructured into wide format, often referred to (sometimes unfairly) as “messy” or “untidy” data. Note that the biomass variable has been split into two columns, one for control plots and one for fertilized plots.\nAdvantages: compact file size, easier for some statistical analyses (ANOVA, for example)\nDisadvantages: may be more difficult to clean/filter/append, multiple observations per row\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, restructured into wide format with biomass values in control and fertilized columns.\n\n\n\n\nBelow is a schematic of the related tables that comprise the ecocomDP7 harmonized data format for biodiversity data. Eight tables are defined, along with a set of relationships between tables (keys), and constraints on the allowable values in each table. Relational formats like this are “normalized” to reduce data redundancy, and increase data integrity.\nAdvantages: reduced redundancy, greater integrity, community standard\nDisadvantages: significant metadata needed to describe and use, more complex to publish\nPossible file formats: Database stores, can be represented in delimited text (CSV)\n\n\n\nThe ecocomDP schema. Each table has a name (top cell) and a list of columns. Shaded column names are primary keys, hashed columns have constraints, and arrows represent relations between keys/constraints in different tables.\n\n\n\n\nThere are many possibilities to make large synthesis datasets available and useful in the cloud. These require specialized knowledge and tooling, and reliable access to cloud platforms.\nAdvantages: easier access to big (high volume) data, can integrate with web apps\nDisadvantages: less familiar/accessible to many scientists, few best practices to follow, costs can be higher\nPossible file formats: Parquet files, object storage, distributed/cloud databases\n\n\n\nA few of the cloud-native technologies that might be useful for synthesis research products.\n\n\n\n\nThere are many, many other possible harmonized data formats. Here are a few possible examples:\n\nDarwinCore archives for biodiversity data\nOrganismal trait databases\nArchives of cropped, labeled images for training machine or deep learning models\nLibraries of standardized raster imagery in Google Earth Engine\n\n\n\n\n\n\nAdditional Resources about Data Preparation\nData cleaning and filtering resources\n\nData cleaning is complicated and varied, and entire books have been written on the subject.89 For some general considerations on cleaning data, see EDI’s “Cleaning Data and Quality Control” resource\nOpenRefine is an open-source, cross-platform tool for iterative, scripted data cleaning.\nIn the R language, the tidyverse libraries (particularly tidyr and dplyr) are often used for data cleaning, as are additional libraries like janitor.\nIn Python, pandas and numpy libraries provide useful data cleaning features. There are also some stand-alone cleaning tools like pyjanitor (started as a re-implementation of the R version) and cleanlab (geared towards machine learning applications).\nBoth the R and Python data science ecosystems have excellent documentation resources that thoroughly cover data cleaning. For R, consider starting with Hadley Wickham’s R for Data Science book chapter on data tidying,10 and for python check Wes McKinney’s Python for Data Analysis book chapter on data cleaning and preparation.11\n\nData harmonization resources\n\nFor R and Python users, there are, again, excellent documentation resources that thoroughly cover data harmonization techniques like data filtering, reformatting, joins, and standardization. In Hadley Wickham’s R for Data Science book, the chapters on data transforms and data tidying are a good place to start. In Wes McKinney’s Python for Data Analysis book, the chapter on data wrangling is helpful."
  },
  {
    "objectID": "module2.html#data-analysis",
    "href": "module2.html#data-analysis",
    "title": "Operational Synthesis",
    "section": "Data Analysis",
    "text": "Data Analysis\nOnce the team has found sufficient source data, then cleaned, filtered, and harmonized countless datasets, and documented and described everything with quality metadata, it is finally time to analyze the data! Great! Load up R or Python and get started, and then tell us how it goes. We simply don’t have enough time to cover all the ins and outs of data analysis in a three-hour course. However, we have put a few helpful resources below to get you started, and many of the best practices we have talked about, or will talk about, apply:\n\nDocument your analysis steps and comment your code, and generally try to make everything reproducible.\nUse version control as you analyze data.\nGive everyone a chance! Analyzing data is challenging, exciting, and a great learning opportunity. Having more eyes on the analysis process also helps catch interesting results or subtle errors.\n\n\nAdditional Resources About Data Analysis\n\nHarrer, M. et al. Doing Meta-Analysis with R: A Hands-On Guide. 2023. GitHub\nOnce again, for R and Python users, the same two books mentioned above provide excellent beginning guidance on data analysis techniques (exploratory analysis, summary stats, visualization, model fitting, etc). In Wickham’s R for Data Science book, the chapter on exploratory data analysis will help. In McKinney’s Python for Data Analysis book, try the chapters on plotting and visualization and the introduction to modeling."
  },
  {
    "objectID": "module2.html#synthesis-group-case-studies",
    "href": "module2.html#synthesis-group-case-studies",
    "title": "Operational Synthesis",
    "section": "Synthesis Group Case Studies",
    "text": "Synthesis Group Case Studies\nEstimated time: 10 min\nTo make some of these concepts more tangible, let’s consider some case studies. The following tabs contain GitHub repositories for real teams that have engaged in synthesis research and chosen to preserve and maintain their scripts in GitHub. Each has different strengths and you may find that facets of each feel most appropriate for your group to adopt. There is no single “right” way of tackling this but hopefully parts of these exemplars inspire you.\n\nExample 1Example 2Example 3\n\n\nLTER SPARC Group: Soil Phosphorus Control of Carbon and Nitrogen\nStored their code here:  lter / lter-sparc-soil-p\nHighlights\n\nStraightforward & transparent numbering of workflow scripts\n\nFile names also reasonably informative even without numbering\n\nSimple README in each folder written in human-readable language\nCustom .gitignore safety net\n\nControls which files are “ignored” by Git (prevents accidentally sharing data/private information)\n\n\n\n\nLTER Full Synthesis Working Group: The Flux Gradient Project\nStored their code here:  lter / lterwg-flux-gradient\nHighlights\n\nExtremely consistent file naming conventions\nStrong use of sub-folders for within-project organization\nTop-level README includes robust description of naming convention, folder structure, and order of scripts in workflow\nActive contribution to code base by nearly all group members\n\nFacilitated by strong internal documentation and consenus-building prior to choosing this structure\n\n\n\n\nLTER Full Synthesis Working Group: From Poles to Tropics: A Multi-Biome Synthesis Investigating the Controls on River Si Exports\nStored their code here:  lter / lterwg-silica-spatial\nHighlights\n\nFiles performing similar functions share a prefix in their file name\nUse of GitHub “Release” feature to get a persistent DOI for their codebase\nSeparate repositories for each manuscript\nNice use of README as pseudo-bookmarks for later reference to other repositories\n\n\n\n\nFor more information about LTER synthesis working groups and how you can get involved in one, click here."
  },
  {
    "objectID": "module2.html#additional-resources",
    "href": "module2.html#additional-resources",
    "title": "Operational Synthesis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nCourses, Workshops, and Tutorials\n\nSynthesis Skills for Early Career Researchers (SSECR) course. 2024. LTER Network Office\nReproducible Approaches to Arctic Research Using R workshop. 2024. Arctic Data Center & NCEAS Learning Hub\nCollaborative Coding with GitHub workshop. 2024. NCEAS Scientific Computing team\nCoding in the Tidyverse workshop. 2023. NCEAS Scientific Computing team\nShiny Apps for Sharing Science workshop. 2022. Lyon, N.J. et al.\nTen Commandments for Good Data Management. 2016. McGill, B.\n\n\n\nLiterature\n\nTodd-Brown, K.E.O., et al. Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. 2022. Biogeosciences\nBorer, E.T. et al. Some Simple Guidelines for Effective Data Management. 2009. Ecological Society of America Bulletin"
  },
  {
    "objectID": "module2.html#footnotes",
    "href": "module2.html#footnotes",
    "title": "Operational Synthesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nMayernik, M.S. and Acker, A. (2018), Tracing the traces: The critical role of metadata within networked communications. Journal of the Association for Information Science and Technology, 69: 177-180. https://doi.org/10.1002/asi.23927↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nNew York Times, 2014↩︎\nAnaconda State of Data Science Report, 2022↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nO’Brien, Margaret, et al. “ecocomDP: a flexible data design pattern for ecological community survey data.” Ecological Informatics 64 (2021): 101374. https://doi.org/10.1016/j.ecoinf.2021.101374↩︎\nOsborne, Jason W. Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data. Sage publications, 2012.↩︎\nVan der Loo, Mark, and Edwin De Jonge. Statistical data cleaning with applications in R. John Wiley & Sons, 2018. https://doi.org/10.1002/9781118897126↩︎\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for data science. ” O’Reilly Media, Inc.”, 2023. https://r4ds.hadley.nz/↩︎\nMcKinney, Wes. Python for data analysis. ” O’Reilly Media, Inc.”, 2022. https://wesmckinney.com/book↩︎"
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nRecognize the importance of establishing group norms and expectations\nEvaluate strategies for group organization and project management"
  },
  {
    "objectID": "module1.html#learning-objectives",
    "href": "module1.html#learning-objectives",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nRecognize the importance of establishing group norms and expectations\nEvaluate strategies for group organization and project management"
  },
  {
    "objectID": "module1.html#workshop-slides---under-construction",
    "href": "module1.html#workshop-slides---under-construction",
    "title": "Starting with Team Science",
    "section": "Workshop Slides - Under Construction",
    "text": "Workshop Slides - Under Construction\nGoogle Slides link"
  },
  {
    "objectID": "module1.html#define-synthesis",
    "href": "module1.html#define-synthesis",
    "title": "Starting with Team Science",
    "section": "Define Synthesis",
    "text": "Define Synthesis\nFor the purposes of today’s discussion, we define synthesis as:\n\nBringing together the results of multiple studies to test novel hypotheses–usually with a team of people."
  },
  {
    "objectID": "module1.html#why-synthesis",
    "href": "module1.html#why-synthesis",
    "title": "Starting with Team Science",
    "section": "Why Synthesis?",
    "text": "Why Synthesis?\n\nThis process–of bringing diverse expertise together to combine existing primary data–definitely has its challenges. Ideniftying and engaging the necessary combination of skills and experience is challenging. Scheduling meeting times can be difficult. Commitment to the process can be uneven. Nonetheless, virtually all who have experienced it find it to be a deeply rewarding experience. Why?\n\nThe products emerging from synthesis typically have higher impact (as defined by both academic and applied measures)\nThe process allows researchers to access and incorporate skills that they don’t (yet) have themselves.\nWorking groups help early career researchers build their science networks.\nKeep experienced researchers fresh and engaging with new ideas\nBuilds on existing investments of the science community by re-using data\nOffers a way to involve individuals who can’t or don’t want to do fieldwork in original research and expands opportunity to less research-intensive institutions.\n\n\nAdditional Resources\n\nAdam, 2023. ‘Disruptive’ science: in-person teams make more breakthroughs than remote groups\nHackett, 2020. Collaboration and Sustainability: Making Science Useful, Making Useful Science\nHampton and Parker, 2011. Collaboration and Productivity in Scientific Synthesis\nHackett et al., 2021. Do synthesis centers synthesize? A semantic analysis of topical diversity in research\nWyborn et al., 2018. Understanding the Impacts of Research Synthesis"
  },
  {
    "objectID": "module1.html#process-overview",
    "href": "module1.html#process-overview",
    "title": "Starting with Team Science",
    "section": "Process Overview",
    "text": "Process Overview\nTypically, a group of researchers–or researchers and managers or community members–will plan a series of meetings over 2-3 years. The mix of in-person v. virtual meetings and work will vary across different groups and different funders, but the general pattern is similar.\n\n\n\nEarly meetings focus on narrowing the questions and deciding what data is needed and what analyses will be most useful. A period of data gathering and assembly comes next. The assembly of data almost always prompts a revision of the initial questions, as data rarely comes in exactly the form that researchers expected. This can be both the most frustrating and the most interesting part of the process as new hypotheses and models are floated and discussed. It is especially important to have the full participation of researchers familiar with different fields and ecosystems in this process.\nWith tractable questions refined, the group will move into analysis mode. Often, a few individuals will do most of the data wrangling and coding, but will need continuous input on analytical decisions. In our experience, GitHub issues is one very good tool for facilitating and recording these decisions. But the “best” tool will be the one that most members of the team are most comfortable with.\nLater meetings will focus on developing manuscripts and/or application-related products such as white papers and decision support tools."
  },
  {
    "objectID": "module1.html#how-to-get-involved",
    "href": "module1.html#how-to-get-involved",
    "title": "Starting with Team Science",
    "section": "How to get Involved",
    "text": "How to get Involved\nOften, early career researchers will be excited about the idea of synthesis but be unsure how to connect with existing or nascent synthesis efforts. Here are a few ideas for how to make yourself available and valuable to synthesis groups.:\n\nMake it known you want to be involved in synthesis\n\nLet your advisor know\nShare your enthusiasm\n\nSkill building:\n\nSynthesis Skills for Early Career Researchers: SSECR\nData Carpentries\nESIIL: innovation summit, hackathons\nEnvironmental Data Science Summit\n\nBuild your community\n\nAsk questions at meetings\nInitiate conversations\n\nStart your own!"
  },
  {
    "objectID": "module1.html#identifying-a-synthesis-ready-question",
    "href": "module1.html#identifying-a-synthesis-ready-question",
    "title": "Starting with Team Science",
    "section": "Identifying a Synthesis-Ready Question",
    "text": "Identifying a Synthesis-Ready Question\nLots of questions are interesting, but not terribly well-suited for a synthesis approach. We’ve learned through experience that there are a few qualities that make some questions a better fit for a) combining data; and b) work by a group. The main qualities that we seek in synthesis projects include:\n\nNovel and interesting enough to keep you engaged for 2-3 years.\nData already exist and you know (at least generally) where to find it\nQuestions cover a large geographic area or data that aren’t normally collected or analyzed together\nClearly framed, but flexible enough to allow adaptation through the process\nResponsive to the funding call\nOutputs could include several kinds of products. As the project progresses, gard students will become postdocs, postdocs will get faculty positions. For the project to remain satisfying to all participants, people will need to be able to take a leadership role on different kinds of products.\n\nPapers (including data papers, perspectives, gap analyses, as well as primary analyses)\nSymposia\nDatasets\nAnalytical Packages"
  },
  {
    "objectID": "module1.html#sources-of-support-for-synthesis",
    "href": "module1.html#sources-of-support-for-synthesis",
    "title": "Starting with Team Science",
    "section": "Sources of Support for Synthesis",
    "text": "Sources of Support for Synthesis\nWhile it is certainly possible to conduct synthesis with no external support, a bit of funding will allow your group to travel to meet up in person and can, in some cases, provide salary support for postdocs, grad students or or assistance with analysis.\n\nDiscussion QuestionSources of Funding and Support\n\n\n\nWhat funding sources support synthesis work?\n\n\n\nSynthesis Centers\n\nEnvironmental Science Innovation and Inclusion Laboratory (ESIIL)\nNational Center for Ecological Analysis and Synthesis (NCEAS)\n\nMorpho Program\n\nUSGS Powell Center\nS-div\nCanadian Institute of Ecology and Evolution (CIEE)\n\nSocieties\n\nNew Phytologist Workshops\nGordon Research Conferences\nChapman Conferences\nBritish Ecological Society\n\none-third of participants from developing world\n\n\nNSF Programs\n\nULTRA-data Dear Colleague letter\nNSF Core Programs, e.g. Division of Environmental Biology: “synthesis activities”, “synthesis projects”\nNSF workshops"
  },
  {
    "objectID": "module1.html#building-a-team",
    "href": "module1.html#building-a-team",
    "title": "Starting with Team Science",
    "section": "Building a Team",
    "text": "Building a Team\n\nThe Leadership Team\nThe composition of the leadership team will affect the success of the project and who you will be able to recruit to the larger group. Look for: - Different (and complementary) areas of expertise - Complementary professional networks - Facilitation skills - Emotional Intelligence\n\n\nThe Broader Team\nIn our experience at NCEAS and the LTER Network Office, we’ve found teams of up to 10 to 15 people to be optimal for synthesis work. As individuals, we all have strengths and weaknesses. The beauty of working in teams is that you can invite people who offset your own weaknesses and who bring strengths you don’t have. Often, you’ll have a few core team members who have generated a synthesis idea, but then you’ll want to take a clear-eyed look at what additional skills and qualities to invite. When you do so, be sure to consider:\n\nSkills, Aptitudes, and Communication Styles\n\nLook for a mix of empiricists, theorists, and modellers\nBig-picture thinkers, organizers, task-oriented do-ers\nDeep thinkers and risk-takers\nAt least some skilled coders\n\nCareer stage\n\nSenior investigators connect the team to existing literature and fields of study, connect to a broad network of experienced researchers, and have good knowledge of resources, but are often have a very limited amount of time to devorte to discussion and analyses\nJunior team members often bring a fresh perspective, familiarity with newer literature, strong coding skills, and time to devote to the project\n\nEmotional intelligence\n\nResearch shows (add ref) that the bump in creativity seen in mixed-gender teams is typically due to an increase in emotional intelligence and attention to team dynamics. Include at least a few people with a process orientation and strong people skills.\n\nPower dynamics\n\nYou won’t be able to anticipate all of the issues related to power dynamics that can arise, but keep them front of mind as you assemble a team.\n\nRemember that participation in synthesis represents a significant career opportunity\n\nBe mindful that such career-building opportunities have not been fairly distributed\nBe intentional seeking out people who may not be part of your typical circles (including gender, ethnicity, career stage, family status, (dis)abilities, etc.)"
  },
  {
    "objectID": "module1.html#setting-expectations",
    "href": "module1.html#setting-expectations",
    "title": "Starting with Team Science",
    "section": "Setting Expectations",
    "text": "Setting Expectations\nGood teams are both chosen and made. Diversity on teams uncovers novel approaches, perspectives, and insights and it can slow the pace and cause misunderstandings that highlight unexamined assumptions. As a synthesis team leader (and even as a participant), there are many things you can do to create opportunities for everyone to contribute their best thinking, learn from one another, and feel heard and respected.\nAs the group gets started\n\nCreate a shared vision for your group\n\nMake sure everyone starts on the same footing with a brief overview of the context for the group and the questions you’re starting with\n\nCo-develop group norms. It builds ownership of shared norms.\nConduct a “pre-mortem” to talk about worries and visions for a healthy group dynamic\nRecognize who you are as a group, culturally, and any power dynamics that might entail\n\nSometimes, simply articulating the potential for oppressive power dynamics can give group members the confidence to assert themselves\n\n\nThroughout the process\n\nGet to know each other\n\nUse creative icebreakers to break a pattern of silence\nInvest in “social time”\n\nValue and accommodate various styles of contribution\n\nFast and slow thinkers,\nVisual, auditory and kinesthetic learners,\nSynchronous and asynchronous contributions\n\nPractice “cultural norming” by educating participants on systemic/structural oppression and racism and ways to work against our own implicit biases\n\nAt intervals\n\nWhen you find yourself questioning whether a practice or activity is still valuable, ask each member for a quick read on whether the group should “Start, Stop, or Continue” the activity\n\nIn any team project, people have different reasons for wanting to participate. For you, getting a high profile paper may be the most important thing. For others, it may be expanding their network or a chance to practice new skills. Being transparent about those motivations makes it easier to resolve tensions when they arise.\n\n\n\n\nImportance of various benefits to working group participants\n\n\n\n\n\n\n\nTime Commitments\n\n\n\nWe all misjudge our availability once in a while, but consistently failing to deliver on commitments disrupts others’ work plans and is a major source of group dissatisfaction. Get buy-in for commitments and plan for both reminders and accountability."
  },
  {
    "objectID": "module1.html#making-a-communication-and-work-plan",
    "href": "module1.html#making-a-communication-and-work-plan",
    "title": "Starting with Team Science",
    "section": "Making a Communication and Work Plan",
    "text": "Making a Communication and Work Plan"
  },
  {
    "objectID": "module1.html#intro-to-establishing-a-project-management-plan",
    "href": "module1.html#intro-to-establishing-a-project-management-plan",
    "title": "Starting with Team Science",
    "section": "Intro to Establishing a Project Management Plan",
    "text": "Intro to Establishing a Project Management Plan"
  },
  {
    "objectID": "module1.html#data-sources",
    "href": "module1.html#data-sources",
    "title": "Starting with Team Science",
    "section": "Data Sources",
    "text": "Data Sources\nSome sources of data–such as modern remote sensing products, NEON data, and census data–have very clear, explicit ways to access and download them or work with them in the cloud. But the most interesting synthesis questions often involve combining such “big” data with other data sources that may have been collected manually, by a variety of methods and different technicians, over decades.\n\nDiscussion QuestionA Few Ideas\n\n\n\nWhat kinds of data sources might you consider including in a synthesis project, in addition to your own or others’ field data?\n\n\n\n\nDataONE\nEnvironmental Data Initiative (EDI)\nGenBank (NCBI)\nNational Ecological Observatory Network (NEON)\nUS Geological Survey Data\nGlobal Biodiversity Information Facility (GBIF)\nNASA Remote Sensing Data\nUS Park Service\nFluxNet\nPhenocam network\niNaturalist\neBird\nCensus data\nData extracted from papers\nScraping social media\nText analysis\n…."
  },
  {
    "objectID": "module1.html#data-use-principles",
    "href": "module1.html#data-use-principles",
    "title": "Starting with Team Science",
    "section": "Data Use Principles",
    "text": "Data Use Principles\nThere are a few ethical and practical guidelines that will save you a lot of trouble if you can adhere to them from the start of a project.\n\nKeep track of your data sources (sources, permissions, notes, related metadata, what’s included)\nData sources should always be cited\nCommunicate with data creators whenever possible\n\nThis doesn’t need to be onerous and it can uncover issues and opportunities associated with data sources.\n\n\n\n\n\n\n\n\nSample data author outreach email for public dataset:\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include your dataset titled “xxx” (doi: xxx). We have downloaded the data from yyy repository, but wanted to let you know we are using it and to inquire whether there is any additional context we should be aware of or related datasets we should be sure to include. A short description of the synthesis project follows. Please let me know by xxx date if you have any questions or concerns with our use of this data.\nThank you,\n\n\n\n\n\n\n\n\n\n\nSample data author outreach email for unpublished dataset:\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include the dataset behind your paper titled {xxx} (doi:{yyy}), which seems highly relevant. Would it be possible to obtain the data? Ideally, we would access it through through a public repository, such as the Environmental Data Initiative, which offers assistance in curation and submission of datasets. Either way, we would credit you as the data originator and want you know we are using it. If there is any additional context we should be aware of or related datasets we should be sure to include, please let us know. A short description of the synthesis project follows.\nThank you,\n\n\n\n\n\nAuthorship DiscussionA Few Considerations\n\n\n\nShould all data contributors be offered authorship? How would you handle a data creator who demanded authorship in order to use their data?\n\n\n\nThere are no pat answers for this situation, but having agreed-on authorship guidelines is really valuable when it comes up. We’ll cover that in more detail soon, but for now, there are a few questions to ask yourself.\n\nAre they really committed enough to join the working group and contribute to the papers? If so, it may be a good investment.\nHow much work will they need to put in to make the data ready?\nHow critical is this particular data source for your analysis?\nYou will need to make your derived dataset public. Are they placing conditions on the use of their data that make that impossible?"
  },
  {
    "objectID": "module1.html#keeping-track-of-data",
    "href": "module1.html#keeping-track-of-data",
    "title": "Starting with Team Science",
    "section": "Keeping Track of Data",
    "text": "Keeping Track of Data\nWe’ve pulled together a few of the forms that we have use to keep track of the data that synthesis groups plan to use…\nSample spreadsheet for initial data surveying\n\nOnce you get into the details of the process, you’ll want to track some more specific information\n\nURL to the data (and metadata) source\nSampling location and site (including both coordinates and associated organizations)\nShort Data Description\nCoverage Dates/Frequency\nFilename (as stored on Google Drive or shared file repository)\nURL to Files (cloud drive, website, server; e.g. google drive link)\nDate the data was last accessed / downloaded\nData Creator/Owner’s Name\nData Creator/Owner’s Email/contact\nWorking group participant who got the data (Name)\nUsed in your analysis? (Y/N)\nAny additional notes or decisions about how the data is or will be harmonized and analyzed"
  }
]