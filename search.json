[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "The course is currently being organized by Greg Maurer (gmaurer@nmsu.edu), with generous assistance from Marty Downs and Nick Lyon. See the Contributors section for more information."
  },
  {
    "objectID": "people.html#contributors",
    "href": "people.html#contributors",
    "title": "People",
    "section": "Contributors",
    "text": "Contributors\nWe have a long list of synthesis scientists actively involved in the course working group who are contributing expertise and instructional content. Names and web profiles are listed below in alphabetical order by last name.\nIcon legend:  = website |  = publications |  = GitHub |  = ORCID\n\nKathryn Barry – Utrecht University | \nJoanna Carey – Babson College |  \nAngel Chen (she/her) – LTER Network Office |   \nLaura Dee – University of Colorado Boulder |   \nMarty Downs – LTER Network Office |  \nStevan Earl – Arizona State University |   \nSarah Elmendorf – | University of Colorado Boulder   \nJalene LaMontagne – University of Missouri-St. Louis / Missouri Botanical Garden |   \nNick J Lyon (they/them) – LTER Network Office |   \nGregory Maurer (he/him) – New Mexico State University |   \nColin Smith (he/him) – Environmental Data Initiative | \nEric Sokol (he/him) – NEON |   \nAlexandra (Sasha) Wright – California State University, Los Angeles"
  },
  {
    "objectID": "people.html#course-instructors",
    "href": "people.html#course-instructors",
    "title": "People",
    "section": "Course instructors",
    "text": "Course instructors\nInstructors and assistants have changed each time the course has run. So far, instructors have come from the list of contributors above.\n\nESA 2025ESA 2024\n\n\nESA 2025 Summer Meeting in Baltimore, MD.\nInstructors:\n\nMarty Downs\nJalene Lamontagne\nNick Lyon\nGreg Maurer\nEric Sokol\n\n\n\nESA 2024 Summer Meeting in Long Beach, CA.\nInstructors:\n\nAngel Chen\nMarty Downs\nStevan Earl\nNick Lyon\nGreg Maurer\nEric Sokol"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "People",
    "section": "Other Contributors",
    "text": "Other Contributors\nIn addition to those above, several people contributed to early iterations on the concept and content of the course.\n\nForest Isbell\nKim Komatsu"
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nSummarize the advantages of creating a defined contribution workflow\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nUnderstand best practices for preparing and analyzing data to be used in synthesis projects"
  },
  {
    "objectID": "module2.html#learning-objectives",
    "href": "module2.html#learning-objectives",
    "title": "Operational Synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nSummarize the advantages of creating a defined contribution workflow\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nUnderstand best practices for preparing and analyzing data to be used in synthesis projects"
  },
  {
    "objectID": "module2.html#introduction",
    "href": "module2.html#introduction",
    "title": "Operational Synthesis",
    "section": "Introduction",
    "text": "Introduction\nScientific research can be defined as “creative and systematic work undertaken in order to increase the stock of knowledge” (2015 Frascati Manual). To this basic definition of research, our definition of synthesis research adds collaborative work, and the integration and analysis of a wide range of data sources.\nIn Module 1 we discussed many of the collaborative considerations for synthesis research, including creating a diverse and inclusive team, asking synthesis-ready scientific questions (often broad in scope or spatial scale), and finding suitable information (or data) from a wide variety of sources to answer those questions.\nOnce the synthesis team moves into the operational phase of research, which includes the integration and analysis of data, there are some key activities that must happen:\n\nMake a plan for technical challenges\nHarmonize and tidy data in a reproducible way\nAnalyze data to answer your questions\n\nOnce you have analyzed your data, you can interpret the results and create products as you likely would in a non-synthesis project–though the scale and impact of your products are likely to be larger for a synthesis project than for a typical scientific effort. More on these later steps of synthesis work in Module 3.\nWe’ve already seen that creating a collaborative, inclusive team can set the stage for successful synthesis research. Each of the operational activities above will also benefit from this mindset, and in this module we highlight some of the most important considerations and practices for a team science approach to the nuts-and-bolts of synthesis research."
  },
  {
    "objectID": "module2.html#make-a-plan",
    "href": "module2.html#make-a-plan",
    "title": "Operational Synthesis",
    "section": "1. Make a Plan",
    "text": "1. Make a Plan\nFor non-synthesis projects, it can feel intuitive to skip this step. Especially if you are working alone or with a small group of close collaborators the formal process of making and documenting your strategy can feel like a waste of time relative to ‘actually’ doing the work. However, synthesis work uses a lot of data, requires a high degree of technical collaboration, and involves a series of big judgement calls that will need to be revisited. Because of these factors, taking the time to create an actual, specific, written plan (and periodically updating that plan as priorities and questions evolve) is vital to the success of synthesis projects!\nSee the sub-sections below for some especially critical elements to consider as you draft your plan. Note this is not exhaustive and there will definitely be other considerations that will be reasonable for your team to add to the plan based on your project’s goals and team composition.\n\nHow Will You Get (and Stay) Organized?\nTwo critical elements of oragnization are (1) the folder structure on your computer of project files and (2) how you will informatively name files so their purpose is immediately apparent. Also, it is easier to start organized than it is to get organized, so implementing a system at the start of your project and sticking to it will be much easier than trying to re-organize months or years worth of work after the fact.\nWhile there is no “one size fits all” solution to project organization, we recommend something like the following:\n\n\nKey properties of this structure:\n\nEverything nested within a project-wide folder\nLimited use of sub-folders\nDedicated “README” files containing high-level information about each folder\nConsistent folder/file naming conventions\n\nGood names should be be both human and machine-readable (and sorted in the same way by machines and people)\nAvoid spaces and special characters\nConsistent use of delimeters (e.g., “-”, “_“, etc.)\n\nShared file prefix (a.k.a. “slug”) connecting code files with files they create\n\nAllows for easy tracing of errors because the file with issues has an explicit tie to the script that likely introduced that error\n\nIncludes a “data log” (a.k.a. “data inventory”) for documenting data sources and critical information about each\n\nE.g., dataset identifiers/DOIs, search terms used in data repository, provenance, spatiotemporal extent and granularity, etc.)\n\n\n\n\n\n\nSuggested Project Structure:\n synthesis_project\n |–  README.md\n |–  code\n |       |–  README.md\n |       |–  01_harmonize.R\n |        L   02_quality-control.R\n |–  data\n |       |–  README.md\n |       |–  data-log.csv\n |       |–  raw\n |        L   tidy\n |             |–  01_data-harmonized.csv\n |              L   02_data-wrangled.csv\n |–  notes\n |–  presentations\n  L   publications\n        |–  README.md\n        |–  community-composition\n         L   biogeochemistry\n\n\n\n\n\n\n\n\n\nActivity: Organization\n\n\n\nOn your own or in a small group, open your computer and find the files for one of your past projects. With that project in mind, ask yourself the following:\n\nHow hard is it for you to reorient yourself to this project based on your organization system?\n\nHow hard do you think it would be to onboard a new collaborator on this project?\n\nHow many of the properties of the recommended structure above does your organization system follow?\nAre there actionable steps you could take to feel more confident in your organization system? If so, what are they?\n\nOnce you have considered these questions, take a few minutes and try to reorganize your chosen project. If that feels risky to you, write some notes on scratch paper of how you would (or could) reorganize this project to be more functional\n\n\n\n\nHow Will You Collaborate?\nOnce you’ve decided on your organization method, you’ll need to decide as a team how you will work together. It is critical that your whole team agrees to whatever method you come up with because it will result in a lot of unnecessary work if a subset of people do not follow the plan–and thus introduce disorganization and inconsistency that someone will have to spend time fixing later.\nA good rule of thumb is that you should plan for “future you.” What collaboration methods will you in 6+ months thank ‘past you’ for implementing? It may also be helpful to consider the negative side of that question: what shortcuts could you take now that ‘future you’ will be unhappy with?\nA huge part of deciding how you will collaborate is deciding how you will communicate as a team. When you work asynchronously, how will you tell others that you are working on a particular code or document file? This does not have to be high tech–an email or Slack message can suffice–but if you don’t communicate about the minutiae, you risk duplicating effort or putting in conflicting work.\n\n\nWhere Will Code Live?\nUnlike the other planning elements, we (the instructors) feel there is a single correct answer to this: your code should live in a version control system. Broadly, “version control” systems track iterative changes to files. In addition to the specific, line-by-line changes to project files, contributions of each team member are tracked, and there are straightforward systems for ‘rolling back’ files to an earlier state if needed.\n\nAs the comic to the right shows–and as all scientists know from experience–you will have several (likely many) drafts of a given product before the finalized version. With a version control system, all the revisions in each draft are saved without needing to manually ‘re-save’ the file with a date stamp or version number in the filename. Version control systems provide a framework for preserving these changes without cluttering your computer with all of the files that precede the final version.\nWhile there are several version control options, we recommend Git as the version control software and GitHub as the online platform for storing Git repositories in a shareable way. Note that there are viable alternatives to GitHub (e.g., GitLab, GitKraken, etc.) but (A) all of these rely on Git ‘under the hood’ and (B) in order to give an effective tutorial of version control in this course, we need to narrow our focus to just one of these website options. From here onwards, we’ll use focus on GitHub but it may be worthwhile for your team to investigate some of the GitHub alternatives as they will all offer similar functionality.\n\nGitHub Tutorial\nGiven the time restrictions for this short course, we’ll only cover how you can engage with GitHub directly through its website today. However, this focus is actually a benefit to synthesis teams because learning how to engage with a Git repository via GitHub can enable collaborators who are less comfortable writing code to still fully contribute and stay abreast of the team’s progress.\nThere are a lot of Git/GitHub tutorials that already exist so, rather than add yet another variant to that list, we’ll instead work through part of the workshop created by the Scientific Computing team of the Long Term Ecological Research (LTER) Network Office. Note too that those materials include a detailed, step-by-step guide for working with Git/GitHub through RStudio, as well as some discussion of the project management tools that GitHub supports. As noted earlier, we won’t cover those parts of that workshop today, but they might be useful for you to revisit later.\nThe workshop materials we will be working through live here but for convenience we have also embedded the workshop directly into this short course’s website (see below).\n\n\n\n\n\n\n\nActivity: Practice with GitHub\n\n\n\nLet’s use some of the skills that we just discussed during the GitHub tutorial!\nWhen you make a repository under your username with the same name as your user (e.g., njlyon0 / njlyon0) this is a “special repository”. Any content that you put in the README file of that repository will show up at the top of your GitHub profile (e.g., here, or here). Making this landing page for your GitHub page can be a nice way of demonstrating your GitHub savvy and–for those without a professional website–can be a nice stand-in until you feel that a full website is warranted.\nFor our purposes today, creating this repository and filling it out through GitHub is a nice chance for you to build muscle memory with creating and editing files directly through GitHub!\nTake a few minutes to create this special repository and add a few sentences about yourself and your personal/professional interests."
  },
  {
    "objectID": "module2.html#prepare-data",
    "href": "module2.html#prepare-data",
    "title": "Operational Synthesis",
    "section": "2. Prepare Data",
    "text": "2. Prepare Data\nSynthesis efforts are intensely data-heavy by their nature; as such, you’ll need to spend more time and effort in the data preparation phase than you would in a non-synthesis project. This means that doing reproducible work pays huge dividends! Making one’s work “reproducible”–particularly in code contexts–has become increasingly popular but is not always clearly defined. For the purposes of this short course, we believe that reproducible work has the following properties:\n\nContains sufficient documentation for those outside of the project team to navigate and understand the project’s contents\nAllows anyone to recreate the entire workflow from start to finish\nLeads to modular, extensible research projects; adding data from a new site, or a new analysis, should be relatively easy in a reproducible workflow\nContains detailed metadata for all data products and prerequisite software\n\n\n\n\n\n\n\nMore About Metadata\n\n\n\n\n\nMetadata is “data about the data,” or information that describes who collected the data, what was observed or measured, when the data were collected, where the data were collected, how the observations or measurements were made, and why they were collected. Metadata provide important contextual information about the origin of the data and how they can be analyzed or used. They are most useful when attached or linked to the data being described, and data and related metadata together are commonly referred to as a dataset.\nMetadata for ecological research data are well described in Michener et al (1997),1 but there are many other kinds of metadata with different purposes.2 If you are publishing a research dataset and have questions about metadata, ask a data manager for your project, or staff at the repository you are working with, for help. Either can typically provide guidance on creating metadata that will describe your data and be useful to the community (here is one example). We’ll return to the subject of metadata in Module 3.\n\n\n\n\nReproducible Coding\nWhile the reproducibility guidelines identified above apply to many different facets of a project, there are a few more when considering reproducible code specifically. For example:\n\nAll interactions with data should be done with code\nA version control system should be used to track changes to code\nAll code should include non-coding “comments” to provide explanation/context\nAny necessary software libraries should be loaded explicitly at the start of each script\nRelative file paths that are agnostic to operating system\n\nE.g., file.path(\"data\", \"bees.csv\") instead of \"~/Users/me/Documents/synthesis_project/data/bees.csv\"\n\nFunctions should be “namespaced” (if not already required by your coding language)\n\nE.g., dplyr::mutate() instead of mutate()\n\n\nThe above list is non-exhaustive and there are other conditions you may want to consider such as using custom functions for repeated operations or adding sequential numbers to script names that must be run in a particular order but generally following the above list will keep your code on the “more reproducible” side of things.\n\n\nReproducibility in Synthesis\nFinally, there are a few additional considerations for reproducible synthesis work in particular. Judgement calls need to be made and agreed to as a team. However, group members should “defer to the doers.” If you have a very strong opinion that differs from the feeling of most of the rest of the group and will result in a lot of extra work, you should be ready to volunteer to do that work or allow those who will be responsible for the work to have a larger role in deciding the shape of that effort.\nReproducible synthesis work also requires dramatically more communication–especially around contribution guidelines and intellectual credit. It’s best to keep track of who contributed what, so that everyone gets credit. This can be challenging in practice and makes sense to start recording early and in a transparent way.\n\n\n\n\n\n\nActivity: Reproducibility\n\n\n\nIn small groups, discuss the following questions:\n\nWhat elements of reproducibility that we identify have you used/are interested in using?\nWhich feel unreasonable or confusing?\nWhat activities do you do in your own work to ensure reproducibility that our list is missing?\n\nIf you have a script you’ve worked on recently, open it and read through it.\n\nHow reproducible do you think that code is?\nHow easy would it be for someone else to understand what the code does?\n\nHow easy would it be for them to actually run the code?\n\n\n\n\n\n\nData Preparation Overview\nThe scientific questions being asked in synthesis projects are usually broad in scope, and it is therefore common to bring together many datasets from different sources for analysis. The datasets selected for analysis (source data) may have been collected by different people, in different places, using different methods, as part of different projects, or all of the above. Typically you will need to harmonize these disparate datasets by standardizing data structure, units of measurement, and file format. Once this is done, you’ll move on to data cleaning where you can check for malformed values, unreasonable rows, and filter out unwanted observations.\nThese processes can be easy or difficult depending on the quality of the source data, the differences between source data, and how much metadata (see callout above) is available to understand them. Also, note that you could clean each input dataset separately first, then harmonize the data, but it may be easier to harmonize first. For example, if you have 10 datasets, each with some type of sampling date column, it will be much easier to filter out particular dates if all that information is in the same format in the same column as opposed to working with whatever idiosyncratic formats and/or column names are present in each original file.\n\n\n\n\n\n\nDiscussion: Data Preparation\n\n\n\n\nHow many of you work directly with data in your day-to-day?\nOf the time you spend working with data, what percentage is spent:\n\nHarmonizing data?\nCleaning data?\nCreating metadata?\nDocumenting your process (including making comments in code)?\n\n\n\n\n\n\nHarmonize Data\nData harmonization is the process of bringing different datasets into a common format for analysis. The harmonized data format chosen for a synthesis project depends on the source data, analysis plans, and overall project goals. It is best to make a plan for harmonizing data before analysis begins, which means discussing this with the team in the early stages of a synthesis project. As a general rule, it is also wise to use a scripted workflow that is as reproducible as possible to accomplish the harmonization you need for your project. Following this guidance lets others understand, check, and adapt your work, and will also make it much, much easier to bring new data and analysis methods into the project.\nData harmonization is hard work that sometimes requires trial and error to arrive at a useful end product. At the end of this section are some additional data harmonization resources to help you get started. Looking at a simple example might also help.\n\n\n\n\n\n\nExample: Harmonizing grassland biomass data\n\n\n\n\n\nIn the figure below, two datasets from different LTER sites have been harmonized for analysis. We don’t have all the metadata here, but based on the column naming we can assume that the file on the left (Konza_harvestplots.txt, yellow) contains columns for the sampling date, a plot identifier, a treatment categorical value, and measured biomass values. The filename suggests that the data come from the Konza Prarie LTER site. The file on the right (2022_clips.csv, blue) has a column denoting the LTER site, in this case Sevilleta LTER (abbreviated as SEV), as well as similar date, plot identifier, treatment, and measured biomass columns.\nThere are some similarities and some differences in these two source files. A harmonized file (lter_grass_biomass_harmonized.txt) appears below.\n\n\n\nTake a minute to look at the harmonized file and consider how these data were harmonized. Then, answer this question:\n\nWhat changes were made to data structure, variable formatting, or units in these data?\n\n\n\n\n\n\n\nClick here for answers…\n\n\n\n\n\nEven though the source data files were similar, several important changes were made to create the harmonized file. Among them:\n\nThe site column was preserved and contains the “SEV” and “KNZ” categorical values denoting which LTER site is observed in each row.\nThe dates in the date column were converted to a standard format (YYYY-MM-DD). Incidentally, this matches the International Organization for Standardization (ISO) recommendation for date/time data, making it generally a good choice for dates.\nA new rep column was created to identify the replicate measurements at each site. This is essentially a standard version the plot identifier columns (PlotID and plot) in the original data file. Also note that the original values are preserved in the new plot_orig column.\nThe treatment columns from the original data files (Treatment and trt) were standardized to one column with “C” and “F” categorical values, for control and fertilized treatments, respectively.\nThe biomass values from Konza were converted to units grams per meter squared (g/m2) because the original Konza measurements were for total biomass in 2x2 meter plots. Note that this conversion is for illustration purposes only–we can’t be sure if this conversion is correct without it being spelled out in the metadata, or asking the data provider directly.\n\n\n\n\n\n\n\n\nA Word About Harmonized Data Formats\nAbove, we have discussed several aspects of selecting a data format. There are at least three related, but not exactly equivalent, concepts to consider when formatting data. First, formats describe the way data are structured, organized, and related within a data file. For example, in a tabular data file about biomass, the measured biomass values might appear in one column, or in muiltiple columns. Second, the values of any variable can be represented in more than one format. The same date, for example, could be formatted using text as “July 2, 1974” or “1974-07-02.” Third, format may refer to the file format used to hold data on a disk or other storage medium. File formats like comma separated value text files (CSV), Excel files (XLSX), JPEG images, are commonly used for research data, and each has particular strengths for certain kinds of data.\nA few guidelines apply:\n\nFor formatting a tabular dataset, err towards simpler data structures, which are usually easier to clean, filter, and analyze\n\nKeeping data in “long format”,3 is one common recommendation for this\n\nWhen choosing a file format, err towards open, non-proprietary file formats that more people know and have access to\n\nDelimited text files, such as CSV files, are a good choice for tabular data\n\nUse existing community standards for formatting variables and files as long as they suit your project methods and scientific goals\n\nUsing ISO standards for date-time variables, or species identifiers from a taxonomic authority, are good examples of this practice\n\nThere is no perfect data format!\n\nHarmonizing data always involves some judgement calls and tradeoffs\n\n\nWhen choosing a destination format for the harmonized data for a synthesis project, the audience and future uses of the data are also an important consideration. Consider how your synthesis team will analyze the data, as well as how the world outside that team will use and interact with the data once it is published. Again, there is no one answer, but below are a few examples of harmonized destination formats to consider.\n\nLong Format (a.k.a. “Tidy”)Wide FormatRelational DatabasesCloud-NativeOther…\n\n\nHere our grassland biomass data is in long format, often referred to as “tidy” data. Data in this format is generally easy to understand and use. There are three rules for tidy data:\n\nEach column is one variable.\nEach row is one observation.\nEach cell contains a single value.\n\n\n\n\nvisual representation of the tidy data structure\n\n\nAdvantages: clear meaning of rows and columns; ease in filtering/cleaning/appending\nDisadvantages: not as human-friendly so it can be difficult to assess the data visually\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, in long format.\n\n\n\n\nIn this dataset, our grassland data has been restructured into wide format, often referred to (sometimes unfairly) as “messy” or “untidy” data. Note that the biomass variable has been split into two columns, one for control plots and one for fertilized plots.\nAdvantages: easier for some statistical analyses; easier to assess the data visually\nDisadvantages: may be more difficult to clean/filter/append, multiple observations per row; more likely to contain empty (NULL) cells\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, restructured into wide format with biomass values in control and fertilized columns.\n\n\n\n\nBelow is an example of how we might structure our grassland data in a relational database. The schema consists of three tables that house information about sampling events (when, where data were collected), the plots from which the samples are collected, and the biomass values for each collection. The schema allows us to define the data types (e.g., text, integer), add constraints (e.g., values cannot be missing), and to describe relationships between tables (keys). Relational formats are normalized to reduce data redundancy and increase data integrity, which can help us to manage complex data4.\n\n\n\nexample grassland database schema\n\n\nAdvantages: reduced redundancy, greater integrity; community standard; powerful extensions (e.g., store and process spatial data); many different database flavors to meet specific needs\nDisadvantages: significant metadata needed to describe and use; more complex to publish; learning curve\nPossible file formats: Database stores, can be represented in delimited text (CSV)\nA richer example is a schematic of the related tables that comprise the ecocomDP5 harmonized data format for biodiversity data. Eight tables are defined, along with a set of relationships between tables (keys), and constraints on the allowable values in each table.\n\n\n\nThe ecocomDP schema. Each table has a name (top cell) and a list of columns. Shaded column names are primary keys, hashed columns have constraints, and arrows represent relations between keys/constraints in different tables.\n\n\n\n\nThere are many possibilities to make large synthesis datasets available and useful in the cloud. These require specialized knowledge and tooling, and reliable access to cloud platforms.\nAdvantages: easier access to big (high volume) data, can integrate with web apps\nDisadvantages: less familiar/accessible to many scientists, few best practices to follow, costs can be higher\nPossible file formats: Parquet files, object storage, distributed/cloud databases\n\n\n\nA few of the cloud-native technologies that might be useful for synthesis research products.\n\n\n\n\nThere are many, many other possible harmonized data formats. Here are a few possible examples:\n\nDarwinCore archives for biodiversity data\nOrganismal trait databases\nArchives of cropped, labeled images for training machine or deep learning models\nLibraries of standardized raster imagery in Google Earth Engine\n\n\n\n\n\n\n\nClean Data\nWhen assembling large datasets from diverse sources, as in synthesis research, not all the source data will be useful. This may be because there are real or suspected errors, missing values, or simply because they are not needed to answer the scientific question being asked (wrong variable, different ecosystem, etc.). Data that are not useful are usually excluded from analysis or removed altogether. Data cleaning tends to be a stepwise, iterative process that follows a different path for every dataset and research project. There are some standard techniques and algorithms for cleaning and filtering data, but they are beyond the scope of this course. Below are a few guidelines to remember, and more in-depth resources for data cleaning are found at the end of this section.\n\nAlways preserve the raw data. Chances are you’ll want to go back and check the original source data at least once.\nUse a scripted workflow to clean and filter the raw data, and follow the usual rules about reproducibility (comments, version control, functionalization).\nConsider using the concept of data processing “levels,” meaning that defined sets of data flagging, removal, or transformation operations are applied consistently to the data in stepwise fashion. For example, incoming raw data would be labeled “level 0” data, and “level 1” data is reached after the first set of processing steps is applied.\nSpread the data cleaning workload around! Data cleaning typically demands a huge fraction of the total time devoted to working with data,678 and it can be tedious work. Make sure the team shares this workload equitably.\n\n\nA Word About Filtering Data\nWhen filtering data, you are removing rows that you do not want to include in analysis. However, “filtering” is not a monolith and can be split into two categories that warrant different strategies. Some data are unusable and are removed for that reason, however, some data are potentially usable, but aren’t relevant to the current analysis.\nBecause of this latter category, filtering data should be done as late as possible in your workflow, ideally, as the final step before analysis. The reason for this is that you will eventually finish testing your first hypothesis and then the criteria for ‘is this data usable’ will change! If the very first step of your workflow as removing all data you didn’t want in that moment, you might have to return all the way to the start in order to get the data you now need for this subsequent hypothesis.\nIf instead, you develop your workflow with as much of the usable data as possible, when your filtering criteria evolve, most of your workflow–i.e., the labor-intensive harmonization and cleaning steps–is already good as-is and likely won’t need to be revisited. You simply create a new ‘filter’ script with the new criteria and can immediately leap into analysis."
  },
  {
    "objectID": "module2.html#analyze-data",
    "href": "module2.html#analyze-data",
    "title": "Operational Synthesis",
    "section": "3. Analyze Data",
    "text": "3. Analyze Data\nOnce the team has found sufficient source data, then harmonized, cleaned, and filtered countless datasets, and documented and described everything with quality metadata, it is finally time to analyze the data! Great! Load up R or Python and get started, and then tell us how it goes. We simply don’t have enough time to cover all the ins and outs of data analysis in a this course. However, we have put a few helpful resources below to get you started, and many of the best practices we have talked about, or will talk about, apply:\n\nDocument your analysis steps and comment your code, and generally try to make everything reproducible.\nUse version control as you analyze data.\nGive everyone a chance! Analyzing data is challenging, exciting, and a great learning opportunity. Having more eyes on the analysis process also helps catch interesting results or subtle errors."
  },
  {
    "objectID": "module2.html#synthesis-group-case-studies",
    "href": "module2.html#synthesis-group-case-studies",
    "title": "Operational Synthesis",
    "section": "Synthesis Group Case Studies",
    "text": "Synthesis Group Case Studies\nTo make some of these concepts more tangible, let’s consider some case studies. The following tabs contain GitHub repositories for real teams that have engaged in synthesis research and chosen to preserve and maintain their scripts in GitHub. Each has different strengths and you may find that facets of each feel most appropriate for your group to adopt. There is no single “right” way of tackling this but hopefully parts of these exemplars inspire you.\n\nExample 1Example 2Example 3\n\n\nLTER SPARC Group: Soil Phosphorus Control of Carbon and Nitrogen\nStored their code here:  lter / lter-sparc-soil-p\nHighlights\n\nStraightforward & transparent numbering of workflow scripts\n\nFile names also reasonably informative even without numbering\n\nSimple README in each folder written in human-readable language\nCustom .gitignore safety net\n\nControls which files are “ignored” by Git (prevents accidentally sharing data/private information)\n\n\n\n\nLTER Full Synthesis Working Group: The Flux Gradient Project\nStored their code here:  lter / lterwg-flux-gradient\nHighlights\n\nExtremely consistent file naming conventions\nStrong use of sub-folders for within-project organization\nTop-level README includes robust description of naming convention, folder structure, and order of scripts in workflow\nActive contribution to code base by nearly all group members\n\nFacilitated by strong internal documentation and consenus-building prior to choosing this structure\n\n\n\n\nLTER Full Synthesis Working Group: From Poles to Tropics: A Multi-Biome Synthesis Investigating the Controls on River Si Exports\nStored their code here:  lter / lterwg-silica-spatial\nHighlights\n\nFiles performing similar functions share a prefix in their file name\nUse of GitHub “Release” feature to get a persistent DOI for their codebase\nSeparate repositories for each manuscript\nNice use of README as pseudo-bookmarks for later reference to other repositories\n\n\n\n\nFor more information about LTER synthesis working groups and how you can get involved in one, click here."
  },
  {
    "objectID": "module2.html#additional-resources",
    "href": "module2.html#additional-resources",
    "title": "Operational Synthesis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nCourses, Workshops, and Tutorials\n\nSynthesis Skills for Early Career Researchers (SSECR) course. 2025. LTER Network Office\nCollaborative Coding with GitHub workshop. 2025. LTER Scientific Computing team\nCoding in the Tidyverse workshop. 2025. LTER Scientific Computing team\nR / Python Bilingualism tutorial. 2025. Nick J Lyon\nR Programming for Biologists (300-level undergraduate) course. 2024. Nick J Lyon\nReproducible Approaches to Arctic Research Using R workshop. 2024. Arctic Data Center & NCEAS Learning Hub\nShiny Apps for Sharing Science workshop. 2022. Lyon, N.J. et al.\nTen Commandments for Good Data Management. 2016. McGill, B.\n\n\n\nLiterature\n\nLyon, N.J. & Earl, S. Conventional Commits: A better way to track changes with Git. 2024. DataBits\nTodd-Brown, K.E.O., et al. Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. 2022. Biogeosciences\nBorer, E.T. et al. Some Simple Guidelines for Effective Data Management. 2009. Ecological Society of America Bulletin\n\n\n\nData Preparation\n\nHarmonizingCleaning & Filtering\n\n\nFor R and Python users, there are excellent documentation resources that thoroughly cover data harmonization techniques like data filtering, reformatting, joins, and standardization.\n\nIn Hadley Wickham’s R for Data Science book, the chapters on data transforms and data tidying are a good place to start\nAlso for R users, the LTER Network’s ltertools package has a nice approach to harmonization with its intuitively-named harmonize function. See the ltertools package vignette for details\nIn Wes McKinney’s Python for Data Analysis book, the chapter on data wrangling is helpful.\nA nice article in “The Analysis Factor” describes wide vs long data formats and when to choose which (TLDR, it depends on your statistical analysis plan).\n\n\n\nData cleaning is complicated and varied, and entire books have been written on the subject.910\n\nFor some general considerations on cleaning data, see EDI’s “Cleaning Data and Quality Control” resource\nOpenRefine is an open-source, cross-platform tool for iterative, scripted data cleaning.\nIn R, the tidyverse libraries (e.g., dplyr, tidyr, stringr) are often used for data cleaning, as are additional libraries like janitor.\nIn Python, pandas and numpy libraries provide useful data cleaning features. There are also some stand-alone cleaning tools like pyjanitor (started as a re-implementation of the R version) and cleanlab (geared towards machine learning applications).\nBoth the R and Python data science ecosystems have excellent documentation resources that thoroughly cover data cleaning. For R, consider starting with Hadley Wickham’s R for Data Science book chapter on data tidying,11 and for python check Wes McKinney’s Python for Data Analysis book chapter on data cleaning and preparation.12\n\n\n\n\n\n\nData Analysis\n\nHarrer, M. et al. Doing Meta-Analysis with R: A Hands-On Guide. 2023. GitHub\nOnce again, for R and Python users, the same two books mentioned above provide excellent beginning guidance on data analysis techniques (exploratory analysis, summary stats, visualization, model fitting, etc). In Wickham’s R for Data Science book, the chapter on exploratory data analysis will help. In McKinney’s Python for Data Analysis book, try the chapters on plotting and visualization and the introduction to modeling."
  },
  {
    "objectID": "module2.html#footnotes",
    "href": "module2.html#footnotes",
    "title": "Operational Synthesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nMayernik, M.S. and Acker, A. (2018), Tracing the traces: The critical role of metadata within networked communications. Journal of the Association for Information Science and Technology, 69: 177-180. https://doi.org/10.1002/asi.23927↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nZimmerman, N. 2016. Hand-crafted relational databases for fun and science↩︎\nO’Brien, Margaret, et al. “ecocomDP: a flexible data design pattern for ecological community survey data.” Ecological Informatics 64 (2021): 101374. https://doi.org/10.1016/j.ecoinf.2021.101374↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nNew York Times, 2014↩︎\nAnaconda State of Data Science Report, 2022↩︎\nOsborne, Jason W. Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data. Sage publications, 2012.↩︎\nVan der Loo, Mark, and Edwin De Jonge. Statistical data cleaning with applications in R. John Wiley & Sons, 2018. https://doi.org/10.1002/9781118897126↩︎\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for data science. ” O’Reilly Media, Inc.”, 2023. https://r4ds.hadley.nz/↩︎\nMcKinney, Wes. Python for data analysis. ” O’Reilly Media, Inc.”, 2022. https://wesmckinney.com/book↩︎"
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "A Course for Collaborative Ecologists",
    "section": "Abstract",
    "text": "Abstract\nIn recent decades, ecology has become a more collaborative discipline motivated by the search for generality across ecosystems. At the same time,the availability, quantity, and quality of environmental data have grown rapidly, creating opportunities for re-use of these data in ecological synthesis research. Though synthesis research is complex and demanding, taking an inclusive and collaborative approach to both the scientific process and the data pays dividends throughout the lifetime of a project. This short course is a survey of methods for making ecological synthesis research a “team sport”.\nObjectives for learners are to\na.  gain project management and data synthesis skills that are immediately useful in a research team setting,\nb.  develop an end-to-end (conception to publication) plan for collaborative synthesis research and\nc.  focus on developing their own synthesis projects, whatever stage they are in, with guidance from instructors and peers.\nInstructors will cover assembling the team, study design, communication, collecting primary data sources, assembly/harmonization of data, managing analytical workflows, and publication of derived datasets and other synthesis products. The course uses real-world examples, demonstrations, and interactive lessons in small groups. Ecologists with synthesis experience will be on hand with seasoned research advice and data tips."
  },
  {
    "objectID": "index.html#agenda-for-2025-08-10",
    "href": "index.html#agenda-for-2025-08-10",
    "title": "A Course for Collaborative Ecologists",
    "section": "Agenda for 2025-08-10",
    "text": "Agenda for 2025-08-10\nThis agenda is subject to change!\n\n\n\nTiming (PT)\nContent\n\n\n\n\n9:00 - 9:30a\nWelcome & Introductions\n\n\n9:30 - 10:30a\nModule 1: Starting with Team Science\n\n\n10:30a - 12:00p\nModule 2: Operational Synthesis\n\n\n12:00 - 1:00p\nBreak\n\n\n1:00 - 2:00p\n… Module 2, continued …\n\n\n2:00 - 3:00p\nModule 3: Tying it all together\n\n\n3:00 - 4:00p\nMentoring session…"
  },
  {
    "objectID": "index.html#shared-notes-document",
    "href": "index.html#shared-notes-document",
    "title": "A Course for Collaborative Ecologists",
    "section": "Shared notes document",
    "text": "Shared notes document\nA communal notes document will be used during some of the discussions we’ll have, and are a good place to drop on-the-fly feedback about the course. Keep the link handy and we’ll ask for your input to these notes at key times."
  },
  {
    "objectID": "index.html#introductions-icebreaker",
    "href": "index.html#introductions-icebreaker",
    "title": "A Course for Collaborative Ecologists",
    "section": "Introductions & Icebreaker",
    "text": "Introductions & Icebreaker\nBefore we begin, we’d love to get a sense for who you all are and why you’re interested in synthesis work! To that end, we’ll take a few minutes and go around the room for introductions. Please include:\n\nYour name and pronouns\nA 1-sentence summary of your work\nBriefly, why are you interested in synthesis?"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "A Course for Collaborative Ecologists",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nESA Code of Conduct\nBriefly: Treat all participants with kindness, respect, and consideration, valuing a diversity of views and opinions. Report unacceptable behavior at (844)-641-4133 or email codeofconduct@esa.org"
  },
  {
    "objectID": "index.html#note-on-course-materials",
    "href": "index.html#note-on-course-materials",
    "title": "A Course for Collaborative Ecologists",
    "section": "Note on Course Materials",
    "text": "Note on Course Materials\nThis short course is being offered for the second time at ESA 2025, and we’ve chosen to assemble these materials as a living website so that we can revisit and improve the materials over time. So, we recommend that you save the link to this site so that you can have easy access to these materials now and as they are refined going forward.\nIf you are a  GitHub aficionado, we have deployed this website via GitHub Pages so you could also “star” the website’s repository. Simply click the  GitHub octocat logo on the right side of the navbar (at the top of the screen) to be redirected to the GitHub repository underpinning this website.\nFinally, we have developed all of this website using Quarto."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "A Course for Collaborative Ecologists",
    "section": "Credits",
    "text": "Credits\nThe course and its content were developed by a large team. See the People page to learn more.\nSupported by:\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollage photo credits: Jacob Bøtter via Flickr, CC BY-SA 2.0 | Jeremy Yoder via Flickr, CC BY-SA 2.0 | Marco Pfeiffer, CC BY-SA 4.0 | Gabriel De La Rosa, CC BY-SA 4.0 | Weecology lab CC BY 4.0 | NEON (National Ecological Observatory Network)"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "contributing.html#overview",
    "href": "contributing.html#overview",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nHave tools for establishing group norms and expectations\nEvaluate strategies for group organization and project management\n\nWorkshop Slides - Under Construction\nGoogle Slides link\nDefine Synthesis\nScientific research can be defined as “creative and systematic work undertaken in order to increase the stock of knowledge” (2015 Frascati Manual). To this basic definition of research, our definition of synthesis research adds collaborative work, and the integration and analysis of a wide range of data sources."
  },
  {
    "objectID": "module1.html#learning-objectives",
    "href": "module1.html#learning-objectives",
    "title": "Starting with Team Science",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nUnderstand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nHave tools for establishing group norms and expectations\nEvaluate strategies for group organization and project management\n\nWorkshop Slides - Under Construction\nGoogle Slides link\nDefine Synthesis\nScientific research can be defined as “creative and systematic work undertaken in order to increase the stock of knowledge” (2015 Frascati Manual). To this basic definition of research, our definition of synthesis research adds collaborative work, and the integration and analysis of a wide range of data sources."
  },
  {
    "objectID": "module1.html#why-synthesis",
    "href": "module1.html#why-synthesis",
    "title": "Starting with Team Science",
    "section": "Why Synthesis?",
    "text": "Why Synthesis?\n\nThis process–of bringing diverse expertise together to combine existing primary data–definitely has its challenges. Identifying and engaging the necessary combination of skills and experience is challenging. Scheduling meeting times can be difficult. Commitment to the process can be uneven. Nonetheless, virtually all who have experienced it find it to be a deeply rewarding experience. Why?\n\nThe products emerging from synthesis typically have higher impact (as defined by both academic and applied measures)\nThe process allows researchers to access and incorporate skills that they don’t (yet) have themselves.\nWorking groups help early career researchers build their science networks.\nKeep experienced researchers fresh and engaging with new ideas\nBuilds on existing investments of the science community by re-using data\nOffers a way to involve individuals who can’t or don’t want to do fieldwork in original research and expands opportunity to less research-intensive institutions.\n\n\nExamples of Recent Synthesis Products\n\nKey products from recent LTER synthesis groups\n\n\n\nEnvironmental drivers of plant reproduction\nCommunity Synchrony in Seed Production is Associated with Trait Similarity and Climate Across North America. Ecology Letters, 2024.\n\n\n\nMetacommunity dynamics mediate community responses to disturbance\n\nThe dual nature of metacommunity variability, Oikos, 2021.\n\n\n\nSoil organic matter: multi-scale observations, manipulations & models\nDataset: SoDaH: the SOils DAta Harmonization database, an open-source synthesis of soil data from research networks, version 1.0., EDI 2020\n\n\n\nPopulation and Community Synchrony\nThe spatial synchrony of species richness and its implications for ecosystem stability. Ecology 2021.\n\n\n\nGlobal Patterns in Stream Energy and Nutrient Cycling\nShifting stoichiometry: Long-term trends in stream-dissolved organic matter reveal altered C:N ratios due to history of atmospheric acid deposition, Global Change Biology, 2022.\n\n\n\nControls on River Silica Exports\nCombined data from more than 1000 watersheds on 6 continents.\nClimate, Hydrology, and Nutrients Control the Seasonality of Si Concentrations in Rivers. JGR Biosciences 2024\n\n\n\nMarine consumer nutrient dynamics\nDataset: Estimates of nitrogen and phosphorus excretion rates in individual marine and estuarine animals ver 1. EDI 2024.\n\n\n\nFire and Aridland Streams\nPersistent and lagged effects of fire on stream solutes linked to intermittent precipitation in arid lands. Biogeochemistry 2024\n\n\n\nEcological Metagenome- Derived Reference Genomes and Traits\nFrom soil to sequence: filling the critical gap in genome-resolved metagenomics is essential to the future of soil microbial ecology, Environmental Microbiome, 2024\n\n\n\nGlobal Synthesis of Multi-year Drought Effects\nIntegrated 100 grassland and shrubland sites across 6 continents\nExtreme drought impacts have been underestimated in grasslands and shrublands globally. PNAS 2024.\n\n\n\nIntegrating plant community and ecosystem responses to chronic global change drivers\nDataset: CoRRE Trait Data: A dataset of 17 categorical and continuous traits for 4079 grassland species worldwide, Scientific Data, 2024\n\n\n\nScaling-Up Productivity Responses to Changes in Biodiversity\n\nBiotic homogenization destabilizes ecosystem functioning by decreasing spatial asynchrony. Ecology 2021.\n\n\n\n\n\nAdditional Resources\n\nAdam, 2023. ‘Disruptive’ science: in-person teams make more breakthroughs than remote groups\nCheruvelil, K. S., & Soranno, P. A. (2018). Data-Intensive Ecological Research Is Catalyzed by Open Science and Team Science. BioScience, 68(10), 813–822. https://doi.org/10.1093/biosci/biy097\nHackett, 2020. Collaboration and Sustainability: Making Science Useful, Making Useful Science\nHampton and Parker, 2011. Collaboration and Productivity in Scientific Synthesis\nHackett et al., 2021. Do synthesis centers synthesize? A semantic analysis of topical diversity in research\nWyborn et al., 2018. Understanding the Impacts of Research Synthesis"
  },
  {
    "objectID": "module1.html#process-overview",
    "href": "module1.html#process-overview",
    "title": "Starting with Team Science",
    "section": "Process Overview",
    "text": "Process Overview\nTypically, a group of researchers–or researchers and managers or community members–will plan a series of meetings over 2-3 years. The mix of in-person v. virtual meetings and work will vary across different groups and different funders, but the general pattern is similar.\n\n\n\nEarly meetings focus on narrowing the questions and deciding what data is needed and what analyses will be most useful. A period of data gathering and assembly comes next. The assembly of data almost always prompts a revision of the initial questions, as data rarely comes in exactly the form that researchers expected. This can be both the most frustrating and the most interesting part of the process as new hypotheses and models are floated and discussed. It is especially important to have the full participation of researchers familiar with different fields and ecosystems in this process.\nWith tractable questions refined, the group will move into analysis mode. Often, a few individuals will do most of the data wrangling and coding, but will need continuous input on analytical decisions. In our experience, GitHub issues is one very good tool for facilitating and recording these decisions. But the “best” tool will be the one that most members of the team are most comfortable with.\nLater meetings will focus on developing manuscripts and/or application-related products such as white papers and decision support tools.\nHow to get Involved\nOften, early career researchers will be excited about the idea of synthesis but be unsure how to connect with existing or nascent synthesis efforts. Here are a few ideas for how to make yourself available and valuable to synthesis groups.:\n\nMake it known you want to be involved in synthesis\n\nLet your advisor know\nShare your enthusiasm\n\nSkill building:\n\nSynthesis Skills for Early Career Researchers: SSECR\nData Carpentries\nESIIL: innovation summit, hackathons\nEnvironmental Data Science Summit\n\nBuild your community\n\nAsk questions at meetings\nInitiate conversations\n\nStart your own!\n\nIdentifying a Synthesis-Ready Question\nLots of questions are interesting, but not terribly well-suited for a synthesis approach. We’ve learned through experience that there are a few qualities that make some questions a better fit for a) combining data; and b) work by a group. The main qualities that we seek in synthesis projects include:\n\nNovel and interesting enough to keep you engaged for 2-3 years.\nData already exist and you know (at least generally) where to find it\nQuestions cover a large geographic area or data that aren’t normally collected or analyzed together\nClearly framed, but flexible enough to allow adaptation through the process\nResponsive to the funding call\nOutputs could include several kinds of products. As the project progresses, gard students will become postdocs, postdocs will get faculty positions. For the project to remain satisfying to all participants, people will need to be able to take a leadership role on different kinds of products.\n\nPapers (including data papers, perspectives, gap analyses, as well as primary analyses)\nSymposia\nDatasets\nAnalytical Packages"
  },
  {
    "objectID": "module1.html#sources-of-support-for-synthesis",
    "href": "module1.html#sources-of-support-for-synthesis",
    "title": "Starting with Team Science",
    "section": "Sources of Support for Synthesis",
    "text": "Sources of Support for Synthesis\nWhile it is certainly possible to conduct synthesis with no external support, a bit of funding will allow your group to travel to meet up in person and can, in some cases, provide salary support for postdocs, grad students or or assistance with analysis.\n\nDiscussion QuestionSome Ideas\n\n\n\nWhat funding sources support synthesis work?\n\n\n\nSources of Funding and Support\nSynthesis Centers\n\nEnvironmental Science Innovation and Inclusion Laboratory (ESIIL)\nNational Center for Ecological Analysis and Synthesis (NCEAS)\n\nMorpho Program\n\nUSGS Powell Center\nS-div\nCanadian Institute of Ecology and Evolution (CIEE)\n\nSocieties\n\nNew Phytologist Workshops\nGordon Research Conferences\nChapman Conferences\nBritish Ecological Society\n\none-third of participants from developing world\n\n\nNSF Programs\n\nULTRA-data Dear Colleague letter\nNSF Core Programs, e.g. Division of Environmental Biology, and others: search for “synthesis activities”, “synthesis projects”\nNSF workshops"
  },
  {
    "objectID": "module1.html#building-a-team",
    "href": "module1.html#building-a-team",
    "title": "Starting with Team Science",
    "section": "Building a Team",
    "text": "Building a Team\nThe Leadership Team\nThe composition of the leadership team will affect the success of the project and who you will be able to recruit to the larger group. Look for:\n\nDifferent (and complementary) areas of expertise\nComplementary professional networks\nFacilitation skills\nEmotional Intelligence\n\nThe Broader Team\nIn our experience at NCEAS and the LTER Network Office, we’ve found teams of up to 10 to 15 people to be optimal for synthesis work. As individuals, we all have strengths and weaknesses. The beauty of working in teams is that you can invite people who offset your own weaknesses and who bring strengths you don’t have. Often, you’ll have a few core team members who have generated a synthesis idea, but then you’ll want to take a clear-eyed look at what additional skills and qualities to invite. When you do so, be sure to consider:\n\nSkills, Aptitudes, and Communication Styles\n\nLook for a mix of empiricists, theorists, and modellers\nBig-picture thinkers, organizers, task-oriented do-ers\nDeep thinkers and risk-takers\nAt least some skilled coders\n\nCareer stage\n\nSenior investigators connect the team to existing literature and fields of study, connect to a broad network of experienced researchers, and have good knowledge of resources, but are often have a very limited amount of time to devorte to discussion and analyses\nJunior team members often bring a fresh perspective, familiarity with newer literature, strong coding skills, and time to devote to the project\n\nEmotional intelligence\n\nResearch shows (Aggarwal and Woolley, 2018) that the bump in creativity seen in mixed-gender teams is typically due to an increase in emotional intelligence and attention to team dynamics. Include at least a few people with a process orientation and strong people skills.\n\nPower dynamics\n\nYou won’t be able to anticipate all of the issues related to power dynamics that can arise, but keep them front of mind as you assemble a team.\n\nRemember that participation in synthesis represents a significant career opportunity\n\nBe mindful that such career-building opportunities have not been fairly distributed\nBe intentional seeking out people who may not be part of your typical circles (including gender, ethnicity, career stage, family status, (dis)abilities, etc.)\n\n\nAdditional Resources\n\nAggarwal, I. & Woolley, A.W. (2018) Team Creativity, Cognition, and Cognitive Style Diversity. Management Science 65(4):1586-1599. doi: 10.1287/mnsc.2017.3001\nCheruvelil, K.S. et al. (2014) Creating and maintaining high-performing collaborative research teams: the importance of diversity and interpersonal skills. doi: 10.1890/130001\nHorowitz, S.K. and Horwitz, I.B. (2007) The effects of team diversity on team outcomes: A meta-analytic review of team demography. Journal of Management 33: 987-1015. doi: 10.1177/0149206307308587\nPieterse, A.N et al. (2013) Cultural Diversity and Team Performance: The Role of Team Member Goal Orientation. doi: 10.5465/amj.2010.0992\nvan Knippenberg,D. and Hoever, I.J. (2017) Team Diversity and Team Creativity: A Categorization-Elaboration Perspective. doi: 10.1093/oso/9780190222093.003.0003\nWoolley, A. W. et al. (2010). Evidence for a Collective Intelligence Factor in the Performance of Human Groups. Science, 330(6004), 686–688. doi: 10.1126/science.1193147"
  },
  {
    "objectID": "module1.html#setting-expectations",
    "href": "module1.html#setting-expectations",
    "title": "Starting with Team Science",
    "section": "Setting Expectations",
    "text": "Setting Expectations\nIn any team project, people have different reasons for wanting to participate - and different anxieties about what “participation” will mean. For you, getting a high profile paper may be the most important thing. For others, it may be expanding their network or a chance to practice new skills. Being transparent about those goals, and the behaviors that support them, makes it easier to resolve tensions when they arise.\n\n\n\n\nImportance of various benefits to working group participants\n\n\n\n\n\n\n\nTime Commitments\n\n\n\nWe all misjudge our availability once in a while, but consistently failing to deliver on commitments disrupts others’ work plans and is a major source of group dissatisfaction. Get buy-in for commitments and plan for both reminders and accountability.\n\n\nThere are many approaches to establishing group norms, but a shared process that helps create ownership and buy-in is one key to a smoothly-functioning working group.\nThe process can be as simple as taking 15 minutes to ask the group about their shared (and diverging) values and what those imply about how the group should function. The best choice for your group will depend on the mix of participants, the nature of the content, and the duration of your collaboration.\n\n\n\n\n\n\nFrom Biodiversity on a Changing Planet working group (PI: Peter Adler)\n\n\n\n\n\nThis version is simplest and most appropiate when your group has some shared history and mainly needs a reminder to attend to their share values. Even so, be sure to leave enough time and space for participants to add new ideas or concerns. Offer some basic starting point values and norms, then ask the group to add any that haven’t been raised yet. Record the results and return to them at the start of meetings.\nGroup Values\n\nInclusion\nCreative Thinking\nTeamwork\nAccountability\nFun\n\nWhat else?\n\n\n\n\n\n\n\n\n\nFrom Response Diversity Network Workshop\n\n\n\n\n\nThis version is a little more involved and directive with resepct to behaviors as well as values. Edit, add, or delete suggestions depending on any concerns in your group.\nCommunity Rules for Inclusive and Productive Discussions\n\nWe are all responsible for cultivating a respectful and inclusive atmosphere to benefit from our diverse community of participants.\nListen with curiosity and resilience, not judgement.\nAssume the best of intentions.\nAllow others to participate and avoid dominating the conversation.\nAccept/meet people where they are.\nBring a spirit of generosity (for yourself and others).\nBe kind to yourself.\nSupport learning.\nBe attentive to power and privilege.\nEnjoy yourself!\n\n\n\n\n\n\n\n\n\n\nFrom Entering Mentoring (CIMER)\n\n\n\n\n\nFor longer collaborations with more challenging power dynamics, it may be worth engaging in a slightly more involved process. In workshops run by the Center for Improved Mentoring of Experiences in Research (CIMER), a facilitator presents a set of group behaviors categorized as group-oriented or self-oriented.\n\n\n\nBreak onto small groups and discuss the following questions:\n\nWhat are some strategies to maximize group-oriented & minimize self-oriented behaviors?\nAre there special considerations for an online space? How can group dynamics be different in an online vs. face-to-face environment?\nHow can we leverage our knowledge of group behaviors as we work together during this workshop?\n\nThe exercise presents an opportunity for self-reflection and for participants to commit to (and ask for help in) curbing their unhelpful behaviors. After all, we all have them!\n\n\n\nMaking a Communication and Work Plan\nGood teams are both chosen and made. Diversity on teams uncovers novel approaches, perspectives, and insights and it can slow the pace and cause misunderstandings that highlight unexamined assumptions. As a synthesis team leader (and even as a participant), there are many things you can do to create opportunities for everyone to contribute their best thinking, learn from one another, and feel heard and respected.\nAs the group gets started\n\nCreate a shared vision for your group\n\nMake sure everyone starts on the same footing with a brief overview of the context for the group and the questions you’re starting with\n\nCo-develop group norms. It builds ownership of shared norms.\nDevelop an authorship policy\n\nWhat contributions warrant authorship on a paper? Collecting data? Contributing data? Being part of a discussion that sparked the idea? Developing figures?\nBut also, how do group members learn that a paper is being developed?\n\nCan all group members opt-in to any paper?\nOr is everyone an author until they opt out?\nNetwork and group authorship policies are more complex than tallying contributions to a single paper.\n\n\nConduct a “pre-mortem” to talk about worries and visions for a healthy group dynamic\nRecognize who you are as a group, culturally, and any power dynamics that might entail\n\nSometimes, simply articulating the potential for oppressive power dynamics can give group members the confidence to assert themselves\n\n\nThroughout the process\n\nGet to know each other\n\nUse creative icebreakers to break a pattern of silence\nInvest in “social time”\n\nValue and accommodate various styles of contribution\n\nFast and slow thinkers,\nVisual, auditory and kinesthetic learners,\nSynchronous and asynchronous contributions\n\nPractice “cultural norming” by educating participants on systemic/structural oppression and racism and ways to work against our own implicit biases\n\nAt intervals\n\nWhen you find yourself questioning whether a practice or activity is still valuable, ask each member for a quick read on whether the group should “Start, Stop, or Continue” the activity"
  },
  {
    "objectID": "module1.html#establishing-a-project-management-plan",
    "href": "module1.html#establishing-a-project-management-plan",
    "title": "Starting with Team Science",
    "section": "Establishing a Project Management Plan",
    "text": "Establishing a Project Management Plan\nAs your group gets started working together, it it easy to assume that you will use the tools and planning strategies that the PI or project organizer is used to using. That information should carry some weight. They will devote a lot of time to the project. But also try to survey the group at an early meeting so that you know which platforms other group members use. Make decisions based on balancing platforms that will allow maximum group participation with those that will make the work easiest for those likely to be doing the work.\nConsider:\n\nHow will you communicate? (Email, Slack, Google Group, Discourse, Discord, Zulip…) Who maintains the list, and how?\n\nA regular “update” email (every 2 weeks or once a month) is a great practice\nYou will likely generate lots of ideas for papers and other products. How do group members who didn’t happen to be in that conversation find out about them?\n\nWhere will you share common documents and how will you organize them? (Box, Dropbox, Google Drive, Other…email them around :eye_roll:)\nHow will you keep track of relevant references (a file folder of pdfs?, Zotero, EndNote, Mendeley…)? Each has strengths and weaknesses–consider how participants are most likely to contribute and how easy they will be to fnd later.\n\nDo you need a shared calendar or virtual bulletin board? Consider Google Calendar, GitHub Pages, Quarto, or a simple WordPress or Weebly site, but beware of the time it can consume.\nWho is doing most of the coding?\n\nWhat platfoms do they use?\nDo others need to see/weigh in on the code? Can they access those platforms? How will they get notice that their input is needed?\n\nHow do you write together? The platform matters, but so does the process.\n\nSome groups outline as a team and then assign sections to different writers.\nIn others, everyone contributes figures, and concepts as bullet points, but a single author crafts the actual prose.\nSuggesting and track changes features are great for modest edits on a nearly completed document, but can be overwhelming when a paper is still in development.\n\nAn alternative is to create a read-only version of a document with line numbers and ask for comments by line number.\nIt is also helpful to be clear about what kind of input you are after at each stage of writing. Are you just trying to get the analysis clear or do you want wordcraft?"
  },
  {
    "objectID": "module1.html#data-sources",
    "href": "module1.html#data-sources",
    "title": "Starting with Team Science",
    "section": "Data Sources",
    "text": "Data Sources\nSome sources of data–such as modern remote sensing products, NEON data, and census data–have very clear, explicit ways to access and download them or work with them in the cloud. But the most interesting synthesis questions often involve combining such “big” data with other data sources that may have been collected manually, by a variety of methods and different technicians, over decades.\n\nDiscussion QuestionA Few Ideas\n\n\n\nWhat kinds of data sources might you consider including in a synthesis project, in addition to your own or others’ field data?\n\n\n\n\nDataONE\nEnvironmental Data Initiative (EDI)\nGenBank (NCBI)\nNational Ecological Observatory Network (NEON)\nUS Geological Survey Data\nGlobal Biodiversity Information Facility (GBIF)\nNASA Remote Sensing Data\nUS Park Service\nFluxNet\nPhenocam network\niNaturalist\neBird\nCensus data\nData extracted from papers\nScraping social media\nText analysis\n….\n\n\n\n\nData Use Principles\nThere are a few ethical and practical guidelines that will save you a lot of trouble if you can adhere to them from the start of a project.\n\nData sources should always be cited\nKeep track of your data sources (sources, permissions, notes, related metadata, what’s included)\nKeep the data that everyone is uding in one place (single source of truth)\nCommunicate with data creators whenever possible\n\nThis doesn’t need to be onerous and it can uncover issues and opportunities associated with data sources.\n\n\n\n\n\n\n\n\nSample data author outreach email for public dataset\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include your dataset titled “xxx” (doi: xxx). We have downloaded the data from yyy repository, but wanted to let you know we are using it and to inquire whether there is any additional context we should be aware of or related datasets we should be sure to include. A short description of the synthesis project follows. Please let me know by xxx date if you have any questions or concerns with our use of this data.\nThank you,\n\n\n\n\n\n\n\n\n\n\nSample data author outreach email for unpublished dataset\n\n\n\n\n\n\nDear Dr. Smith,\nI am working on a synthesis of soil inverbrate diversity across North America and would like to include the dataset behind your paper titled {xxx} (doi:{yyy}), which seems highly relevant. Would it be possible to obtain the data? Ideally, we would access it through through a public repository, such as the Environmental Data Initiative, which offers assistance in curation and submission of datasets. Either way, we would credit you as the data originator and want you know we are using it. If there is any additional context we should be aware of or related datasets we should be sure to include, please let us know. A short description of the synthesis project follows.\nThank you,\n\n\n\n\n\nAuthorship DiscussionA Few Considerations\n\n\n\nShould all data contributors be offered authorship? How would you handle a data creator who demanded authorship in order to use their data?\n\n\n\nThere are no pat answers for this situation, but having agreed-on authorship guidelines is really valuable when it comes up. We’ll cover that in more detail soon, but for now, there are a few questions to ask yourself.\n\nAre they really committed enough to join the working group and contribute to the papers? If so, it may be a good investment.\nHow much work will they need to put in to make the data ready?\nHow critical is this particular data source for your analysis?\nYou will need to make your derived dataset public. Are they placing conditions on the use of their data that make that impossible?\n\n\n\n\nKeeping Track of Data\nWe’ve pulled together a few of the forms that we have use to keep track of the data that synthesis groups plan to use…\nSample spreadsheet for initial data surveying\n\nOnce you get into the details of the process, you’ll want to track some more specific information, but you’ll hear more about that in Module 2.\n\nURL to the data (and metadata) source\nSampling location and site (including both coordinates and associated organizations)\nShort Data Description\nCoverage Dates/Frequency\nFilename (as stored on Google Drive or shared file repository)\nURL to Files (cloud drive, website, server; e.g. google drive link)\nDate the data was last accessed / downloaded\nData Creator/Owner’s Name\nData Creator/Owner’s Email/contact\nWorking group participant who got the data (Name)\nUsed in your analysis? (Y/N)\nAny additional notes or decisions about how the data is or will be harmonized and analyzed"
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the three primary “products” that come out of synthesis groups.\nUnderstand the metadata and other features that make published datasets useful.\nEvaluate the reach and reproducibility of an ecological synthesis project’s outputs.\nCreate a plan for your synthesis team’s research products that applies contribution, publishing, and citation practices that will benefit the team."
  },
  {
    "objectID": "module3.html#learning-objectives",
    "href": "module3.html#learning-objectives",
    "title": "Tying It All Together",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify the three primary “products” that come out of synthesis groups.\nUnderstand the metadata and other features that make published datasets useful.\nEvaluate the reach and reproducibility of an ecological synthesis project’s outputs.\nCreate a plan for your synthesis team’s research products that applies contribution, publishing, and citation practices that will benefit the team."
  },
  {
    "objectID": "module3.html#introduction",
    "href": "module3.html#introduction",
    "title": "Tying It All Together",
    "section": "Introduction",
    "text": "Introduction\nSo far in the course, we’ve made the point that ecological synthesis research benefits from an inclusive, team-based approach to science, and that teams should use effective, collaborative methods to integrate and analyze their data. Synthesis research is also intended to be influential and useful. There are many definitions of “influential and useful” to consider here, but successful ecological synthesis teams typically aim to expand our understanding of ecological systems, and to improve human lives and the environment.  Their ability to accomplish this frequently depends on what research products are created, and how thosw are communicated and shared with the outside world.\n“Research products” can be defined broadly, but there are three interconnected, publishable products that are the most common outputs from a synthesis project (or any research project, really): the data, analytical workflows (code for data cleaning or statistics, for example) and research results. Each of these is a valuable product of synthesis science, and each one should reference the others. In this module we’ll discuss the mechanics of publishing each one, and how they can be made accessible and useful for the long-term."
  },
  {
    "objectID": "module3.html#publishing-a-synthesis-dataset",
    "href": "module3.html#publishing-a-synthesis-dataset",
    "title": "Tying It All Together",
    "section": "Publishing a synthesis dataset",
    "text": "Publishing a synthesis dataset\nIn Module 2 we discussed some considerations for creating and formatting harmonized data files in synthesis research. We also introduced the importance of metadata for describing data and making it more usable. Publishing harmonized data files and descriptive metadata together as a dataset helps ensure that the data products produced by a synthesis team are findable, accessible, interoperable, and reusable (FAIR). FAIR data are an important outcome for most ecological synthesis projects.\n\n\n\n\n\n\nMore about Findable, Accessible, Interoperable, Reusable (FAIR) data\n\n\n\n\n\nThe FAIR principles, standing for Findability, Accessibility, Interoperability, and Reusability, are a community-standard set of guidelines for evaluating the quality and utility of published research data. Making an effort to meet the FAIR criteria promotes both human and machine usability of data, and is a worthy objective when preparing to publish data from a synthesis research project.\nThe FAIR principles were first defined in the paper by Wilkinson et al.(2016). Since this time, many resources have arisen to guide the implementation of the FAIR principles1 and to quantify FAIR data successes and failures in the research and publishing communities (Bahim et al. 2020; Gries et al. 2023).\n\n\n\n\nActivity 1: Evaluate published datasets\nLets start our journey to publishing datasets by looking at some that are already published. Form breakout groups and course instructors will assign each group a dataset for evaluation. With your group, answer these questions about the dataset:\n\nWhere were the data collected?\nWhat variables were measured and in what units?\nWhat is the origin of the data and how have they been altered since collection?\nWere the first three questions easy to answer? Why or why not?\n\n\nGroup 1Group 2Group 3Group 4Group 5Group 6\n\n\nExample dataset: Jarzyna, M.A., K.E. Norman, J.M. LaMontagne, M.R. Helmus, D. Li, S.M. Parker, M. Perez Rocha, S. Record, E.R. Sokol, P. Zarnetske, and T.D. Surasinghe. 2021. temporalNEON: Repository containing raw and cleaned-up organismal data from the National Ecological Observatory Network (NEON) useful for evaluating the links between change in biodiversity and ecosystem stability ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/7f0e0598132e3fea1bfd36a4257af643.\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset is published in the EDI repository and comes from a synthesis group that harmonized NEON data to examine synchrony and stability in communities.\n\nRaw data, processed data, and the R code for the processing steps are all published together here.\nUseful metadata on the geographic, temporal, and spatial coverage of the dataset are included.\nA couple areas for improvement would be that the Abstract and Methods are a bit short on details, and no ORCIDs have been included for contributors.\n\n\n\n\n\n\nExample dataset: Gutenson, J. (2025). Yellowstone Synthetic Rating Curve Analysis. HydroShare. http://www.hydroshare.org/resource/c8388cc25940447296c0cd8eea58d97a\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset consists of a variety of data from different sources used to create “synthetic rating curves” - something to do with flood risk - in the Yellowstone Basin. The dataset contains lots of files in a folder structure, mainly consisting of spatial data (maps and raster data) as well as some scripts and possible metadata files in text format.\n\nThough this seems to be part of a study published in, or being prepared for, the “Journal of Flood Risk Management,” but there aren’t many details given to help find that article.\nThere are many useful-looking files here, but there aren’t any descriptions given in the metadata, so it would be hard to know how to use them.\nThe provenance (origin info) or many of the data files is given in the Abstract, which is helpful.\nThere is no DOI issued for this dataset, so it is difficult to know how permanent the dataset is and it would be tricky to cite it in a paper. That said, this was only recently published and it may still be a work in progress. Perhaps more metadata will be added and a DOI issued as the study progresses!\n\n\n\n\n\n\nExample dataset: Craine, Joseph M. et al. (2019). Data from: Isotopic evidence for oligotrophication of terrestrial ecosystems [Dataset]. Dryad. https://doi.org/10.5061/dryad.v2k2607\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset contains a harmonized dataset from a global study of oligotrophication (declining nutrient availability) in terrestrial ecosystems. Included in the dataset are a combined data file with values from many data sources, and the R code to analyze the combined data and create figures used in a related paper.\n\nIt is easy to find the “related work” from the Dryad dataset because the DOI appears on the landing page. The DOI (https://doi.org/10.1038/s41559-018-0694-0) points to the paper that used and cited this dataset, and this paper has some detail about the data sources and methods for assembling the combined dataset.\nIn theory it should be possible to reproduce the figures presented in the text using the R code provided.\nWithout having access to the related paper above, it would be difficult, or potentially impossible, to understand and use the CSV published in Dryad because few descriptive metadata are provided there. Though this appears to be a harmonized dataset, without the paper it is also unclear the origin of the source data, and how they have been changed to generate the harmonized CSV. Knowing important information like units, categorical code meanings, original data sources, and methods of harmonization might require contacting the authors.\n\n\n\n\n\n\nExample dataset: Wieder, W.R., D. Pierson, S.R. Earl, K. Lajtha, S. Baer, F. Ballantyne, A.A. Berhe, S. Billings, L.M. Brigham, S.S. Chacon, J. Fraterrigo, S.D. Frey, K. Georgiou, M. de Graaff, A.S. Grandy, M.D. Hartman, S.E. Hobbie, C. Johnson, J. Kaye, E. Snowman, M.E. Litvak, M.C. Mack, A. Malhotra, J.A.M. Moore, K. Nadelhoffer, C. Rasmussen, W.L. Silver, B.N. Sulman, X. Walker, and S. Weintraub. 2020. SOils DAta Harmonization database (SoDaH): an open-source synthesis of soil data from research networks ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/9733f6b6d2ffd12bf126dc36a763e0b4\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis is an EDI dataset from the SoDaH (Soil Data Harmonization) LTER working group. It provides a nice example of a harmonized data product that also includes provenance metadata. We’ll talk a little more about this later, but note that all the original data sources are linked to this dataset on the landing page.\n\nData provenance is clear and extensive, and the geographic coverage adds useful detail.\nThere are many, many variables available in the dataset, most having to do with soil data and where that data came from.\nThe methods provide adequate information about how the data were harmonized. Links to related journal articles should provide examples of how those data can be used.\nThe flattened database table format probably isn’t very space efficient, but should be fairly easy to use once you understand the columns.\n\n\n\n\n\n\nExample dataset: Woods, B., Trebilco, R., Walters, A., Hindell, M., Duhamel, G., Flores, H., Moteki, M., Pruvost, P., Reiss, C., Saunders, R., Sutton, C., & Van de Putte, A. (2021). Myctobase (1.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.6131579\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nMyctobase is a global database of mesopelagic fish data.\n\nLink to a descriptive data paper in Scientific Data is clear.\nThere are three tables and they described in the metadata, and metadata are provided in a separate Excel file.\nTaxonomic names have been standardized and checked.\nData provenance is not very clear. Did the data come from other published sources like those in the reference list, or are there contributed data too?\n\n\n\n\n\n\nExample dataset: Ross, C.W., L. Prihodko, J.Y. Anchang, S.S. Kumar, W. Ji, and N.P. Hanan. 2018. Global Hydrologic Soil Groups (HYSOGs250m) for Curve Number-Based Runoff Modeling. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1566 (2018)\n\n\n\n\n\n\nObservations from the instructors\n\n\n\n\n\nThis dataset seems like a reasonably good example of a published, global-scale spatial data synthesis product.\n\nThe R code is published with the data.\nProvenance of the source data is reasonably clear in the User Guide.\nThe ORNL DAAC repository provides extensive metadata and good tools to preview and access the data. It is login only.\n\n\n\n\n\n\n\n\n\nDesigning your dataset\n\nThe overall design of a dataset to be published is often difficult to imagine, especially for newcomers. One of the most common questions data managers hear is “What should we publish?” This is usually a question about what files to include in the published dataset, or what data will be useful as a published dataset. Every dataset is different, but the answer often depends where your data fall on the Raw → Ready → Results spectrum.\n\nRaw refers to “source data” that have not been altered very much from their original state at collection. They have undergone no, or minimal, QA/QC, filtering, reformatting, and other changes.\nReady data are derived from raw data and are made ready for analysis with cleaning, filtering, reformatting, summary statistics, joining with other data, and other changes.\nResults data are the product of a planned analysis. Often these are the data used to produce figures and statistics for a journal article or other research product.\n\nSynthesis groups typically rely on previously-published datasets as their raw, or source, data and therefore usually publish ready and results datasets. Activity 1 provided a few examples of well-designed (and not-so-well-designed) synthesis datasets. Lets review what we saw there.\n\nDiscussion questionData filesMetadataOther files\n\n\n\nWhat should be included in a published dataset?\n\n\n\n\nTo publish a harmonized synthesis dataset (ready data), follow the guidance in Module 2 about preparing the data so it is most useful to your team and others (use simple data structures, open file formats, community-accepted data standards, etc.).\nWhen publishing a dataset for an analysis of your team’s synthesis data (results data, such as for a journal article), include any derived data files that were used to generate figures, statistics, and other research results being reported on.\nIt is ok to include numerous data files in one dataset as long as they are related to each other, they are well-described, and their purpose is clear (see the points about metadata below).\n\n\n\nWe’ll talk about the details of metadata below, but you should always include descriptive metadata with your data files. There are many ways to do this and it may depend on the repository you will publish to.\n\nSome repositories accept standardized metadata files and some have form-based metadata editors that can be used to enter metadata during the submission of a dataet.\nIt is ok to also include separate metadata files in the form of text files, MS Word documents, PDF files, and other formats, with your published dataset if additional metadata is necessary.\nMany repositories don’t require much metadata, but it is a good idea to provide it anyways. Your colleagues and your future self will thank you.\n\n\n\nA few other things may be included in published datasets that don’t necessarily fall into the data or metadata categories.\n\nOften it makes sense to ensure the reproducibility of research results by publishing a workflow with the data. Usually this means publishing code files, such as scripts written in R, python, or a shell language, in the dataset with results.\nThe workflow for generating a harmonized synthesis dataset, especially detailed, reusable workflows like an R package, can often stand alone as an independent publication. We discuss that in a later section.\nSome repositories may include additional files with published datasets such as quality reports, manifests, and other contextual information about the dataset.\n\n\n\n\nThese are general guidelines, but for more advice on what to include in a published dataset you can also look to repositories like EDI and BCO-DMO, or a research network like NEON. Asking a data manager, especially one involved with your synthesis group’s work, can also be helpful, as will discussion among the full synthesis team.\n\n\nMetadata\nAnother thing that Activity 1 introduced is the importance of metadata. Metadata are data about the data. As a general rule, metadata should describe\n\nWho collected the data\nWhat was observed or measured\nWhen the data were collected\nWhere the data were collected\nHow the data were collected (methods, instruments, etc.)\nSometimes, stating why the data were collected can help future users understand data context and evaluate fitness for use.\n\nIncluding metadata of this nature makes data more usable, and helps prevent the deterioration of information about data over time, as illustrated in the figure below.\n\n\n\nExample of the normal degradation in information content associated with data and metadata over time (“information entropy”). Accidents or changes in technology (dashed line) may eliminate access to remaining raw data and metadata at any time (from Michener et al. 1997).\n\n\n\nData Provenance Metadata\n\nProvenance metadata deserves special attention for ecological data synthesis projects. Data provenance refers to information detailing the origin of the values in a dataset, which is important for synthesis projects that bring together data from many different sources. Synthesis activities typically produce new data products that are derived from the original source data after they have been cleaned, harmonized, and analyzed. Provenance metadata should be included with the derived products to point back to the original source data, similar to the way bibliographic references point to the source material for a book or scholarly article.\nA few other notes on provenance:\n\nDocumenting your data sources in a log file or spreadsheet as you collect and analyze them is a great way to start gathering the provenance metadata you need.\nMany data repositories provide guidelines, tools, and features for data provenance metadata2.\nProvenance metadata can become very detailed if the software and computing environment is also taken into account. This is an active area of study 3.\n\n\n\nLicensing\nPublished datasets should include a license in every copy of the metadata that defines who has what rights to use, reproduce, or distribute the data. Licensing decisions should be made in consultation with the synthesis team after considering the nature of the data (does it contain human subject data, for instance?), its origin (including restrictions on source data, if applicable), where the data will be published and used (different countries have different intellectual property and copyright laws), and the requirements of the funders and institutions associated with the project. There are quite a few guides available online that can help your group make a decision.4 For publicly-funded research data, it is generally appropriate to use open licenses, and the Creative Commons CC-BY attribution, and CC0 public domain, licenses are probably a good choice for most ecological synthesis data. This is not legal advice and your mileage may vary.\n\n\nMetadata Creation and Management\nAssembling metadata should be an integral part of the data synthesis activities discussed in Module 2, and can even be built into the workflows and project management practices the synthesis team uses. Make sure to plan for and start creating metadata early. Below are a few ways to do that.\n\nKeep a detailed project log and populate it with metadata for the project, including information like:\n\nwhat source data the team is using and where they came from\nhow source data are being harmonized for analysis\ndata analysis steps and methods used to create figures,statistics, and derived data products\nwho is doing what\n\nStart creating distinct publishable datasets (data plus metadata) as data are processed and analyzed. The team can do this:\n\nlocally, using a labeled directory for the cleaned, harmonized, and derived data, along with related code and metadata files. Metadata files may be plain text, or use a metadata template.\nwith a repository-based metadata editor, such as ezEML from the Environmental Data Initiative (EDI) repository.\n\nGet a professional data manager or data curator involved with the synthesis project. For example, the LTER Network has a community of “Information Managers” 5 trained in data management, metadata creation, and data publishing. Research data repositories and academic libraries often have professional data curators on staff who are well-trained and can advise researchers on community metadata standards and best practices for data publishing.6\n\n\n\n\n\n\n\nKey insight\n\n\n\nReproducibility and the creation of metadata are closely related. Your team’s detailed documentation of the research process allows for reproducible science, and can be mined as a source of metadata when you prepare data for publication.\n\n\n\n\n\nChoosing and Publishing to a Repository\nThere are a multitude of research data repositories available to researchers now7, which can make choosing where to publish data feel overwhelming. A few basic data repository features are essential when publishing a synthesis dataset. First, the repository should issue persistent, internet-resolveable, unique identifiers for every dataset published. Generally this will be a Digital Object Identifier, or DOI, that can be cited every time the dataset is used after publication. Second, repositories should require, and provide some means to publish, metadata describing each dataset. Without requiring at least minimal metadata, no repository can ensure that published data are FAIR. Finally, research data repositories should be stable and well supported so that data remain available and usable in perpetuity. Choosing a repository from the CoreTrustSeal certified repository list is one way to assess this. Beyond this, asking a few questions about the dataset will help with repository selection:\n\nWho are the likely users for this data? Will they belong to a specific scientific discipline, research network, or community of stakeholders?\nHow specialized are your data? Do they fall into a common data type or follow a special formatting standard?\nWill the data be updated regularly?\nDoes the repository charge for publication?\nWill the dataset benefit from some level of peer review?\n\n\n\n\nA small selection from the broad spectrum of research data repositories available for publishing synthesis data. These repositories are weighted towards those based in the U.S.A. (re3data.org has a comprehensive list). Also note that the FAIR gradient below refers primarily to repository requirements. It is possible, but not always required, to include detailed, community-standard metadata in generalist repositories.\n\n\nMore specialized repositories tend to offer enhanced documentation, custom software tools, and data curation staff that will review submitted data and assist users with data publication. Selecting a data repository with metadata requirements or standards, and a review and curation process for submissions, will help ensure that you are publishing a more FAIR data product. Consulting a project data manager if one is available to the synthesis team also helps with repository selection. After making a choice, the process of publishing data varies from repository to repository.\n\n\nAdditional Data Publishing Resources\n\nNEON’s derived data publishing guide\nEDI repository data authorship guide\nBCO-DMO repository data publishing guide"
  },
  {
    "objectID": "module3.html#sharing-the-teams-workflow",
    "href": "module3.html#sharing-the-teams-workflow",
    "title": "Tying It All Together",
    "section": "Sharing the Team’s Workflow",
    "text": "Sharing the Team’s Workflow\nOne of the most valuable, shareable outputs of synthesis research is the analytical workflow used to derive datasets and produce scientific results. Most often, these workflows are written in computer code, such as R, Python, or another language. The code may consist of a collection of scripts, or they may be organized into stand-alone modules or libraries. The latter is easier to share and re-use, but requires more advanced knowledge of software design. Code can be published in a repository (see the options below) with a DOI issued for particular versions of the code, which allows the workflow and code to be cited by the research products that they were used to generate. Ecological forecasting projects are one good place to look for example workflows (e.g. White et al. 2019), but the practice of publishing workflows is generally applicable to synthesis research.\nSharing and citing workflows and code are an essential element of reproducible science because doing so:\n\ndescribes the exact process used to prepare and analyze data.\nprovides a reproducible method to integrate or analyze new data.\nallows other scientists to verify results.\n\nEven for people who will not directly use the code, a published workflow provides information about:\n\nthe origin of the data.\nmethods for data cleaning, harmonization, analysis, and presentation of results (figures).\nhow the workflow was developed or changed over time.\nthe contributions made by the team.\n\n\nDiscussion questionSome ideas\n\n\n\nWhat features of published code would let you assess whether it is useful for your purposes?\n\n\n\n\nClear documentation and examples provided\nCommenting in the code\nTests and build indicators\nPublication in a repository that provides review (more on this below)\n\n\n\n\nIn other parts of the course, we have strongly recommended using version control and collaboration platforms, particulary GitHub. GitHub’s platform provides several options for sharing & publishing code, but lets explore some others too.\n\nGitHubNEON Code HubROpenSciPyPICRAN\n\n\n\nGitHub is huge and widely used for sharing code (among many other services). In combination with other software and services, GitHub can be reliably used to publish scientific code in a reproducible way.\nSome features:\n\nZenodo integration is already included in GitHub,8 which makes it fairly easy to publish a repository with a DOI.\nLarge array of features for managing contributions and sharing of the code\n\n\n\n\nThe NEON Code Hub is a good example of a research network focused code repository.\nSome features:\n\nFocus is on code useful for working with NEON data.\nReview and placement of submitted code.\n\n\n\n\nROpenSci publishes R packages for scientific applications.\nSome features:\n\nWide array of R packages useful for working with scientific data.\nTeam provides review and vetting of the code before publication.\nMost packages also go to CRAN.\n\n\n\n\nThe Python Package Index (PyPI) is the most widely used venue for publishing Python packages.\nSome features:\n\nPython compatibility checks are performed and metadata about the code resource are required.\n\n\n\n\nThe Comprehensive R Archive Network (CRAN) is a widely used resource for publishing R packages.\nSome features:\n\nR compatibility checks are performed and metadata about the code resource are required.\n\n\n\n\n\n\n\n\n\n\nKey insight\n\n\n\nPeer review is valuable for all research outputs. We expect a peer review process for journal articles, but published datasets and code can undergo peer review as well. As with manuscripts, the review process for data and code leads to higher quality, more useful products."
  },
  {
    "objectID": "module3.html#communicating-research-results",
    "href": "module3.html#communicating-research-results",
    "title": "Tying It All Together",
    "section": "Communicating Research Results",
    "text": "Communicating Research Results\nOne of the primary goals of synthesis research is to find useful, generalizable research results about the system under study. Most often this means writing scientific journal articles. While we aren’t going to go into full detail about what constitutes, or how to write, a manuscript for a journal, there are some unique features of writing articles for synthesis projects. First, data papers are often an important product for synthesis groups, and these are somewhat different than standard research journal articles. Second, given, the large size and cooperative nature of most synthesis teams, a collaborative writing process is called for. An appropriate collaborative writing method, and some team norms and contribution guidelines, should be in place to reduce the potential for conflict or mistakes.\n\nData papers\nA data descriptor article, usually known as a data paper, is a peer-reviewed journal article written to introduce and describe a (usually) new dataset. For synthesis teams, who are often producing a harmonized dataset as their first major research product, writing a data paper to accompany the dataset makes sense as a way to introduce the data, demonstrate their utility, and get the word out about the dataset. Data papers also lay the groundwork for any future papers that will answer the science questions of interest to the synthesis team.\nData papers may be simpler and shorter than research articles (not always though), but there are still a few gotchas that can arise. Below are some recommendations, and the rationale behind them.\n\nPublish the dataset described by the data paper in a reputable data repository.\n\nAlthough some publishers host the data described in a data paper themselves, this is often as supplementary material to the article, and sometimes the data are only held for review. Most data-focused journals require that accepted data papers should describe and reference a dataset published in a research data repository. Follow the guidance above to select a repository and prepare the dataset for publication.\n\nBe sure to cite the data paper and the dataset properly.\n\nThe existence of a data paper and a dataset, each describing the same data and each with its own DOI, can create confusion about what to cite in related works. If the novelty and utility of the dataset, or the methods used to assemble it, are being referenced by a related work, then it may be most appropriate to cite the data paper. If the actual data are being used (analyzed, interpreted, etc.) in a related work, then definitely cite the published dataset. In many cases it is expected to cite both.\n\nDon’t shortchange the metadata in the published dataset just because there is also a data paper.\n\nConsider the future usability of the data the data paper describes, and ensure that the associated published dataset contains detailed, community-standard metadata. Not all users will find or be able to access the data paper, and data paper publishers may have incomplete or quirky requirements for metadata.\n\n\n\n\n\n\n\n\nData paper examples and publication venues\n\n\n\n\n\nSome examples of data papers related to synthesis projects:\n\nKomatsu, Kimberly J., et al. “CoRRE Trait Data: A dataset of 17 categorical and continuous traits for 4079 grassland species worldwide.” Scientific Data 11.1 (2024): 795. https://doi.org/10.1038/s41597-024-03637-x\nWieder, William R., Derek Pierson, Stevan Earl, Kate Lajtha, Sara Baer, Ford Ballantyne, Asmeret Asefaw Berhe et al. “SoDaH: the SOils DAta Harmonization database, an open-source synthesis of soil data from research networks, version 1.0.” Earth System Science Data Discussions 2020 (2020): 1-19. https://doi.org/10.5194/essd-13-1843-2021\nZhang, Liang, Edom Moges, James W. Kirchner, Elizabeth Coda, Tianchi Liu, Adam S. Wymore, Zexuan Xu, and Laurel G. Larsen. “CHOSEN: A synthesis of hydrometeorological data from intensively monitored catchments and comparative analysis of hydrologic extremes.” Hydrological Processes 35, no. 11 (2021): e14429. https://doi.org/10.1002/hyp.14429\n\nA few suggested venues for publishing data papers:\n\nScientific Data (Nature Publishing Group)\nData (MDPI)\nPLOS ONE (usually termed “database papers”)\nThe ESA journal Ecology, and quite a few other disciplinary journals, now publish data papers.\n\nGBIF also maintains a helpful list of data paper journals.\n\n\n\n\n\nWriting collaboratively\nWriting a paper with a large team can be a challenge. It is important to encourage team members to contribute in a way they are comfortable with, but there is the potential for technical, editorial, and personal conflict without some prior planning. Practically, there are two models for writing a manuscript with a bunch of contributors.\n\nCloud-based collaborative writing“Pass the manuscript”\n\n\nIn this model manuscripts live mainly in web-based writing platforms managed by a cloud service provider (e.g. Google Docs) and all contributors write and edit the document within that platform. Contributions may be asynchronous or synchronous since version control and conflict resolution is generally built into the platform. Most platforms have additional collaboration features, such as user account management, suggested edits, and commenting systems.\nSoftware platform: Google Docs, Microsoft 365 Online, Authorea, Overleaf (LaTeX)\nPros: Strong collaboration features (user/permission management, contribution tracking, comments and suggestions). No need to distribute copies and then merge contributions.\nCons: Can be unfamiliar to senior contributors. Easy to lose track of links. Limited formatting features compared to local word processors. Privacy/tracking concerns.\n\n\nThis model relies on word processing software installed on contributors’ local machines. Copies of the manuscript are distributed to contributors for asynchronous writing and editing assignments, and contributions are then merged together into a synchronized version of the manuscript. In large teams, it may be best to have one person managing the copy/merge process.\nSoftware platform: Microsoft Word (usually), email\nPros: Familiar to most. Integrates with local data management practices. Most word processors have powerful collaboration and versioning features now. Advanced formatting and editing. Less reliance on cloud providers.\nCons: License pricing and institutional availability may be limited. Multiple versions in use, and the copy/merge workflow can easily generate conflicts or become unmanageable in large groups.\n\n\n\nIn addition to these practical considerations, there are some team considerations as well\n\nMake the expectations for contributing to a manuscript clear.\n\nHow, when, and where should contributions be made\nAuthorship expectations discussed in advance\n\nMake space for new, or early-career team members to contribute.\n\nEfficiency and experience level aren’t good reasons to exclude contributors\nSynthesis papers are a great learning experience and career opportunity\n\nTeam discussions are preferable to unilateral editorial decisions.\n\nThis can help avoid hurt feelings during the editing process.\n\nIt can be beneficial to have a manuscript coordinator.\n\nThe coordinator can help split up writing and editing tasks equitably\nSomeone needs to manage conflicts, check for consistency, make some executive decisions, etc.\nOften this is the lead author"
  },
  {
    "objectID": "module3.html#connecting-the-pieces",
    "href": "module3.html#connecting-the-pieces",
    "title": "Tying It All Together",
    "section": "Connecting the Pieces",
    "text": "Connecting the Pieces\nWe’ve now covered how a synthesis team should approach creating and publishing its main research outputs (data, code, research results). Now we’ll discuss how to begin making these useful to the world, which starts with making sure the products of synthesis research point to each other. Lets begin with an activity.\n\nActivity 2: Synthesis project detective\nForm breakout groups and course instructors will assign each one a link to a product from a synthesis project (the code, a paper, a dataset, etc.). Using any means necessary (metadata, web search, etc.) figure out what other products are part of the same project (related publications, source/derived data, etc.) and who is involved in the synthesis team. Answer these questions as a group:\n\nIf your group received a link to a paper, were you able to find datasets and a code repository (for an analytical workflow)?\nIf your group received a link to a code repository, were you able to find papers and datasets?\nIf your group received a link to a dataset, were you able to find papers and a code repository?\nWho was involved in the synthesis project?\nCould you understand the overall scope and impact of the synthesis project? Why or why not?\n\n\nGroup 1Group 2Group 3Group 4Group 5\n\n\nClue: https://cran.r-project.org/web/packages/codyn/index.html\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the “community dynamics” synthesis working group that was at least partly supported by the LTER Network.\nPapers\nThere are two papers describing the R package and the analysis methods it encodes:\n\nHallett, L.M., Jones, S.K., MacDonald, A.A.M., Jones, M.B., Flynn, D.F.B., Ripplinger, J., Slaughter, P., Gries, C. and Collins, S.L. (2016), codyn: An r package of community dynamics metrics. Methods Ecol Evol, 7: 1146-1151. https://doi.org/10.1111/2041-210X.12569\nAvolio, M. L., I. T. Carroll, S. L. Collins, G. R. Houseman, L. M. Hallett, F. Isbell, S. E. Koerner, K. J. Komatsu, M. D. Smith, and K. R. Wilcox. 2019. A comprehensive approach to analyzing community dynamics using rank abundance curves. Ecosphere 10(10):e02881. https://doi.org/10.1002/ecs2.2881\n\nWorkflows\nThe original clue is a link to the codyn package on CRAN.\nDatasets\n\nData are included in the R package… but not sure of the provenance.\n\nOther\n\n?\n\n\n\n\n\n\nClue: https://corredata.weebly.com/\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the CoRRE synthesis working group, which has been supported by iDiv and LTER (and possibly others). The group’s website lays out many of the products fairly clearly, though it may not be perfectly up-to-date.\nPapers\n\nAvolio, M. L., Pierre, K. J. L., Houseman, G. R., Koerner, S. E., Grman, E., Isbell, F., … & Wilcox, K. R. (2015). A framework for quantifying the magnitude and variability of community responses to global change drivers. Ecosphere, 6(12), 1-14.\nWilcox, K. R., Tredennick, A. T., Koerner, S. E., Grman, E., Hallett, L. M., Avolio, M. L., … & Zhang, Y. (2017). Asynchrony among local communities stabilises ecosystem function of metacommunities. Ecology letters, 20(12), 1534-1545.\nLangley, J. A., Chapman, S. K., La Pierre, K. J., Avolio, M., Bowman, W. D., Johnson, D. S., … & Tilman, D. (2018). Ambient changes exceed treatment effects on plant species abundance in global change experiments. Global Change Biology, 24(12), 5668-5679.\nKomatsu, K. J., Avolio, M. L., Lemoine, N. P., Isbell, F., Grman, E., Houseman, G. R., … & Zhang, Y. (2019). Global change effects on plant communities are magnified by time and the number of global change factors imposed. Proceedings of the National Academy of Sciences, 116(36), 17867-17873. https://doi.org/10.1073/pnas.1819027116\nand quite a few more….\n\nA recent data paper\n\nKomatsu, K.J., Avolio, M.L., Padullés Cubino, J. et al. CoRRE Trait Data: A dataset of 17 categorical and continuous traits for 4079 grassland species worldwide. Sci Data 11, 795 (2024). https://doi.org/10.1038/s41597-024-03637-x\n\nWorkflows\n\nNothing public found so far… There may be scripts provided as supplementary files with some publications.\n\nDatasets\n\nOther than the data paper mentioned above and some associated data in EDI, the primary data appear to be available by request only.\n\nOther\n\n?\n\n\n\n\n\n\nClue: https://doi.org/10.1029/2022GB007678\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is a paper from the LTER-supported Silica exports working group. We talked in Module 2 about their repositories and project management practices.\nPapers\nThe original clue is a paper from this group. Note the data availability statement links to a dataset published with the USGS, and the acknowledgements give some clue as to the LTER support recieved.\nWorkflows\nThere are several GitHub repositories. The first one listed is a guide to others.\n\nhttps://github.com/lter/lterwg-silica-data\nhttps://github.com/SwampThingPaul/SiSyn\nhttps://github.com/lsethna/NCEAS_SiSyn_CQ\nhttps://github.com/lter/lterwg-silica-spatial\nhttps://github.com/njlyon0/lter_silica-high-latitude\n\nDatasets\n\nJankowski, K.J., Carey, J.C., Julian, P., Johnson, K., Sethna, L.R., Thomas, P.K., Wymore, A.S., Shogren, A.J., McKnight, D.M., McDowell, W.H., Heindel, R.C., Sullivan, P.L., and Jones, J. B., 2023, Dissolved silicon concentration and yield estimates from streams and rivers in North America and Antarctica,1964-2021: U.S. Geological Survey data release, https://doi.org/10.5066/P951UKQB\n\nOther\n\n?\n\n\n\n\n\n\nClue: https://doi.org/10.3389/fmars.2021.724913\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is a paper describing a new effort to create a harmonized, global ocean oxygen product, called GO2DAT. The paper was published in 2021, and there is currently not much other information about progress on the effort. The paper is best understood as a community-accepted roadmap for this global ocean data synthesis project.\nPapers\nThe original clue is the primary paper describing this effort.\nTwo years later, this paper assessed progress on GO2DAT among other global ocean biogeochemistry synthesis projects and described it as still being in the “Documentation phase.”\nWorkflows\n\n???\n\nDatasets\n\n???\n\nOther\n\n???\n\n\n\n\n\n\nClue: https://github.com/sokole/ltermetacommunities\n\n\n\n\n\n\nCracking the case\n\n\n\n\n\nThis is the LTER Synthesis group called “Metacommunities”\nPapers\n\nWisnoski, Nathan I., Riley Andrade, Max C. N. Castorani, Christopher P. Catano, Aldo Compagnoni, Thomas Lamy, Nina K. Lany, et al. 2023. “ Diversity–Stability Relationships across Organism Groups and Ecosystem Types Become Decoupled across Spatial Scales.” Ecology 104(9): e4136. https://doi.org/10.1002/ecy.4136\n\nWorkflows\n\nThe original “clue” is a link to GitHub where code is stored.\nThis is one of the LTER-supported synthesis groups that led to the creation of the ecocomDP data model and R package (O’Brien et al. 2021)\n\nDatasets\nThere is a dataset in Zenodo\n\nNathan I. Wisnoski. (2023). nwisnoski/dsr-metacom: Code for Ecology publication (v1.0.4). Zenodo. https://doi.org/10.5281/zenodo.8067504\n\nOther\n\nEric may know more!\n\n\n\n\n\n\n\n\n\nMore ways to synthesize\n\nWe’ve talked about the three most common products of synthesis: papers, datasets, and workflows. But, we’ve also seen that there are plenty of other ways to share synthesis research! Education and outreach can become an important goal in for some synthesis teams, and providing access to data and actionable research results, such as forecasts, can be very useful to stakeholders. As time goes on, synthesis teams may produce many things that meet these goals and needs, moving well beyond the three kinds of products we’ve already talked about. See below for a few ideas and examples.\n\nTeaching materialsWeb appsAutomationProject websites\n\n\nSynthesis research produces new scientific knowledge that other researchers, students, or stakeholders can learn and build on. Synthesis can also generate applied-science tools and methods that others need to learn how to use for themselves. Teaching modules are an important way of sharing both of these outcomes, and of broadening the reach of a synthesis project.\nExamples:\n\nThe EDDIE project is a clearinghouse of contributed teaching materials for the earth and environmental sciences.\nThis website is an example of teaching materials produced by a synthesis team.\n\n\n\nInteractive web applications can provide users with easy access to scientific datasets, especially large ones, analytical results, visualizations, interpretation, and many other things. Creating web apps is not necessarily an easy task, but if your synthesis team has the expertise, or access to web developers, web apps may be useful for outreach, or as tools the synthesis team itself can use. Frameworks like Shiny (for R), Streamlit, or Flask (both for python), and services like Shinyapps.io and Plotly, can make creation of apps relatively painless.\nExamples\n\nAn app for finding and exploring ecocomDP data in the NEON and EDI repositories.\nA dashboard app for the NEON ecological forecasting challenge.\nThe Jornada LTER interactive viewer for weather station data.\n\n\n\nSome research efforts have developed automation systems for research data processing, analytics, and publishing. These often fall into the “continuous integration/continuous deployment” class of web-enabled software and data pipelines, in which one software processes (data processing, analytics, publication, etc.) may be automatically triggered by events that occur in another, connected software service (such as adding new data to a GitHub repository). These technologies enable researchers to build software pipelines that can be useful for quality control of new data, updating forecasts, and rapid deployment of data or analysis products.\nExamples:\n\nThe Portal Project in southeast Arizona has developed a well-described near-term ecological forecasting pipeline (also described in Kim et al. 2022).\nAutomated quality control of dendrometer band data.[^16]\nForecasting Lake and Reservoir Ecosystems (FLARE) project.\n\n\n\nAt a certain point, the outputs of a synthesis project can become numerous and challenging to present to the public in an organized way. Project websites can serve as a gateway to an entire synthesis project by providing comprehensive listings of project outputs (papers, datasets, GitHub repositories, etc), participants and data contributors, a narrative for the research, appealing images or graphics for outreach, and links to related projects, funders, or institutions. Websites built with GitHub Pages are a common solution for creating simple, cost-effective (free, usually) project websites nowadays, but there are other options. A good project website can become a cohesive, engaging clearinghouse for information about a synthesis project, but they can be laborious to create and keep up-to-date.\nExamples:\n\nThe Portal Project\nThe SoDaH project.\nThe CoRRE project\n\n\n\n\n\n\nLinking synthesis products together\nReflecting on all the information above, we can see one common feature of the many different products of a synthesis team: they exist primarily as digital objects on the internet. The internet may seem fluid and ephemeral, but fortunately there are ways to identify and connect these digital objects in a stable way.\n\nPersistent identifiers\nPersistent identifiers, or PIDs, are references to digital objects or identities that are intended to last a long time. To be useful on the internet, PIDs should be unique, i.e. having a 1:1 relationship between the PID and the digital object, and machine actionable, meaning they can be understood by software like web browsers. There are many different types of PIDs, but the most useful ones in the context of publishing research products are:\n\nDigital Object Identifiers (DOI), used to identify digital publications like journal articles, datasets, or governement reports.\nOpen Researcher and Contributor ID (ORCID), used to identify individuals, usually in the context of research or publishing activities.\nResearch Organization Registry (ROR), used to identify organizations, also in the context of research and publishing, primarily.\n\nThese identifiers can and should be associated with all journal articles and published datasets resulting from synthesis projects. DOIs and ORCIDs can easily be associated with GitHub and other code repositories as well.\n\n\nCiting synthesis products\nThe best way to ensure that use of a research product is recognized is through proper citation. This is already common practice for journal articles, but is only recently being adopted for published datasets. The most logical place in an article to cite a published dataset is in the Methods section and in the Data Availability Statement, which most reputable journals now require. Be sure to check journal data sharing requirements and begin preparing for data publication well in advance of article submission. When citing datasets, be sure that the full bibliographic entry is correctly included in the article’s reference list. Citation of code is not as widely practiced, but some journals require it and it is a best practice.\n\nA useful data availability statementNot as helpful\n\n\nIn the Data Availability Statement from Currier and Sala (2022), note that source datasets are properly cited in the Data Availability Statement, meaning an in-text citation is given and the full bibliographic entry is provided in the article reference list (not shown). The DOIs included here are helpful for quickly finding the data.\n\nAll original and derived phenology data produced by the authors, and R scripts for data processing, statistical analyses, and figure production are publicly available in the Environmental Data Initiative (EDI) repository. EDI package knb-lter-jrn.210574001.2 (Currier & Sala, 2022a) contains daily phenocam image data, derived timeseries and associated scripts for processing and is available at https://doi.org/10.6073/pasta/836360dce9311130383c9672e836d640. EDI package knb-lter-jrn.210574002.2 (Currier & Sala, 2022b) contains observed phenological indicators and environmental drivers as well as associated scripts for final analyses and figure construction presented in this manuscript and these data are available at https://doi.org/10.6073/pasta/d327a77f6474131db8aa589011e29c29. No novel code was generated by the authors of this manuscript. The precipitation data used in all analyses are derived from G-BASN data in EDI package knb-lter-jrn.210520001 (Yao et al., 2020) available at https://doi.org/10.6073/pasta/cf3c45e5480551453f1f9041d664a28f. Daily air temperature summaries from 4 June 1914 to the present for the Jornada Experimental Range Headquarters (NOAA station GHCND:USC00294426) are freely available upon request via the National Ocean and Atmospheric Administration (https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USC00294426/detail).\n\n\n\nAlthough conventions are changing, it is still common to find data availability statements that introduce barriers to data access or reproducibility. Often they are worded along these lines:\n\nData used in the figures are included in the supplementary material. The full dataset will be provided upon reasonable request to the corresponding author."
  },
  {
    "objectID": "module3.html#maintaining-momentum",
    "href": "module3.html#maintaining-momentum",
    "title": "Tying It All Together",
    "section": "Maintaining Momentum",
    "text": "Maintaining Momentum\nAs we discussed in Module 1, starting a synthesis project benefits from motivating scientific questions, a well-planned foundation for team science, and significant activation energy from the team. When successful, synthesis projects gather enough momentum to be productive for many years. Below are a few ideas on how to maintain this momentum.\n\nGive everyone credit\nEveryone deserves credit for the work they do, and in academic environments this is too often overlooked. Synthesis working groups commonly begin without any dedicated personnel support, which means that some participants, usually early-career scientists, will be contributing unpaid time to the project. In the absence of pay, leaders of a synthesis team should take the initiative to make sure everyone receives appropriate credit and opportunities for career advancement when they contribute to the project. Below are a few thoughts on how to do that.\n\nDo’sDon’tsDiscuss\n\n\n\nDiscuss and define in advance some of the contributions team members will make.\n\nThis is particularly important for deciding authorship of journal articles.\nThe CRediT framework is a good starting point.\nMore detail on this is in Module 1.\n\nBe willing to credit participants for a wide variety of contributions.\n\nThis includes writing code, cleaning data, taking meeting notes, and more.\n\nMake sure all contributors have an ORCID. They are easy to obtain and widely used.\nUse ORCIDs to associate contributors with a research product whenever possible.\nList contributors on websites, GitHub repositories, and other public-facing team materials.\n\nIts nice to include affiliations, bios, links to profile pages, and other information too.\n\n\n\n\n\nDon’t rely on any one metric for valuing contributions to the team.\n\nCode commits in GitHub, for example, may reflect the input of many people besides the one that actually wrote and committed the code.\n\nDon’t forget students, technicians, early-career scientists, and others.\nDon’t forget to put your name on your work!\n\n\n\n\nWhat are we missing here?\n\n\n\n\n\n\nEncourage new contributions\nInterests and commitment to synthesis projects change over time. To sustain active research contributions by the team, and continued use of the data, make sure new people can find a way to participate.\n\nProvide a path for new data contributions.\n\nConsider adding a “How to contribute” section to a project website or GitHub repository README.\nMaking your synthesis project’s data preparation/harmonization workflow reproducible and well-documented will make it easy for new data contributors.\n\nHave open meetings when possible.\n\nThis helps bring in new team members that are interested and willing to contribute.\n\nGive all team members the freedom and support to lead analyses, papers, and other valuable project activities.\n\n\n\nFind support\n\nMaintaining momentum for a synthesis project over the long term is highly dependent on the ability to keep scientists engaged and find support for dedicated personnel time. Usually this means getting monetary support in the form of grants.\n\nExplore and apply to the funding sources presented in Module 1.\n\nPersonnel support may need to come from larger grants since working group funding often provides only meeting support.\n\nThink creatively about how to get students and postdocs participating in synthesis projects.\n\nIf student/postdoc research interests & plans overlap, dedicating some time to synthesis group work can lead to career-building opportunities (networking, high-impact papers).\n\nPromote the synthesis team’s work!\n\nIt is difficult to attract interest from new participants and new resources for a project without doing this.\n\n\n\n\nHAVE FUN!\nWhen done correctly, ecological synthesis research means having lots of fun doing science with a great team."
  },
  {
    "objectID": "module3.html#footnotes",
    "href": "module3.html#footnotes",
    "title": "Tying It All Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe GO FAIR Initiative’s “How to GO FAIR” and the FAIR Cookbook are two examples of implementation guides.↩︎\nThe EDI repository’s provenance metadata documentation is an example applicable to publishing with the EML metadata standard. Other metadata standards and repositories have different systems.↩︎\nSee Lerner et al. (2023) or resources at the End-to-End Provenance project.↩︎\nBoth the Creative Commons (CC) and the Open Knowledge Foundation (OKF) have extensive documentation about licensing data in different contexts, including the CC data FAQ and OKF’s Open data definition. The CC license chooser is a quick and easy way make a licensing choice once you understand your options.↩︎\nHere is a list of LTER Information Managers↩︎\nThe Data Curation Network is a professional membership organization for data curators.↩︎\nThe Registry of Research Data Repositories (re3data.org) project is a comprehensive tracker of information about research data repositories.↩︎\nThe process of publishing a GitHub repository release to Zenodo is covered in the GitHub and Zenodo documentation, and other places.↩︎"
  }
]