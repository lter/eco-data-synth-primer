[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "The course is currently being organized by Greg Maurer (gmaurer@nmsu.edu), with generous assistance from Marty Downs and Nick Lyon. See the Contributors section for more information."
  },
  {
    "objectID": "people.html#contributors",
    "href": "people.html#contributors",
    "title": "People",
    "section": "Contributors",
    "text": "Contributors\nWe have a long list of synthesis scientists actively involved in the course working group who are contributing expertise and instructional content, and many will be present as instructors at the 2024 ESA Meeting. Names and web profiles are listed below.\n\nKathryn Barry –  Website –  Publications\nJoanna Carey –  Website –  Publications\nAngel Chen (she/her) –  Website –  GitHub –  ORCID\nLaura Dee –  Website –  –  GitHub Publications\nMarty Downs –  Website –  Publications\nStevan Earl –  Website –  GitHub –  Publications\nSarah Elmendorf –  Website –  GitHub –  ORCID\nJalene LaMontagne –  Website –  Publications –  ORCID\nNick J Lyon (they/them) –  Website –  GitHub –  ORCID\nGregory Maurer (he/him) –  Website –  GitHub –  ORCID\nColin Smith –  Website –  Publications\nEric Sokol –  Website –  Publications\nAlexandra (Sasha) Wright –  Website –  Publications"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "People",
    "section": "Other contributors",
    "text": "Other contributors\nIn addition to those above, several people contributed to early iterations on the concept and content of the course.\n\nForest Isbell –  Website –  Publications\nKim Komatsu –  Website –  Publications"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A course for collaborative ecologists",
    "section": "",
    "text": "This course and the website are under construction. Please check back soon!\nIn recent decades, ecology has become a more collaborative discipline motivated by the search for generality across ecosystems. At the same time, the availability, quantity, and quality of environmental data have grown rapidly, creating opportunities for re-use of these data in ecological synthesis research. Though synthesis research is complex and demanding, taking an inclusive and collaborative approach to both the scientific process and the data pays dividends throughout the lifetime of a project. This short course is a survey of methods for making ecological synthesis research a ”team sport”. Objectives for learners are to (a) develop an end-to-end (conception to publication) plan for collaborative synthesis research, and (b) gain data synthesis skills that are immediately useful in a research team setting. Instructors will cover assembling the team, study design, communication, collecting primary data sources, assembly/harmonization of data, analytical workflows, and publication of derived datasets. The course uses real-world examples, demonstrations, and interactive lessons. Ecologists with synthesis experience will be on hand with seasoned research advice and data tips. Many workshop activities will be oriented toward helping learners develop their own ideas and plans for ecological data synthesis."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "A course for collaborative ecologists",
    "section": "Agenda",
    "text": "Agenda\nThis agenda is subject to change as the course is developed.\n\nModule 1: Starting with Team Science\nModule 2: Operational Synthesis\nModule 3: Tying It All Together"
  },
  {
    "objectID": "index.html#photo-credits",
    "href": "index.html#photo-credits",
    "title": "A course for collaborative ecologists",
    "section": "Photo credits:",
    "text": "Photo credits:\nIn the collage above: Jacob Bøtter via Flickr, CC BY-SA 2.0 | Jeremy Yoder via Flickr, CC BY-SA 2.0 | Marco Pfeiffer, CC BY-SA 4.0 | Gabriel De La Rosa, CC BY-SA 4.0 | Weecology lab CC BY 4.0 | NEON (National Ecological Observatory Network)"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "contributing.html#overview",
    "href": "contributing.html#overview",
    "title": "Editing the Website",
    "section": "",
    "text": "This page houses the beginnings of our ‘how to edit the website’ instructions. Note that this page is primarily for internal use so it assumes you already are a member of the group and have the necessary access to the source GitHub repository. There are a few viable options for contributing to this website. Choose the method that aligns best with your skills and/or the one that sparks the most joy for you.\n\n\nGitHub makes a lot of the technical aspects of collaborating much simpler but there is one thing to watch out for: conflicts. Essentially, GitHub is not Google Docs. This means that while we can edit this website collaboratively we cannot do so simultaneously. When multiple people edit the same file in different ways, GitHub doesn’t know which set of changes should take priority (and it is often the case that we want both sets of edits!) and so it returns a conflict message that essentially says ‘you figure it out human’.\nFortunately the way to avoid conflicts is straightforward: communicate with one another! Before making any edits to the files in this website, reach out to the rest of the group and let them know! This can be as simple as “hey, I’ll be working on topic X from 10-11 ET so please don’t edit anything in the meantime.” For those of you working in GitHub directly you don’t really need to worry too much about this because your edits will always take priority but it’s good to think about so you don’t set the stage for others to encounter conflicts.\n\n\n\nWe’ve done some fancy behind-the-scenes wrangling with GitHub to support this method (see here if that is of interest) but the key take-away is that edits made directly in GitHub should “just work.” There is however a small caveat to this option: you can only do this if the page you’re editing has no “code chunks”. The processing of code chunks is handled slightly differently from ‘normal’ text and requires a local computer. Even if your changes don’t affect code chunks directly it only matters whether or not there are code chunks in the page at all. Hopefully that’s not too limiting of a caveat!\nHere’s what you’ll need to do beforehand:\n\nCreate a GitHub account\nMake sure you have edit access to the GitHub repository where the materials live\n\nOnce you’ve done that you can follow these steps to directly affect the website via GitHub! To start, go to the course’s GitHub repository and click the file for the page you want to edit. You can deduce that because the URL of that page will exactly match the file name. For example, let’s imagine that we want to edit the contributing page lter.github.io/eco-data-synth-primer/contributing.html. The file that controls this page is contributing.qmd which you can be sure of because it shares everything but the file extension with the link.\n\n\n\nOnce you’re there, click the name of the file you want to edit (for the demonstration purposes we’ll click contributing.qmd). On the resulting page, you’ll want to click the small pencil icon just above and to the right of the website content (see beneath the word “History” in the screenshot below).\n\n\n\nNow you can make any edits you’d like! Don’t worry about writing one super long line because when the website “renders” (i.e., builds itself) the text will be automatically wrapped to the width of the screen. In the screenshot below I’ve added “Here I can edit the website file directly through GitHub” on line 20.\n\n\n\nOnce you’re happy with your edits and want them to be sent to the “real” website, you’ll need to “commit” those changes. Click the bright blue–or green depending on your GitHub settings–button in the top right. In the small pop-up window that results, type a short description of what purpose your edits serve. This message is important because these comments will create the timeline that we can use to see or return to previous versions of the course materials.\n\n\n\nNow you’ll find yourself back in the ‘viewing’ part of GitHub and you should see your edits in the file. However, your edits are not yet in the live website. The site must first render your changes. This rendering effort is essentially the computer parsing your edits to a Quarto document (i.e., a .qmd file) into the HTML format needed for the website.\nYou can check the status of this rendering by clicking the “Actions” tab of the repository. You’ll see an orange–or red, again depending on your GitHub settings–circle with a progress wheel. After a few minutes that should turn into a blue–or green–check mark. You may notice that the name of the Action exactly matches the text you put in the top field of the commit message pop-up window earlier.\n\n\n\nOnce that Action finishes you’ll see a much faster one called “pages build and deployment” show up and go through the same process. This action is tying together the new website (all the unchanged files plus whatever files were changed by your edits) and updating the living website with that content.\n\n\n\nOnce both of those Actions have finished you should be able to visit the website link and see your edits in the live version! Note that you may need to refresh your browser page to update your cache so if you don’t immediately see your edits you may want to wait a moment and refresh the page.\n\n\n\n\n\n\nIf you’re more confident with a Git-based workflow, you can use RStudio (or any comparable platform) to make edits to the website.\nHere’s what you’ll need to do beforehand:\n\nDo the five setup steps in this GitHub workshop\nInstall Quarto on your computer\nMake sure you have edit access to the GitHub repository where the materials live\nClone that repository to your computer\n\nEditing a Quarto website is fairly straightforward once you’ve handled the prepartory steps outlined above. You’ll begin by editing your chosen files and–periodically–committing those changes.\nThe .gitignore should be set up in such a way that you cannot commit files you “shouldn’t” be committing but the rule of thumb is that you should feel free to commit either (A) changes to files ending in .qmd or (B) anything in the _freeze/ directory. The _freeze/ directory is how Quarto stores files that need some amount of computing so that it doesn’t waste computing resources re-rendering those files all the time and instead only re-renders when needed. Note that only files with one or more code chunks will ever put anything in that folder.\nOur GitHub Actions (see the ‘Method 1’ tutorial) are set up in such a way that once you push your commits the website should re-build itself automatically. Just wait for the GitHub Actions to complete and/or refresh your browser after pushing and you should see your edits reflected in the website.\nWhile editing you may find the  quarto preview command line call helpful as it will create a new tab in your browser that will dynamically show any edits you make. This is really nice particularly for writing or debugging a code chunk as you’ll be able to see the outputs in real time without waiting for the site to render only to find out your code doesn’t work as intended.\n\n\n\nIf you’d prefer, you can write the text that you’d like to include in the text editing software (e.g, Microsoft Word, Google Docs, etc.) of your choice. Once you’ve written it there, send it to someone else on the team who can use one of the preceding methods to get that content integrated. Ideally, hosting through GitHub (and creating this document) empowers you to make edits on your own behalf but if that feels out of reach in this moment we absolutely still want your insight so please don’t hesitate to go this direction if necessary."
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Module 3: Tying it all together",
    "section": "",
    "text": "Identify the features that make published data easily re-useable\nEvaluate the metadata and fitness-for-use of published synthesis datasets\nUnderstand the three primary “products” that come out of synthesis groups\nUnderstand concepts of data provenance, reproducible analysis, and citations\nEvaluate the reproducibility of a recent data synthesis project\nUnderstand the different models of data accessibility, licensing, and authorship practices and apply them to a synthesis group’s desired outcomes\nUnderstand several funding opportunities that for synthesis research, their requirements and expectations, and their respective strengths and weaknesses for starting and sustaining synthesis research.\nCreate a plan to maintain a synthesis project and associated data over the long-term"
  },
  {
    "objectID": "module3.html#synthesis-data-structures-and-formats",
    "href": "module3.html#synthesis-data-structures-and-formats",
    "title": "Module 3: Tying it all together",
    "section": "Synthesis data structures and formats",
    "text": "Synthesis data structures and formats\nData structure refers to the arrangement and organization of individual units of information in a data file. Data format generally refers to the data file type, i.e. text files, Microsoft Excel files, .jpeg image files, etc. Some data files are both a format and a structure. For example, comma separated value, or CSV, files are a common text-based data files where each line of text represents a table row, and data in columns are separated by commas.\n\nDatabase\nText tables\nSpecial data types"
  },
  {
    "objectID": "module3.html#metadata",
    "href": "module3.html#metadata",
    "title": "Module 3: Tying it all together",
    "section": "Metadata",
    "text": "Metadata\nMetadata are data about the data. As a general rule, metadata should describe\n\nWho collected the data\nWhat was observed or measured\nWhen the data were collected\nWhere the data were collected\nHow the data were collected (methods, instruments, etc.)\n\nOftentimes, including information about why the data were collected can help future users understand the context of the data and use them.\nAssembling metadata should begin early, and is an integral part of doing data synthesis throughout the life-cycle of the project. Consider building metadata collection in to the workflow and project management elements of your project."
  },
  {
    "objectID": "module3.html#activity-evaluate-published-datasets",
    "href": "module3.html#activity-evaluate-published-datasets",
    "title": "Module 3: Tying it all together",
    "section": "Activity: Evaluate published datasets",
    "text": "Activity: Evaluate published datasets\nEstimated time: 10 min\nForm breakout groups and course instructors will assign each group a dataset (a DOI) for evaluation. With your group, answer these questions about the dataset:\n\nWhere were the data collected?\nWhat variables were measured and in what units?\nAre the data raw or have they been altered since collection? If the latter, how?\nWere the first three questions easy to answer? Why or why not?\n\n\nExample 1Example 2Example 3\n\n\nData from the SoDaH working group\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=edi.521.1\n\n\nA Dryad dataset from a synthesis paper about oligotrophication\nhttps://doi.org/10.5061/dryad.v2k2607\n\n\nMaybe this: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.493.16 (needs editing)\nMaybe this: https://doi.org/10.6084/m9.figshare.10735652.v1 (but pretty bad)"
  },
  {
    "objectID": "module3.html#summary",
    "href": "module3.html#summary",
    "title": "Module 3: Tying it all together",
    "section": "Summary",
    "text": "Summary\n\n…\nKeep in mind that there is no PERFECT dataset!"
  },
  {
    "objectID": "module3.html#additional-resources",
    "href": "module3.html#additional-resources",
    "title": "Module 3: Tying it all together",
    "section": "Additional resources",
    "text": "Additional resources\n\nWilkinson, Mark D., et al. “The FAIR Guiding Principles for scientific data management and stewardship.” Scientific data 3.1 (2016): 1-9. DOI: 10.1038/sdata.2016.18\nNEON’s guide\nEDI repository data author guide"
  },
  {
    "objectID": "module3.html#publishing-data-5",
    "href": "module3.html#publishing-data-5",
    "title": "Module 3: Tying it all together",
    "section": "Publishing data (5)",
    "text": "Publishing data (5)\n\nData and metadata\nData provenance - especially important in synthesis\nResearch data repositories\nLicensing, etc."
  },
  {
    "objectID": "module3.html#sharing-the-workflow-7",
    "href": "module3.html#sharing-the-workflow-7",
    "title": "Module 3: Tying it all together",
    "section": "Sharing the workflow (7)",
    "text": "Sharing the workflow (7)\n\nData sources\nCode for analysis\nVersion control systems\nFuture contributions"
  },
  {
    "objectID": "module3.html#writing-the-results-3",
    "href": "module3.html#writing-the-results-3",
    "title": "Module 3: Tying it all together",
    "section": "Writing the results (3)",
    "text": "Writing the results (3)\n\nMost often this means writing a paper\n\nCiting data in a paper\nPresenting a reproducible workflow\n\nData papers\n\nAnatomy of a data paper?\n\nOther products may be apps, teaching modules, etc.\n\nMacrosystemsEDDIE as an example?\nShiny apps"
  },
  {
    "objectID": "module3.html#tying-these-all-together-5",
    "href": "module3.html#tying-these-all-together-5",
    "title": "Module 3: Tying it all together",
    "section": "Tying these all together (5)",
    "text": "Tying these all together (5)\nSome general considerations\n\nSynthesis projects can become complex"
  },
  {
    "objectID": "module3.html#giving-everyone-credit-5",
    "href": "module3.html#giving-everyone-credit-5",
    "title": "Module 3: Tying it all together",
    "section": "Giving everyone credit (5)",
    "text": "Giving everyone credit (5)"
  },
  {
    "objectID": "module3.html#activity-evaluate-three-synthesis-projects",
    "href": "module3.html#activity-evaluate-three-synthesis-projects",
    "title": "Module 3: Tying it all together",
    "section": "Activity: Evaluate three synthesis projects",
    "text": "Activity: Evaluate three synthesis projects\nEstimated time: 12 min\nForm breakout groups and course instructors will assign each group a synthesis project, with links to one or more elements (the code, a paper, a dataset, etc.). Try to put together the rest of the picture (other publications, source/derived data, etc.). Then, answer these questions:\n\nWho was involved in the synthesis project?\nHow many journal articles and datasets were published?\nCould you reproduce results or contribute new data to the project based on what you found?\nWhat elements are missing or difficult to understand?\n\n\nExample 1Example 2Example 3\n\n\nSoDAH\n\n\nCoRRE or metacommunities\nhttps://corredata.weebly.com/\n\n\nSilica exports\nhttps://github.com/lter/lterwg-silica-data"
  },
  {
    "objectID": "module3.html#additional-resources-1",
    "href": "module3.html#additional-resources-1",
    "title": "Module 3: Tying it all together",
    "section": "Additional resources",
    "text": "Additional resources"
  },
  {
    "objectID": "module3.html#funding-sources",
    "href": "module3.html#funding-sources",
    "title": "Module 3: Tying it all together",
    "section": "Funding sources",
    "text": "Funding sources\n???"
  },
  {
    "objectID": "module3.html#footnotes",
    "href": "module3.html#footnotes",
    "title": "Module 3: Tying it all together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18↩︎\nGoFAIR initiative↩︎\nThe FAIR Cookbook↩︎\nBahim, C., Casorrán-Amilburu, C., Dekkers, M., Herczog, E., Loozen, N., Repanas, K., Russell, K. and Stall, S. (2020) ‘The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments’, Data Science Journal, 19(1), p. 41. Available at: https://doi.org/10.5334/dsj-2020-041.↩︎\nGries, Corinna, et al. “The environmental data Initiative: Connecting the past to the future through data reuse.” Ecology and Evolution 13.1 (2023): e9592. https://doi.org/10.1002/ece3.9592↩︎"
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Module 2: Operational synthesis",
    "section": "",
    "text": "After completing this module, you will be able to:\n\nIdentify characteristics of reproducible coding / project organization\nExplain benefits of reproducibility (to your team and beyond)\nSummarize the advantages of creating a defined contribution workflow\nDefine fundamental vocabulary of version control systems\nCreate a repository on GitHub\nExplain how synthesis teams can use GitHub to collaborate more efficiently and reproducibly"
  },
  {
    "objectID": "module2.html#general-considerations",
    "href": "module2.html#general-considerations",
    "title": "Module 2: Operational synthesis",
    "section": "General Considerations",
    "text": "General Considerations\nEstimated time: 10 min\n\n Documentation\n\nOne folder per project\nFurther organize content via sub-folders\nMake file names informative and intuitive\n\nAvoid spaces and special characters in file names\nFollow a consistent naming convention throughout\n\nUse READMEs to record organization rules / explanation\nKeep a log of where source data came from.\n\nWhere did you search?\nWhat search terms did you use?\nList the dataset identifiers you downloaded/used.\n\n\n\n\n\n Code\n\nUse a version control system\nLoad libraries/packages explicitly\nTrack (and document) software versions\nNamespace functions (if not already required by your coding language)\nUse relative file paths that are operating system-agnostic\nBalance how descriptive object names are with striving for concise names\nUse comments in the code!\nConsider custom functions\nFor scripts that need to be run in order, consider adding step numbers to the file name\n\n\n\n\n Contributing\n\nCreate a formal plan for collaborating with which your whole team agrees\nQuarantine external inputs\nPlan for “future you”\nCommunicate to your collaborators whenever you’re working on a specific script to avoid conflicting edits"
  },
  {
    "objectID": "module2.html#synthesis-considerations",
    "href": "module2.html#synthesis-considerations",
    "title": "Module 2: Operational synthesis",
    "section": "Synthesis Considerations",
    "text": "Synthesis Considerations\nEstimated time: 10 min\nHow does reproducibility in synthesis considerations differ from individual / non-synthesis applications?\n\nJudgement calls need to be made / agreed to as a group\n\nBut “defer to the doers”\n\nIncreased emphasis on contribution guidelines / planning being formalized\nMore communication needs\nMust ensure that every team member has sufficient access to the project files\nIts best to keep track of who contributed what, so that everyone gets credit. This can be challenging in practice."
  },
  {
    "objectID": "module2.html#data-cleaning-and-filtering",
    "href": "module2.html#data-cleaning-and-filtering",
    "title": "Module 2: Operational synthesis",
    "section": "Data cleaning and filtering",
    "text": "Data cleaning and filtering\nWhen assembling large datasets from diverse sources, as in synthesis research, not all data from the source datasets will be useful. This may be because there are real or suspected errors in the data, values are mission, or simply because they are not needed to answer the scientific question being asked (wrong variable, different ecosystem, etc.). Data that are not useful are usually excluded from analysis or removed altogether. Data cleaning tends to be a stepwise, iterative process that follows a different path for every dataset and research project. Though there are many standard techniques and algorithms for cleaning and filtering data, they are beyond the scope of this course. Below are a few guidelines to remember when cleaning and filtering data, and more in-depth resources for data cleaning are found at the end of this section.\n\nAlways preserve the raw data. Chances are you’ll want to go back and check the original source data at least once.\nUse a scripted workflow to clean and filter the raw data, and follow the usual rules about reproducibility (comments, version control, functionalization).\nSpread the data cleaning workload around! Data cleaning typically demands a HUGE fraction of the total time devoted to working with data 3 4 5, and it can be tedious work. Make sure the team shares this workload equitably.\n\nData cleaning and filtering resources\n\nData cleaning is a complex topic, and entire books have been written on the subject 6 7, For some general considerations on cleaning data, see EDI’s “Cleaning Data and Quality Control” resource\nOpenRefine is an open-source, cross-platform tool for iterative, scripted data cleaning.\nIn the R language, the tidyverse libraries (particularly tidyr and dplyr) are often used for data cleaning, as are additional libraries like janitor.\nIn python, pandas and numpy libraries provide useful data cleaning features. There are also some stand-alone cleaning tools like pyjanitor (started as a re-implementation of the R version) and cleanlab (geared towards machine learning applications).\nBoth the R and Python data science ecosystems have excellent documentation resources that thoroughly cover data cleaning. For R, consider starting with Hadley Wickham’s R for Data Science 8 book chapter on data tidying, and for python, check Wes McKinney’s Python for Data Analysis 9 book chapter on data cleaning and preparation."
  },
  {
    "objectID": "module2.html#data-harmonization",
    "href": "module2.html#data-harmonization",
    "title": "Module 2: Operational synthesis",
    "section": "Data harmonization",
    "text": "Data harmonization\nData harmonization is the process of bringing different datasets into a common format for analysis. The harmonized data format chosen for a synthesis project depends on the source data, analysis plans, and overall project goals. It is best to make a plan for harmonizing data BEFORE analysis begins, which means discussing this with the team in the early stages of a synthesis project. As a general rule, it is also wise to use a scripted workflow that is as reproducible as possible to accomplish the harmonization you need for your project. Following this guidance lets others understand, check, and adapt your work, and will also make it much, much easier to bring new data and analysis methods into the project.\nData harmonization is hard work that sometimes requires trial and error to arrive at a useful end product. Looking at a simple example might help.\n\nExample: Harmonizing grassland biomass data\nIn the figure below, two datasets from different LTER sites have been harmonized into one file for analysis. We don’t have all the metadata here, but based on the column naming we can assume that the file on the left (Konza_harvestplots.txt, yellow) contains columns for the sampling date, a plot identifier, a treatment categorical value, and measured biomass values. The filename suggests that the data come from the Konza Prarie LTER site. The file on the right (2022_clips.csv, blue) has a column denoting the LTER site, in this case Sevilleta LTER (abbreviated as SEV), as well as similar date, plot identifier, treatment, and measured biomass columns.\n\n\n\n\n\n\nThere are some similarities and some differences in these two source files. A harmonized file (lter_grass_biomass_harmonized.txt) appears below.\nCan you identify some changes made to data structure, variable formatting, or units to harmonize these data?\n\n\n\n\n\n\nEven though the data files were similar, several important changes were made to create the harmonized file. Among them:\n\nThe site column was preserved and contains the “SEV” and “KNZ” categorical values denoting which LTER site is observed in each row.\nThe dates in the date column were converted to a standard format (YYYY-MM-DD). Incidentally, this matches the International Organization for Standardization (ISO) recommendation for date/time data, making it generally a good choice for dates.\nA new rep column was created to identify the replicate measurements at each site. This is essentially a standard version the plot identifier columns (PlotID and plot) in the original data file. Also note that the original values are preserved in the new plot_orig column.\nThe treatment columns from the original data files (Treatment and trt) were standardized to one column with “C” and “F” categorical values, for control and fertilized treatments, respectively.\nThe biomass values from Konza were converted to units grams per meter squared (g/m2) because the original Konza measurements were for total biomass in 2x2 meter plots. Note that this conversion is for illustration purposes only - we can’t be sure if this conversion is correct without it being spelled out in the metadata, or asking the data provider directly."
  },
  {
    "objectID": "module2.html#a-word-about-harmonized-data-formats",
    "href": "module2.html#a-word-about-harmonized-data-formats",
    "title": "Module 2: Operational synthesis",
    "section": "A word about harmonized data formats",
    "text": "A word about harmonized data formats\nAbove, we’ve discussed several aspects of selecting a data format. There are at least three related, but not exactly equivalent, concepts to consider when formatting data. First, formats describe the way data are structured, organized, and related within a data file. For example, in a tabular data file about biomass, the measured biomass values might appear in one column, or in muiltiple columns. Second, the values of any variable can be represented in more than one format. The same date, for example, could be formatted using text as “June 1, 1977” or “1977-06-01.” Third, format may refer to the file format used to hold data on a disk or other storage medium. File formats like comma separated value text files (CSV), Excel files (.xlsx), JPEG images, are commonly used for research data, and each has particular strengths for certain kinds of data.\nA few guidelines apply:\n\nFor formatting a tabular dataset, err towards simpler data structures, which are usually easier to clean, filter, and analyze. Long-format tables, or tidy data 10, is one common recommendation for this.\nWhen choosing a file format, err towards open, non-proprietary file formats that more people know and have access to. Delimited text files, such as CSV files, are a good choice for tabular data.\nUse existing community standards for formatting variables and files as long they suit your project methods and scientific goals. Using ISO standards for date-time variables, or species identifiers from a taxonomic authority, are good examples of this practice.\nThere is no perfect data format! Harmonizing data always involves some judgement calls and tradeoffs.\n\nWhen choosing a destination format for the harmonized data for a synthesis project, the audience and future uses of the data are also an important consideration. Consider how your synthesis team will analyze the data, as well as how the world outside that team will use and interact with the data once it is published. Again, there is no one answer, but below are a few examples of harmonized destination formats to consider.\n\nLong (Tidy)Wide (Untidy)Relational (database style)Cloud-nativeOther…\n\n\nHere our grassland biomass data is in long format, often referred to as “tidy” data. Data in this format is generally easy to understand and use. There are three rules for tidy data:\n\nEach column is one variable.\nEach row is one observation.\nEach cell contains a single value.\n\nAdvantages: clear meaning of rows and columns, ease in filtering/cleaning/appending\nDisadvantages: observation information is repeated, file size larger\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nIn this dataset, our grassland data has been restructured into wide format, often referred to (sometimes unfairly) as “messy” or “untidy” data. Note that the biomass variable has been split into two columns, one for control plots and one for fertilized plots.\nAdvantages: compact file size, easier for some statistical analyses (ANOVA, for example)\nDisadvantages: may be more difficult to clean/filter/append, multiple observations per row\nPossible file formats: Delimited text (tab delimited shown here), spreadsheets, database tables\n\n\n\nThe harmonized grassland data, restructured into wide format with biomass values in control and fertilized columns.\n\n\n\n\nBelow is a schematic of the related tables that comprise the ecocomDP11 harmonized data format for biodiversity data. Eight tables are defined, along with a set of relationships between tables (keys), and constraints on the allowable values in each table. Relational formats like this are “normalized” to reduce data redundancy, and increase data integrity.\nAdvantages: reduced redundancy, greater integrity, community standard\nDisadvantages: significant metadata needed to describe and use, more complex to publish\nPossible file formats: Database stores, can be represented in delimited text (CSV)\n\n\n\nThe ecocomDP schema. Each table has a name (top cell) and a list of columns. Shaded column names are primary keys, hashed columns have constraints, and arrows represent relations between keys/constraints in different tables.\n\n\n\n\nThere are many possibilities to make large synthesis datasets available and useful in the cloud. These require specialized knowledge and tooling, and reliable access to cloud platforms.\nAdvantages: easier access to big data\nDisadvantages: less familiar/accessible to many scientists, few best practices to follow, costs can be higher\nPossible file formats: Parquet files, distributed/cloud databases\n\n\n\nThere are many, many other possible harmonized data formats. Here are a few possible examples:\n\nDarwinCore archives for biodiversity data\nOrganismal trait databases\nArchives of cropped, labeled images for training machine or deep learning models\nLibraries of standardized raster imagery in Google Earth Engine"
  },
  {
    "objectID": "module2.html#vocabulary",
    "href": "module2.html#vocabulary",
    "title": "Module 2: Operational synthesis",
    "section": "Vocabulary",
    "text": "Vocabulary\nEstimated time: 5 min\nBrief definitions for a selection of fundamental version control vocabulary terms\n\nVersion control system: software that tracks iterative changes to your code and other files\nRepository: the specific folder/directory that is being tracked by a version control system\nGit: a popular open-source distributed version control system\nGitHub: a website that allows users to store their Git repositories online and share them with others"
  },
  {
    "objectID": "module2.html#github",
    "href": "module2.html#github",
    "title": "Module 2: Operational synthesis",
    "section": "GitHub",
    "text": "GitHub\nEstimated time: 10 min\nWhile this section of the module focuses on GitHub, there are several other viable alternatives for working with Git individually or as part of a larger team (e.g., GitLab, GitKraken, etc.). Any of these may be viable option for your team and we focus on GitHub here only to ensure a standard backdrop for the case studies we’ll discuss shortly.\nThere are a lot of GitHub tutorials that exist already so, rather than add our own variant to the list, we’ll work through part of one created by the Scientific Computing team of the National Center for Ecological Analysis and Synthesis (NCEAS).\nSee the workshop materials here.\nGiven the time restrictions for this short course, we’ll only cover how you engage with GitHub directly through the GitHub website. However, your chosen software for writing code will certainly have a method of connecting to GitHub/etc., so if this topic is of interest it will be beneficial for you to search out the relevant tutorial."
  },
  {
    "objectID": "module2.html#courses-workshops-and-tutorials",
    "href": "module2.html#courses-workshops-and-tutorials",
    "title": "Module 2: Operational synthesis",
    "section": "Courses, Workshops, and Tutorials",
    "text": "Courses, Workshops, and Tutorials\n\nSynthesis Skills for Early Career Researchers (SSECR) course. 2024. LTER Network Office\nReproducible Approaches to Arctic Research Using R workshop. 2024. Arctic Data Center & NCEAS Learning Hub\nCollaborative Coding with GitHub workshop. 2024. NCEAS Scientific Computing team\nCoding in the Tidyverse workshop. 2023. NCEAS Scientific Computing team\nShiny Apps for Sharing Science workshop. 2022. Lyon, N.J. et al.\nTen Commandments for Good Data Management. 2016. McGill, B."
  },
  {
    "objectID": "module2.html#literature",
    "href": "module2.html#literature",
    "title": "Module 2: Operational synthesis",
    "section": "Literature",
    "text": "Literature\n\nHarrer, M. et al. Doing Meta-Analysis with R: A Hands-On Guide. 2023. GitHub\nTodd-Brown, K.E.O., et al. Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. 2022. Biogeosciences\nBorer, E.T. et al. Some Simple Guidelines for Effective Data Management. 2009. Ecological Society of America Bulletin"
  },
  {
    "objectID": "module2.html#footnotes",
    "href": "module2.html#footnotes",
    "title": "Module 2: Operational synthesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMichener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. and Stafford, S.G. (1997), NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES. Ecological Applications, 7: 330-342. https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2↩︎\nMayernik, M.S. and Acker, A. (2018), Tracing the traces: The critical role of metadata within networked communications. Journal of the Association for Information Science and Technology, 69: 177-180. https://doi.org/10.1002/asi.23927↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nNew York Times, 2014↩︎\nAnaconda State of Data Science Report, 2022↩︎\nOsborne, Jason W. Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data. Sage publications, 2012.↩︎\nVan der Loo, Mark, and Edwin De Jonge. Statistical data cleaning with applications in R. John Wiley & Sons, 2018. https://doi.org/10.1002/9781118897126↩︎\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for data science. ” O’Reilly Media, Inc.”, 2023. https://r4ds.hadley.nz/↩︎\nMcKinney, Wes. Python for data analysis. ” O’Reilly Media, Inc.”, 2022. https://wesmckinney.com/book↩︎\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎\nO’Brien, Margaret, et al. “ecocomDP: a flexible data design pattern for ecological community survey data.” Ecological Informatics 64 (2021): 101374. https://doi.org/10.1016/j.ecoinf.2021.101374↩︎"
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Module 1: Starting with team science",
    "section": "",
    "text": "Understand the power and benefits of synthesis\nIdentify funding sources for supporting synthesis\nRecognize the importance of establishing group norms and expectations\nEvaluate strategies for group organization and project management"
  },
  {
    "objectID": "module1.html#additional-resources",
    "href": "module1.html#additional-resources",
    "title": "Module 1: Starting with team science",
    "section": "Additional resources",
    "text": "Additional resources\n\nAdam, 2023. ‘Disruptive’ science: in-person teams make more breakthroughs than remote groups\nHackett, 2020. Collaboration and Sustainability: Making Science Useful, Making Useful Science\nHampton and Parker, 2011. Collaboration and Productivity in Scientific Synthesis\nHackett et al., 2021. Do synthesis centers synthesize? A semantic analysis of topical diversity in research\nWyborn et al., 2018. Understanding the Impacts of Research Synthesis"
  },
  {
    "objectID": "module1.html#sources-of-funding-and-support",
    "href": "module1.html#sources-of-funding-and-support",
    "title": "Module 1: Starting with team science",
    "section": "Sources of Funding and Support",
    "text": "Sources of Funding and Support\nNSF Programs - ULTRA-data Dear Colleague letter - NSF Core Programs, e.g. Division of Environmental Biology: “synthesis activities”, “synthesis projects” - NSF workshops,\nSynthesis Centers - Environemntal Science Innovation and Inclusion Laboratory (ESIIL) - National Center for Ecological Analysis and Synthesis - Morpho Program - USGS Powell Center - S-div - Canadian Institute of Ecology and Evolution (CIEE)\nSocieties - New Phytologist Workshops - Gordon Research Conferences - Chapman Conferences - British Ecological Society - one-third of participants from developing world"
  },
  {
    "objectID": "module1.html#typical-process",
    "href": "module1.html#typical-process",
    "title": "Module 1: Starting with team science",
    "section": "Typical Process",
    "text": "Typical Process\nTypically, a group of researchers–or researchers and managers or community members–will plan a series of meetings over 2-3 years. The mix of in-person v. virtual meetings and work will vary across different groups and different funders, but the general pattern is similar.\nEarly meetings focus on narrowing the questions and deciding what data is needed and what analyses will be most useful. A period of data gathering and assembly comes next. The assembly of data almost always prompts a revision of the initial questions, as data rarely comes in exactly the form that researchers expected. This can be both the most frustrating and the most interesting part of the process as new hypotheses and models are floated and discussed. It is especially important to have the full participation of researchers familiar with different fields and ecosystems in this process.\nWith tractable questions refined, the group will move into analysis mode. Often, a few individuals will do most of the data wrangling and coding, but will need continuous input on analytical decisions. In our experience, GitHub issues is one very good tool for facilitating and recording these decisions.\nLater meetings will focus on developing manuscripts and/or application-related products such as white papers and decision support tools."
  },
  {
    "objectID": "module1.html#how-to-get-involved",
    "href": "module1.html#how-to-get-involved",
    "title": "Module 1: Starting with team science",
    "section": "How to get Involved",
    "text": "How to get Involved\nOften, early career researchers will be excited about the idea of synthesis but be unsure how to connect with existing (or forming) synthesis efforts. Here are a few ideas:\n\nMake it known you want to be involved in synthesis\n\nLet your advisor know\nShare your enthusiasm\n\nSkill building:\n\nSynthesis Skills for Early Career Researchers: SSECR\nData Carpentries\nESIIL: innovation summit, hackathons\nEnvironmental Data Science Summit\n\nBuild your community\n\nAsk questions at meetings\nInitiate conversations\n\nStart your own!"
  }
]